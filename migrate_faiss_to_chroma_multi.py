# migrate_faiss_to_chroma_multi.py
# -*- coding: utf-8 -*-

import os
import json
import argparse
from typing import Any, Dict, List, Tuple

import faiss
from chromadb import PersistentClient
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain_core.embeddings import Embeddings


class _NoopEmbeddings(Embeddings):
    """load_localìš© ë”ë¯¸ ì„ë² ë”© (ì‹¤ì œ ì„ë² ë”© ì¬ê³„ì‚° ì•ˆ í•¨)"""
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        raise RuntimeError("ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì„ë² ë”©ì„ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    def embed_query(self, text: str) -> List[float]:
        raise RuntimeError("ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì„ë² ë”©ì„ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


def _scrub_metadata(meta: Dict[str, Any]) -> Dict[str, Any]:
    """Chromaê°€ JSON-serializableë§Œ ë°›ìœ¼ë¯€ë¡œ, ë¹„ì§ë ¬í™” í•„ë“œëŠ” strë¡œ ë³€í™˜"""
    out = {}
    for k, v in (meta or {}).items():
        try:
            json.dumps(v)
            out[k] = v
        except Exception:
            out[k] = str(v)
    return out


def _l2_normalize(vec: List[float], do_norm: bool) -> List[float]:
    if not do_norm:
        return vec
    import numpy as np
    v = np.array(vec, dtype="float32")
    n = float(np.linalg.norm(v))
    return (v / (n + 1e-12)).tolist()


def _load_faiss_as_iter(
    faiss_dir: str,
    label: str,
    normalize: bool,
) -> Tuple[int, Any]:
    """
    ì£¼ì–´ì§„ LC-FAISS ë””ë ‰í„°ë¦¬ë¥¼ ì—´ê³ , (pos -> item dict) ë¥¼ ë°°ì¹˜ë¡œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆë„ë¡
    ì œë„ˆë ˆì´í„°ë¥¼ ë°˜í™˜.
    """
    print(f"ğŸ“¦ FAISS ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {faiss_dir} (label={label})")
    vs: LCFAISS = LCFAISS.load_local(
        faiss_dir, _NoopEmbeddings(), allow_dangerous_deserialization=True
    )
    faiss_index = vs.index
    mapping = vs.index_to_docstore_id     # { int_index -> doc_id(str) }
    docstore = vs.docstore                # ë¬¸ì„œ ì €ì¥ì†Œ

    if not hasattr(faiss_index, "reconstruct"):
        raise RuntimeError(
            f"[{faiss_dir}] ì´ FAISS ì¸ë±ìŠ¤ëŠ” reconstructë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "- ë³´í†µ LangChain FAISS ê¸°ë³¸ì€ IndexFlatL2ë¼ reconstruct ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "- IVF/PQ ë“±ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤ë©´, ì›ë³¸ ë²¡í„° ì—†ì´ ì •í™• ë³µì›ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´ìš”.\n"
            "  (ê°€ëŠ¥í•˜ë©´ IndexFlat ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ë¹Œë“œí•˜ê±°ë‚˜, ì„ë² ë”©ì„ ë”°ë¡œ ë³´ê´€í–ˆë‹¤ê°€ ì‚¬ìš©í•˜ì„¸ìš”.)"
        )

    positions = sorted(mapping.keys())
    total = len(positions)

    def iterator(batch_size: int):
        for start in range(0, total, batch_size):
            end = min(start + batch_size, total)
            batch_pos = positions[start:end]

            ids: List[str] = []
            docs: List[str] = []
            metas: List[Dict[str, Any]] = []
            embs: List[List[float]] = []

            for pos in batch_pos:
                doc_id = mapping[pos]
                doc = docstore.search(doc_id)  # langchain.schema.Document
                text = doc.page_content if hasattr(doc, "page_content") else str(doc)
                meta = _scrub_metadata(getattr(doc, "metadata", {}) or {})
                # ì¶œì²˜ ë¼ë²¨ ì¶”ê°€
                meta = {**meta, "_source": label}

                vec = faiss_index.reconstruct(pos)  # numpy array(float32)
                vec_list = [float(x) for x in vec]
                vec_list = _l2_normalize(vec_list, normalize)

                # ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ composite id (label:doc_id)
                ids.append(f"{label}:{doc_id}")
                docs.append(text)
                metas.append(meta)
                embs.append(vec_list)
            yield start, end, ids, docs, metas, embs

    return total, iterator


def main():
    parser = argparse.ArgumentParser(description="ì—¬ëŸ¬ FAISS(VectorStore) â†’ ë‹¨ì¼ ChromaDB ì»¬ë ‰ì…˜ ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument(
        "--faiss_dirs",
        nargs="+",
        required=True,
        help="LC FAISS ë””ë ‰í„°ë¦¬ ëª©ë¡ (ì˜ˆ: notion_faiss_index gdrive_faiss_index github_faiss_index)",
    )
    parser.add_argument(
        "--labels",
        nargs="+",
        default=None,
        help="ê° FAISS ë””ë ‰í„°ë¦¬ì˜ ë¼ë²¨(ì¶œì²˜) ëª©ë¡. ë¯¸ì§€ì • ì‹œ ë””ë ‰í„°ë¦¬ basename ì‚¬ìš©",
    )
    parser.add_argument("--chroma_dir", required=True, help="Chroma persist ë””ë ‰í„°ë¦¬ (ë‹¨ì¼)")
    parser.add_argument("--collection", required=True, help="Chroma ì»¬ë ‰ì…˜ ì´ë¦„ (ë‹¨ì¼)")
    parser.add_argument("--batch", type=int, default=1024, help="ì¶”ê°€(add) ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--space", choices=["cosine", "l2", "ip"], default="cosine",
                        help="Chroma HNSW distance space (ê¸°ë³¸: cosine)")
    parser.add_argument("--normalize", action="store_true",
                        help="ì½”ì‚¬ì¸ ìœ ì‚¬ë„(space=cosine) ì‚¬ìš© ì‹œ ë²¡í„°ë¥¼ L2 ì •ê·œí™”í•˜ì—¬ ì‚½ì…")
    args = parser.parse_args()

    # labels ì •ë¦¬
    faiss_dirs: List[str] = args.faiss_dirs
    if args.labels is None:
        labels = [os.path.basename(os.path.normpath(d)) for d in faiss_dirs]
    else:
        if len(args.labels) != len(faiss_dirs):
            raise ValueError("--labels ê°œìˆ˜ëŠ” --faiss_dirs ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.")
        labels = args.labels

    print(f"ğŸ’¾ ChromaDB ì´ˆê¸°í™”: {args.chroma_dir} (collection: {args.collection}, space: {args.space})")
    client = PersistentClient(path=args.chroma_dir)
    coll = client.get_or_create_collection(
        name=args.collection,
        metadata={"hnsw:space": args.space}
    )

    grand_total = 0
    for d in faiss_dirs:
        # ë¯¸ë¦¬ ì´ ê°œìˆ˜ íŒŒì•…
        vs_tmp: LCFAISS = LCFAISS.load_local(
            d, _NoopEmbeddings(), allow_dangerous_deserialization=True
        )
        grand_total += len(vs_tmp.index_to_docstore_id)

    print(f"ğŸšš ì´ì „í•  ì „ì²´ ë²¡í„°/ë¬¸ì„œ ê°œìˆ˜ (í•©): {grand_total:,}")

    # ê° ë””ë ‰í„°ë¦¬ ìˆœíšŒí•˜ë©° ë™ì¼ ì»¬ë ‰ì…˜ìœ¼ë¡œ add
    processed = 0
    for d, label in zip(faiss_dirs, labels):
        total, iterator = _load_faiss_as_iter(d, label, args.normalize)
        print(f"â–¶ ì‹œì‘: {d} (ì´ {total:,}ê°œ, label={label})")

        for start, end, ids, docs, metas, embs in iterator(args.batch):
            print(f"  â• [{label}] {start:,} ~ {end-1:,} ì¶”ê°€ ì¤‘...")
            coll.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
            processed += (end - start)

    print("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ! â†’", os.path.abspath(args.chroma_dir))
    print(f"âœ… ì»¬ë ‰ì…˜ '{args.collection}' ë¬¸ì„œ ìˆ˜ (ì¶”ì •): {coll.count()}")
    print("   ì´ì œ Chroma ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰/ì„œë¹™ ì½”ë“œë¥¼ êµì²´í•˜ë©´ ë©ë‹ˆë‹¤.")
    print(f"   (ì´ ì‚½ì… ì‹œë„ ìˆ˜: {processed:,})")


if __name__ == "__main__":
    main()


'''
python migrate_faiss_to_chroma_multi.py \
  --faiss_dirs gdrive_faiss_index notion_faiss_index github_faiss_index \
  --labels gdrive notion github \
  --chroma_dir ./chroma_all \
  --collection all \
  --space cosine \
  --normalize \
  --batch 1024
'''

# python migrate_faiss_to_chroma_multi.py   --faiss_dirs gdrive_faiss_index notion_faiss_index    --labels gdrive notion   --chroma_dir ./chroma_all --collection all --space cosine --normalize   --batch 1024