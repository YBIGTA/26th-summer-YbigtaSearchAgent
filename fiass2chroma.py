# migrate_faiss_to_chroma.py
# -*- coding: utf-8 -*-
"""
FAISS(VectorStore, LangChain ì €ì¥í˜•ì‹) -> ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
- ì„ë² ë”© ì¬ê³„ì‚° ì—†ì´ FAISSì—ì„œ ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ê·¸ëŒ€ë¡œ Chromaì— ì ì¬
- ë¬¸ì„œ(document)ì™€ ë©”íƒ€ë°ì´í„°(metadata)ë„ í•¨ê»˜ ì´ì „
- gdrive, notion, github ì„¸ ê°œì˜ FAISS ì¸ë±ìŠ¤ë¥¼ í†µí•©
"""

import os
import json
import argparse
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import faiss
from chromadb import Client
from chromadb.config import Settings
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain.schema import Document


def load_langchain_faiss(faiss_dir: str) -> LCFAISS:
    if not os.path.isdir(faiss_dir):
        raise FileNotFoundError(f"FAISS ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {faiss_dir}")
    # embeddings ì¸ìŠ¤í„´ìŠ¤ëŠ” ë¡œë“œ ì‹œ í•„ìš”ì—†ê³ , ì¬ì§ë ¬í™” í—ˆìš© í”Œë˜ê·¸ë§Œ ì§€ì •
    # (ë¬¸ì„œ/ë©”íƒ€ë°ì´í„°ì™€ faiss indexë§Œ ì½ì–´ì˜´)
    vec = LCFAISS.load_local(
        folder_path=faiss_dir,
        embeddings=None,
        allow_dangerous_deserialization=True,
    )
    return vec


def extract_all_doc_ids(vec: LCFAISS) -> List[str]:
    # LangChain FAISS êµ¬ì¡°:
    # - vec.index_to_docstore_id: List[int->str] ë˜ëŠ” Dict[int->str] í˜•íƒœ
    # - vec.docstore._dict: {doc_id(str): Document}
    mapping = vec.index_to_docstore_id
    if isinstance(mapping, dict):
        # ì¼ë¶€ ë²„ì „ì—ì„  dictì¼ ìˆ˜ ìˆìŒ (key: int index, value: doc_id)
        ordered = [mapping[i] for i in sorted(mapping.keys())]
        return ordered
    elif isinstance(mapping, list):
        return list(mapping)
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” index_to_docstore_id íƒ€ì…: {type(mapping)}")


def get_documents_and_metadatas(vec: LCFAISS, doc_ids: List[str]) -> Tuple[List[str], List[Dict[str, Any]]]:
    texts: List[str] = []
    metas: List[Dict[str, Any]] = []
    # vec.docstore._dict ì—ì„œ Documentë¥¼ êº¼ëƒ„
    store: Dict[str, Document] = getattr(vec.docstore, "_dict", {})
    for doc_id in doc_ids:
        doc: Optional[Document] = store.get(doc_id)
        if doc is None:
            # ë“œë¬¼ê²Œ ëˆ„ë½ë¼ ìˆì„ ìˆ˜ ìˆìŒ
            texts.append("")
            metas.append({"_warning": "document_missing"})
        else:
            texts.append(doc.page_content or "")
            # metadataê°€ dictê°€ ì•„ë‹ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ë°©ì–´ì  ìºìŠ¤íŒ…
            md = dict(doc.metadata) if isinstance(doc.metadata, dict) else {}
            metas.append(md)
    return texts, metas


def faiss_reconstruct_all(index: faiss.Index, expected_dim: Optional[int] = None) -> np.ndarray:
    """FAISS ì¸ë±ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë²¡í„°ë¥¼ ì¶”ì¶œ.
    - IndexFlat* ê³„ì—´: reconstruct_n ë˜ëŠ” xb ë…¸ì¶œ
    - IVF ë“±: reconstruct/reconstruct_n ì§€ì› ì‹œ ì‚¬ìš© (ì§€ì› ì•ˆë˜ë©´ ì‹¤íŒ¨ ê°€ëŠ¥)
    """
    n = index.ntotal
    if n == 0:
        return np.empty((0, expected_dim or 0), dtype="float32")

    # ì‹œë„ 1) reconstruct_n
    if hasattr(index, "reconstruct_n"):
        try:
            arr = np.zeros((n, index.d), dtype="float32")
            index.reconstruct_n(0, n, faiss.swig_ptr(arr))
            if expected_dim is not None and arr.shape[1] != expected_dim:
                raise ValueError(f"ì°¨ì› ë¶ˆì¼ì¹˜: FAISS={arr.shape[1]}, ê¸°ëŒ€ê°’={expected_dim}")
            return arr
        except Exception:
            pass

    # ì‹œë„ 2) IndexFlat ê³„ì—´ì˜ xb ì¶”ì¶œ
    # ì¼ë¶€ SWIG ë°”ì¸ë”©ì—ì„œ index.xb ì ‘ê·¼ ê°€ëŠ¥
    if hasattr(index, "xb"):
        try:
            xb = faiss.vector_to_array(index.xb)
            arr = xb.reshape(n, index.d).astype("float32")
            if expected_dim is not None and arr.shape[1] != expected_dim:
                raise ValueError(f"ì°¨ì› ë¶ˆì¼ì¹˜: FAISS={arr.shape[1]}, ê¸°ëŒ€ê°’={expected_dim}")
            return arr
        except Exception:
            pass

    # ì‹œë„ 3) í•œ ê°œì”© reconstruct
    if hasattr(index, "reconstruct"):
        try:
            arr = np.zeros((n, index.d), dtype="float32")
            for i in range(n):
                v = np.zeros((index.d,), dtype="float32")
                index.reconstruct(i, faiss.swig_ptr(v))
                arr[i] = v
            if expected_dim is not None and arr.shape[1] != expected_dim:
                raise ValueError(f"ì°¨ì› ë¶ˆì¼ì¹˜: FAISS={arr.shape[1]}, ê¸°ëŒ€ê°’={expected_dim}")
            return arr
        except Exception:
            pass

    raise RuntimeError(
        "ì´ FAISS ì¸ë±ìŠ¤ íƒ€ì…ì—ì„œëŠ” ë²¡í„°ë¥¼ ì¬êµ¬ì„±(reconstruct)í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        "- IndexIVF/PQ ê³„ì—´ì€ ì›ë³¸ ë²¡í„°ë¥¼ ì €ì¥í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "- ì´ ê²½ìš°, ì„ë² ë”©ì„ ë³´ìœ í•œ ì›ì²œì—ì„œ ì¬ê³„ì‚° í›„ Chromaì— ì ì¬í•´ì•¼ í•©ë‹ˆë‹¤."
    )


def chunked(iterable, size: int):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf


def process_faiss_index(faiss_dir: str, source_name: str, expected_dim: int) -> Tuple[List[str], List[str], List[Dict[str, Any]], np.ndarray]:
    """ë‹¨ì¼ FAISS ì¸ë±ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë¬¸ì„œì™€ ë²¡í„°ë¥¼ ì¶”ì¶œ"""
    print(f"ğŸ”¹ {source_name} FAISS ë¡œë“œ ì¤‘... ({faiss_dir})")
    vec = load_langchain_faiss(faiss_dir)
    
    # ë¬¸ì„œ ID, í…ìŠ¤íŠ¸/ë©”íƒ€ë°ì´í„°
    print(f"ğŸ”¹ {source_name} ë¬¸ì„œ/ë©”íƒ€ë°ì´í„° ì ì¬ ì¤‘...")
    doc_ids = extract_all_doc_ids(vec)
    texts, metas = get_documents_and_metadatas(vec, doc_ids)
    
    # ì†ŒìŠ¤ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
    for meta in metas:
        meta["source_type"] = source_name
    
    # ë²¡í„° ì¶”ì¶œ
    print(f"ğŸ”¹ {source_name} FAISS ì¸ë±ìŠ¤ì—ì„œ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
    index: faiss.Index = vec.index
    emb = faiss_reconstruct_all(index, expected_dim=expected_dim)
    
    n_total = emb.shape[0]
    if n_total != len(doc_ids):
        raise ValueError(f"{source_name}: ë²¡í„° ìˆ˜({n_total})ì™€ ë¬¸ì„œ ìˆ˜({len(doc_ids)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
    
    print(f"âœ… {source_name}: {n_total}ê°œ ë²¡í„° ì¶”ì¶œ ì™„ë£Œ")
    return doc_ids, texts, metas, emb


def main():
    parser = argparse.ArgumentParser(description="FAISS(4096ì°¨ì›) -> ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ - í†µí•© ë²„ì „")
    parser.add_argument("--chroma-dir", required=True, help="ChromaDB persist ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--collection-name", required=True, help="ìƒì„±/íƒ€ê²Ÿ ì»¬ë ‰ì…˜ ì´ë¦„")
    parser.add_argument("--batch-size", type=int, default=1000, help="Chroma add ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 1000)")
    parser.add_argument("--expected-dim", type=int, default=4096, help="ì„ë² ë”© ì°¨ì› ê²€ì¦ (ê¸°ë³¸ 4096)")
    parser.add_argument("--gdrive-faiss-dir", default="gdrive_faiss_index", help="Google Drive FAISS ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--notion-faiss-dir", default="notion_faiss_index", help="Notion FAISS ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--github-faiss-dir", default="github_faiss_index", help="GitHub FAISS ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    args = parser.parse_args()

    # FAISS ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬ë“¤
    faiss_dirs = {
        "gdrive": args.gdrive_faiss_dir,
        "notion": args.notion_faiss_dir,
        "github": args.github_faiss_dir
    }

    # ëª¨ë“  FAISS ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    all_doc_ids = []
    all_texts = []
    all_metas = []
    all_embeddings = []
    
    total_vectors = 0
    
    for source_name, faiss_dir in faiss_dirs.items():
        if os.path.isdir(faiss_dir):
            try:
                doc_ids, texts, metas, emb = process_faiss_index(faiss_dir, source_name, args.expected_dim)
                
                # ê³ ìœ í•œ ID ìƒì„± (ì†ŒìŠ¤ë³„ prefix ì¶”ê°€)
                prefixed_doc_ids = [f"{source_name}_{doc_id}" for doc_id in doc_ids]
                
                all_doc_ids.extend(prefixed_doc_ids)
                all_texts.extend(texts)
                all_metas.extend(metas)
                all_embeddings.append(emb)
                
                total_vectors += len(doc_ids)
                print(f"âœ… {source_name}: {len(doc_ids)}ê°œ ë²¡í„° ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ {source_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        else:
            print(f"âš ï¸ {source_name} FAISS ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {faiss_dir}")

    if total_vectors == 0:
        raise ValueError("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ëª¨ë“  ì„ë² ë”©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    print("ğŸ”¹ ëª¨ë“  ì„ë² ë”© í†µí•© ì¤‘...")
    combined_embeddings = np.vstack(all_embeddings)
    print(f"âœ… ì´ {total_vectors}ê°œ ë²¡í„° í†µí•© ì™„ë£Œ")

    # Chroma í´ë¼ì´ì–¸íŠ¸/ì»¬ë ‰ì…˜ ì¤€ë¹„
    print("ğŸ”¹ ChromaDB ì´ˆê¸°í™” ì¤‘...")
    client = Client(
        Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=args.chroma_dir,
        )
    )

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒì„±
    try:
        collection = client.get_collection(args.collection_name)
        print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {args.collection_name}")
    except Exception:
        collection = client.create_collection(
            name=args.collection_name,
            metadata={"source": "faiss_migration", "dim": args.expected_dim, "sources": list(faiss_dirs.keys())},
            embedding_function=None,  # ì§ì ‘ embeddings ê³µê¸‰
        )
        print(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {args.collection_name}")

    print(f"ğŸ”¹ ì´ {total_vectors}ê°œ ë²¡í„°ë¥¼ ë°°ì¹˜({args.batch_size})ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
    
    # ì•ˆì •ì ì¸ id ë¬¸ìì—´ êµ¬ì„±
    def make_id(i: int, doc_id: str) -> str:
        return doc_id if (doc_id and isinstance(doc_id, str)) else f"combined_{i}"

    # ì—…ë¡œë“œ
    uploaded = 0
    for batch_indices in chunked(range(total_vectors), args.batch_size):
        ids = [make_id(i, all_doc_ids[i]) for i in batch_indices]
        batch_embeddings = combined_embeddings[batch_indices].astype("float32")
        batch_texts = [all_texts[i] for i in batch_indices]
        batch_metas = [all_metas[i] for i in batch_indices]

        # Chroma add: documents/embeddings/metadatas/ids ê¸¸ì´ëŠ” ë™ì¼í•´ì•¼ í•¨
        collection.add(
            ids=ids,
            embeddings=batch_embeddings.tolist(),  # list of list[float]
            documents=batch_texts,
            metadatas=batch_metas,
        )
        uploaded += len(batch_indices)
        print(f"  - ëˆ„ì  ì—…ë¡œë“œ: {uploaded}/{total_vectors}")

    # Persist ì €ì¥
    client.persist()
    print("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
    print(f"- To Chroma: {args.chroma_dir} (collection='{args.collection_name}')")
    print(f"- ì´ ì—…ë¡œë“œ: {uploaded} vectors")
    print(f"- ì²˜ë¦¬ëœ ì†ŒìŠ¤: {list(faiss_dirs.keys())}")


if __name__ == "__main__":
    main()

'''
ì•„ë˜ ì½”ë“œë¡œ ì‹¤í–‰
python migrate_faiss_to_chroma.py \
  --chroma-dir ./chroma_store \
  --collection-name my_collection \
  --gdrive-faiss-dir ./gdrive_faiss_index \
  --notion-faiss-dir ./notion_faiss_index \
  --github-faiss-dir ./github_faiss_index
'''