import os
import requests
import base64
import shutil
import json
import sys
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from typing import List

# --- Python ë²„ì „ í™•ì¸ ---
if sys.version_info < (3, 10):
    print("="*50)
    print("ğŸš¨ ì˜¤ë¥˜: ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Python 3.10 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
    print(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Python ë²„ì „ì€ {sys.version} ì…ë‹ˆë‹¤.")
    print("ê°€ìƒí™˜ê²½ì˜ íŒŒì´ì¬ ë²„ì „ì„ 3.10 ì´ìƒìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”.")
    print("="*50)
    sys.exit(1)

# --- ì„¤ì • (Configuration) ---
load_dotenv()
ORG_NAME = "YBIGTA"
FAISS_INDEX_PATH = "github_faiss_index"
PROCESSED_REPOS_CACHE = "processed_repos_cache.json"
GITHUB_TOKEN = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")

# --- ë§ì¶¤ ì„ë² ë”© í´ë˜ìŠ¤ (ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë¨) ---
class CustomUpstageEmbeddings(Embeddings):
    def __init__(self, model: str = "embedding-passage", chunk_size: int = 100):  # âœ… ìƒˆë¡œìš´ ëª¨ë¸ëª…
        self.model = model
        self.api_key = os.getenv("UPSTAGE_API_KEY")
        self.base_url = "https://api.upstage.ai/v1/embeddings"  # âœ… ì§ì ‘ API í˜¸ì¶œ
        self.chunk_size = chunk_size

    def _embed(self, texts: List[str]) -> List[List[float]]:
        if not self.api_key:
            raise ValueError(".env íŒŒì¼ì— UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        all_embeddings = []
        # APIì˜ í† í° ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(texts), self.chunk_size):
            batch = texts[i:i + self.chunk_size]
            
            headers = {"Authorization": f"Bearer {self.api_key}"}
            data = {"input": batch, "model": self.model}
            
            response = requests.post(self.base_url, headers=headers, json=data)
            
            if response.status_code == 200:
                response_data = response.json().get("data", [])
                batch_embeddings = [None] * len(batch)
                for item in response_data:
                    batch_embeddings[item['index']] = item['embedding']
                all_embeddings.extend(batch_embeddings)
            else:
                raise Exception(f"Upstage Embeddings API ì˜¤ë¥˜: {response.status_code} - {response.text}")
        
        return all_embeddings

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        query_model = self.model.replace("passage", "query")
        temp_model = self.model
        self.model = query_model
        embedding = self._embed([text])[0]
        self.model = temp_model
        return embedding


# --- Vector DB ì—…ë°ì´íŠ¸ ë° ì €ì¥ í•¨ìˆ˜ ---
def update_vectorstore(vectorstore, documents, embeddings):
    if not documents:
        return vectorstore
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)
    print(f"  - ğŸ’¾ READMEë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Vector DBì— ì¶”ê°€í•©ë‹ˆë‹¤...")
    if vectorstore is None:
        vectorstore = FAISS.from_documents(split_docs, embeddings)
    else:
        vectorstore.add_documents(split_docs)
    vectorstore.save_local(FAISS_INDEX_PATH)
    print("  - âœ… ì €ì¥ ì™„ë£Œ.")
    return vectorstore

# --- GitHub API í•¨ìˆ˜ ---
def get_all_repos_from_org(org_name):
    """ì¡°ì§ì˜ ëª¨ë“  ê³µê°œ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    repos = []
    page = 1
    headers = {"Accept": "application/vnd.github.v3+json"}
    if GITHUB_TOKEN:
        headers["Authorization"] = f"token {GITHUB_TOKEN}"

    while True:
        url = f"https://api.github.com/orgs/{org_name}/repos?type=public&page={page}&per_page=100"
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            data = response.json()
            if not data:
                break
            repos.extend(data)
            page += 1
        except requests.RequestException as e:
            print(f"ğŸš¨ GitHub APIì—ì„œ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
    return repos

def get_readme_content(repo_full_name):
    """GitHub APIë¥¼ í†µí•´ ë¦¬í¬ì§€í† ë¦¬ì˜ README íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    headers = {"Accept": "application/vnd.github.v3.raw"}
    if GITHUB_TOKEN:
        headers["Authorization"] = f"token {GITHUB_TOKEN}"
    
    readme_names = ["README.md", "README.rst", "README.txt", "readme.md"]
    for name in readme_names:
        url = f"https://api.github.com/repos/{repo_full_name}/contents/{name}"
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
        except requests.RequestException:
            continue
    return None

# --- ë©”ì¸ ë¡œì§ ---
print("="*50)
print("GitHub README ìˆ˜ì§‘ ë° Vector DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

embeddings = CustomUpstageEmbeddings(model="embedding-passage") # ìƒˆë¡œìš´ ëª¨ë¸ëª… ì‚¬ìš©
vectorstore = None
if os.path.exists(FAISS_INDEX_PATH):
    try:
        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ê¸°ì¡´ '{FAISS_INDEX_PATH}' ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        shutil.rmtree(FAISS_INDEX_PATH)
        vectorstore = None

if os.path.exists(PROCESSED_REPOS_CACHE):
    with open(PROCESSED_REPOS_CACHE, 'r', encoding='utf-8') as f:
        processed_repos = set(json.load(f))
    print(f"âœ… ê¸°ì¡´ ìºì‹œì—ì„œ {len(processed_repos)}ê°œì˜ ì²˜ë¦¬ëœ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
else:
    processed_repos = set()

try:
    print("="*50)
    print(f"ğŸ¢ {ORG_NAME} ì¡°ì§ì˜ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
    all_repos = get_all_repos_from_org(ORG_NAME)
    print(f"ğŸ” ì´ {len(all_repos)}ê°œì˜ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    for repo in all_repos:
        repo_name = repo['name']
        repo_full_name = repo['full_name']
        print("-" * 50)
        
        if repo_name in processed_repos:
            print(f"âš¡ï¸ ìºì‹œ íˆíŠ¸! '{repo_name}' ë¦¬í¬ì§€í† ë¦¬ëŠ” ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        print(f"ğŸš€ '{repo_name}' ë¦¬í¬ì§€í† ë¦¬ ì²˜ë¦¬ ì‹œì‘...")

        try:
            print(f"  - ğŸ“„ README.md íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤...")
            readme_content = get_readme_content(repo_full_name)

            if readme_content is None:
                print(f"  - âš ï¸ README íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ READMEë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                readme_content = f"# {repo_name}\n\nì´ ë¦¬í¬ì§€í† ë¦¬ì— ëŒ€í•œ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤."

            doc = Document(
                page_content=readme_content,
                metadata={'source': f"https://github.com/{repo_full_name}"}
            )
            
            vectorstore = update_vectorstore(vectorstore, [doc], embeddings)
            processed_repos.add(repo_name)

        except Exception as e:
            print(f"  - ğŸš¨ ì‹¤íŒ¨! '{repo_name}' ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

except Exception as e:
    print(f"\nğŸš¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

finally:
    print("="*50)
    print("ìµœì¢… ì •ë¦¬ ë° ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    with open(PROCESSED_REPOS_CACHE, 'w', encoding='utf-8') as f:
        json.dump(list(processed_repos), f, ensure_ascii=False, indent=4)
    print(f"âœ… ì—…ë°ì´íŠ¸ëœ ìºì‹œë¥¼ '{PROCESSED_REPOS_CACHE}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print("\nï¿½ï¿½ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")