import os
import requests
import subprocess
import shutil
import json
import sys
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.document_loaders import UnstructuredFileLoader
from typing import List

# --- ì„¤ì • (Configuration) ---
load_dotenv()
ORG_NAME = "YBIGTA"
TEMP_REPOS_DIR = "temp_github_repos"
GENERATED_READMES_DIR = "generated_readmes"
FAISS_INDEX_PATH = "github_faiss_index"
# ì´ë¯¸ ì²˜ë¦¬í•œ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ê¸°ë¡í•  ìºì‹œ íŒŒì¼
PROCESSED_REPOS_CACHE = "processed_repos_cache.json"

# --- ë§ì¶¤ ì„ë² ë”© í´ë˜ìŠ¤ ---
class CustomUpstageEmbeddings(Embeddings):
    def __init__(self, model: str = "embedding-passage", chunk_size: int = 100):
        self.model = model
        self.api_key = os.getenv("UPSTAGE_API_KEY")
        self.base_url = "https://api.upstage.ai/v1/embeddings"
        self.chunk_size = chunk_size

    def _embed(self, texts: List[str]) -> List[List[float]]:
        if not self.api_key:
            raise ValueError(".env íŒŒì¼ì— UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        all_embeddings = []
        for i in range(0, len(texts), self.chunk_size):
            batch = texts[i:i + self.chunk_size]
            headers = {"Authorization": f"Bearer {self.api_key}"}
            data = {"input": batch, "model": self.model}
            response = requests.post(self.base_url, headers=headers, json=data)
            
            if response.status_code == 200:
                response_data = response.json().get("data", [])
                batch_embeddings = [None] * len(batch)
                for item in response_data:
                    batch_embeddings[item['index']] = item['embedding']
                all_embeddings.extend(batch_embeddings)
            else:
                raise Exception(f"Upstage Embeddings API ì˜¤ë¥˜: {response.status_code} - {response.text}")
        
        return all_embeddings

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        query_model = self.model.replace("passage", "query")
        temp_model = self.model
        self.model = query_model
        embedding = self._embed([text])[0]
        self.model = temp_model
        return embedding

# --- Vector DB ì—…ë°ì´íŠ¸ ë° ì €ì¥ í•¨ìˆ˜ ---
def update_vectorstore(vectorstore, documents, embeddings):
    if not documents:
        return vectorstore

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)
    
    print(f"\nğŸ’¾ {len(documents)}ê°œì˜ READMEë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Vector DBì— ì¶”ê°€/ì €ì¥í•©ë‹ˆë‹¤...")
    
    if vectorstore is None:
        vectorstore = FAISS.from_documents(split_docs, embeddings)
    else:
        vectorstore.add_documents(split_docs)
    
    vectorstore.save_local(FAISS_INDEX_PATH)
    print("ğŸ’¾ ì €ì¥ ì™„ë£Œ.")
    return vectorstore

# --- ë©”ì¸ ë¡œì§ ---
print("="*50)
print("GitHub README ìƒì„± ë° Vector DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

# 1. í•„ìš” í´ë” ìƒì„±
os.makedirs(TEMP_REPOS_DIR, exist_ok=True)
os.makedirs(GENERATED_READMES_DIR, exist_ok=True)

# 2. ì„ë² ë”© ëª¨ë¸ ë° ê¸°ì¡´ DB/ìºì‹œ ë¡œë“œ
embeddings = CustomUpstageEmbeddings(model="embedding-passage")
vectorstore = None
if os.path.exists(FAISS_INDEX_PATH):
    try:
        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ê¸°ì¡´ '{FAISS_INDEX_PATH}' ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        shutil.rmtree(FAISS_INDEX_PATH)

if os.path.exists(PROCESSED_REPOS_CACHE):
    with open(PROCESSED_REPOS_CACHE, 'r', encoding='utf-8') as f:
        processed_repos = set(json.load(f))
    print(f"âœ… ê¸°ì¡´ ìºì‹œ íŒŒì¼ì—ì„œ {len(processed_repos)}ê°œì˜ ì²˜ë¦¬ëœ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
else:
    processed_repos = set()

# 3. GitHub ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_all_repos_from_org(org_name):
    repos = []
    page = 1
    github_token = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
    headers = {"Accept": "application/vnd.github.v3+json"}
    if github_token:
        headers["Authorization"] = f"token {github_token}"

    while True:
        url = f"https://api.github.com/orgs/{org_name}/repos?type=public&page={page}&per_page=100"
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        if not data:
            break
        repos.extend([(repo['clone_url'], repo['name']) for repo in data])
        page += 1
    return repos

try:
    print("="*50)
    print(f"ğŸ¢ {ORG_NAME} ì¡°ì§ì˜ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
    all_repos = get_all_repos_from_org(ORG_NAME)
    print(f"ğŸ” ì´ {len(all_repos)}ê°œì˜ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    newly_processed_docs = []

    for repo_url, repo_name in all_repos:
        print("-" * 50)
        
        if repo_name in processed_repos:
            print(f"âš¡ï¸ ìºì‹œ íˆíŠ¸! '{repo_name}' ë¦¬í¬ì§€í† ë¦¬ëŠ” ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        print(f"ğŸš€ '{repo_name}' ë¦¬í¬ì§€í† ë¦¬ ì²˜ë¦¬ ì‹œì‘...")
        repo_path = os.path.join(TEMP_REPOS_DIR, repo_name)
        output_readme_path = os.path.join(GENERATED_READMES_DIR, f"{repo_name}_README.md")

        try:
            # 1. Git Clone
            print(f"  - ğŸ“‚ '{repo_name}'ë¥¼ ë³µì œí•©ë‹ˆë‹¤...")
            if os.path.exists(repo_path):
                shutil.rmtree(repo_path)
            subprocess.run(["git", "clone", "--depth", "1", repo_url, repo_path], check=True, capture_output=True)

            # 2. readme-ai ì‹¤í–‰ (Upstage Solar ëª¨ë¸ ì‚¬ìš©)
            print(f"  - ğŸ¤– readme-ai(Upstage Solar)ë¡œ READMEë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            # ğŸ‘‡ [ìˆ˜ì •] 'readme-ai' ì§ì ‘ í˜¸ì¶œ ëŒ€ì‹  'python -m readmeai.cli.main'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            readme_command = [
                sys.executable,
                "-m",
                "readmeai.cli.main",
                "--model", "solar-1-mini", 
                "--endpoint", "https://api.upstage.ai/v1/chat/completions",
                "--repository", repo_path,
                "--output", output_readme_path
            ]
            result = subprocess.run(readme_command, capture_output=True, text=True, check=True)
            print(f"  - âœ… README ìƒì„± ì„±ê³µ!")

            # 3. ìƒì„±ëœ README ë¡œë“œ
            loader = UnstructuredFileLoader(output_readme_path)
            doc = loader.load()[0]
            doc.metadata['source'] = f"https://github.com/{ORG_NAME}/{repo_name}"
            newly_processed_docs.append(doc)

            # 4. ì²˜ë¦¬ ì™„ë£Œ í›„ ìºì‹œì— ê¸°ë¡ ë° DBì— ì¦‰ì‹œ ì €ì¥
            processed_repos.add(repo_name)
            vectorstore = update_vectorstore(vectorstore, [doc], embeddings)

        except subprocess.CalledProcessError as e:
            print(f"  - ğŸš¨ ì‹¤íŒ¨! '{repo_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            print("  - ì˜¤ë¥˜ ë‚´ìš©:", e.stderr)
        except Exception as e:
            print(f"  - ğŸš¨ ì‹¤íŒ¨! '{repo_name}' ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # 5. ì„ì‹œ í´ë¡  í´ë” ì‚­ì œ
            if os.path.exists(repo_path):
                print(f"  - ğŸ§¹ '{repo_name}' ì„ì‹œ í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
                shutil.rmtree(repo_path)

except Exception as e:
    print(f"\nğŸš¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

finally:
    # 6. ìµœì¢… ìºì‹œ íŒŒì¼ ì €ì¥
    print("="*50)
    print("ìµœì¢… ì •ë¦¬ ë° ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    with open(PROCESSED_REPOS_CACHE, 'w', encoding='utf-8') as f:
        json.dump(list(processed_repos), f, ensure_ascii=False, indent=4)
    print(f"âœ… ì—…ë°ì´íŠ¸ëœ ìºì‹œë¥¼ '{PROCESSED_REPOS_CACHE}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
