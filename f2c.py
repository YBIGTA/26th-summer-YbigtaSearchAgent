# migrate_faiss_to_chroma.py
# -*- coding: utf-8 -*-

import os
import json
import math
import argparse
from typing import Any, Dict, List

import faiss
from chromadb import PersistentClient
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain_core.embeddings import Embeddings


class _NoopEmbeddings(Embeddings):
    """load_localìš© ë”ë¯¸ ì„ë² ë”© (ì‹¤ì œ ì„ë² ë”© ì¬ê³„ì‚° ì•ˆ í•¨)"""
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        raise RuntimeError("ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì„ë² ë”©ì„ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    def embed_query(self, text: str) -> List[float]:
        raise RuntimeError("ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì„ë² ë”©ì„ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


def _scrub_metadata(meta: Dict[str, Any]) -> Dict[str, Any]:
    """Chromaê°€ JSON-serializableë§Œ ë°›ìœ¼ë¯€ë¡œ, ë¹„ì§ë ¬í™” í•„ë“œëŠ” strë¡œ ë³€í™˜"""
    out = {}
    for k, v in (meta or {}).items():
        try:
            json.dumps(v)
            out[k] = v
        except Exception:
            out[k] = str(v)
    return out


def main():
    parser = argparse.ArgumentParser(description="FAISS(VectorStore) â†’ ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--faiss_dir", required=True, help="LC FAISSê°€ ì €ì¥ëœ ë””ë ‰í„°ë¦¬ (ì˜ˆ: notion_faiss_index)")
    parser.add_argument("--chroma_dir", required=True, help="Chroma persist ë””ë ‰í„°ë¦¬")
    parser.add_argument("--collection", required=True, help="Chroma ì»¬ë ‰ì…˜ ì´ë¦„")
    parser.add_argument("--batch", type=int, default=1024, help="ì¶”ê°€(add) ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--space", choices=["cosine", "l2", "ip"], default="cosine",
                        help="Chroma HNSW distance space (ê¸°ë³¸: cosine)")
    parser.add_argument("--normalize", action="store_true",
                        help="ì½”ì‚¬ì¸ ìœ ì‚¬ë„(space=cosine) ì‚¬ìš© ì‹œ ë²¡í„°ë¥¼ L2 ì •ê·œí™”í•˜ì—¬ ì‚½ì…")
    args = parser.parse_args()

    # 1) LC FAISS ì—´ê¸° (ì„ë² ë”© ì¬ê³„ì‚° ì—†ìŒ)
    print(f"ğŸ“¦ FAISS ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {args.faiss_dir}")
    vs: LCFAISS = LCFAISS.load_local(
        args.faiss_dir, _NoopEmbeddings(), allow_dangerous_deserialization=True
    )

    faiss_index = vs.index
    mapping = vs.index_to_docstore_id     # { int_index -> doc_id(str) }
    docstore = vs.docstore                # ë¬¸ì„œ ì €ì¥ì†Œ

    if not hasattr(faiss_index, "reconstruct"):
        raise RuntimeError(
            "ì´ FAISS ì¸ë±ìŠ¤ëŠ” reconstructë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "- ë³´í†µ LangChain FAISS ê¸°ë³¸ì€ IndexFlatL2ë¼ reconstruct ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "- IVF/PQ ë“±ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤ë©´, ì›ë³¸ ë²¡í„° ì—†ì´ ì •í™• ë³µì›ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´ìš”.\n"
            "  (ê°€ëŠ¥í•˜ë©´ IndexFlat ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ë¹Œë“œí•˜ê±°ë‚˜, ì„ë² ë”©ì„ ë”°ë¡œ ë³´ê´€í–ˆë‹¤ê°€ ì‚¬ìš©í•˜ì„¸ìš”.)"
        )

    # 2) Chroma í´ë¼ì´ì–¸íŠ¸/ì»¬ë ‰ì…˜ ì¤€ë¹„
    print(f"ğŸ’¾ ChromaDB ì´ˆê¸°í™”: {args.chroma_dir} (collection: {args.collection}, space: {args.space})")
    client = PersistentClient(path=args.chroma_dir)
    # ê±°ë¦¬í•¨ìˆ˜ëŠ” ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„°ì˜ hnsw:spaceë¡œ ì§€ì •
    coll = client.get_or_create_collection(
        name=args.collection,
        metadata={"hnsw:space": args.space}
    )

    # 3) FAISS â†’ Chroma ì´ì‹
    positions = sorted(mapping.keys())  # ì¸ë±ìŠ¤ í¬ì§€ì…˜ ì •ë ¬
    total = len(positions)
    print(f"ğŸšš ì´ì „í•  ë²¡í„°/ë¬¸ì„œ ê°œìˆ˜: {total}")

    def l2_normalize(vec: List[float]) -> List[float]:
        if not args.normalize:
            return vec
        import numpy as np
        v = np.array(vec, dtype="float32")
        n = np.linalg.norm(v)
        return (v / (n + 1e-12)).tolist()

    # ë°°ì¹˜ ì¶”ê°€
    for start in range(0, total, args.batch):
        end = min(start + args.batch, total)
        batch_pos = positions[start:end]

        ids: List[str] = []
        docs: List[str] = []
        metas: List[Dict[str, Any]] = []
        embs: List[List[float]] = []

        for pos in batch_pos:
            doc_id = mapping[pos]
            doc = docstore.search(doc_id)  # langchain.schema.Document
            text = doc.page_content if hasattr(doc, "page_content") else str(doc)
            meta = _scrub_metadata(getattr(doc, "metadata", {}) or {})

            vec = faiss_index.reconstruct(pos)  # numpy array(float32)
            vec_list = [float(x) for x in vec]  # to Python list
            vec_list = l2_normalize(vec_list)

            ids.append(str(doc_id))
            docs.append(text)
            metas.append(meta)
            embs.append(vec_list)

        print(f"  â• {start:,} ~ {end-1:,} ì¶”ê°€ ì¤‘...")
        coll.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)

    # 4) ë””ìŠ¤í¬ì— ì €ì¥
    print("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ! â†’", os.path.abspath(args.chroma_dir))

    # 5) ê°„ë‹¨ ê²€ì¦
    print(f"âœ… ì»¬ë ‰ì…˜ '{args.collection}' ë¬¸ì„œ ìˆ˜ (ì¶”ì •): {coll.count()}")
    print("   ì´ì œ Chroma ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰/ì„œë¹™ ì½”ë“œë¥¼ êµì²´í•˜ë©´ ë©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()


# python f2c.py --faiss_dir notion_faiss_index --normalize

'''
python f2c.py \
  --chroma-dir ./chroma_all \
  --collection-name all \
  --gdrive-faiss-dir ./gdrive_faiss_index \
  --notion-faiss-dir ./notion_faiss_index \
  --github-faiss-dir ./github_faiss_index \
'''