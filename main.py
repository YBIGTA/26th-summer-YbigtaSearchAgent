# main.py (í”¼ë“œë°± ê¸°ëŠ¥ ì¶”ê°€ ë²„ì „)

from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Tuple
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from openai import OpenAI
import csv
from datetime import datetime
import os

# --- (ê¸°ì¡´ setup ë° app ì´ˆê¸°í™” ì½”ë“œëŠ” ë™ì¼) ---
load_dotenv()

# Upstage API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
upstage_client = OpenAI(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1"
)

# ì»¤ìŠ¤í…€ Upstage ì„ë² ë”© í´ë˜ìŠ¤
class UpstageEmbeddings:
    def __init__(self, client, model="embedding-query"):
        self.client = client
        self.model = model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        embeddings = []
        for text in texts:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            embeddings.append(response.data[0].embedding)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        response = self.client.embeddings.create(
            input=text,
            model=self.model
        )
        return response.data[0].embedding

# Upstage ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
embeddings = UpstageEmbeddings(upstage_client)
vectorstore = FAISS.load_local("notion_faiss_index", embeddings, allow_dangerous_deserialization=True)
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever()
)
app = FastAPI()

class ConversationRequest(BaseModel):
    query: str
    chat_history: List[Tuple[str, str]] = []

# --- (ê¸°ì¡´ /conversation ê²½ë¡œëŠ” ë™ì¼) ---
@app.post("/conversation")
def ask_conversation(request: ConversationRequest):
    result = qa_chain.invoke({
        "question": request.query,
        "chat_history": request.chat_history
    })
    return {"answer": result.get("answer")}

# --- ğŸ‘‡ ìƒˆë¡œìš´ í”¼ë“œë°± API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ ğŸ‘‡ ---
class FeedbackRequest(BaseModel):
    query: str
    answer: str
    feedback: str # "good" or "bad"

@app.post("/feedback")
def receive_feedback(request: FeedbackRequest):
    """
    ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ 'feedback_log.csv' íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    try:
        with open("feedback_log.csv", "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            # íŒŒì¼ì´ ë¹„ì–´ìˆìœ¼ë©´ í—¤ë”ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
            if f.tell() == 0:
                writer.writerow(["timestamp", "query", "answer", "feedback"])

            # ë°ì´í„° í–‰ì„ ì‘ì„±í•©ë‹ˆë‹¤.
            writer.writerow([
                datetime.now().isoformat(),
                request.query,
                request.answer,
                request.feedback
            ])
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "detail": str(e)}

@app.get("/")
def read_root():
    return {"message": "YBIGTA Conversational RAG Agent API is running!"}