# main.py

import os # os ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Tuple
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
import csv
from datetime import datetime
import logging
from logging.handlers import RotatingFileHandler
from langchain_upstage import UpstageEmbeddings
from langchain.retrievers import EnsembleRetriever 

# --- ì„¤ì •(Setup) ë¶€ë¶„ ---
load_dotenv()

# ì„ë² ë”© ëª¨ë¸ì€ ê¸°ì¡´ OpenAI ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì°¸ê³  ì‚¬í•­ í™•ì¸)
embeddings = UpstageEmbeddings(    
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="embedding-query"
)

# ğŸ‘‡ LLM(ì–¸ì–´ ëª¨ë¸)ì„ Upstage Solarë¡œ ë³€ê²½í•˜ëŠ” ë¶€ë¶„
llm = ChatOpenAI(
    model_name="solar-pro2",
    temperature=0,
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1"
)

# ë‘ ê°œì˜ Vectorstore ë¶ˆëŸ¬ì˜¤ê¸°
notion_vs = FAISS.load_local("notion_faiss_index", embeddings, allow_dangerous_deserialization=True)
gdrive_vs = FAISS.load_local("gdrive_faiss_index", embeddings, allow_dangerous_deserialization=True)

# retrieverë¥¼ Ensembleë¡œ ë¬¶ê¸°
retriever = EnsembleRetriever(
    retrievers=[
        notion_vs.as_retriever(search_kwargs={"k": 3}),
        gdrive_vs.as_retriever(search_kwargs={"k": 3})
    ],
    weights=[0.5, 0.5]   # ğŸ‘ˆ í•„ìš”í•˜ë©´ ê°€ì¤‘ì¹˜ ì¡°ì • (ì˜ˆ: [0.7, 0.3])
)

# Conversational Retrieval Chain êµ¬ì„±
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever
)


# --- ë¡œê¹… ì„¤ì • ---
logger = logging.getLogger("rag-logger")
logger.setLevel(logging.INFO)
handler = RotatingFileHandler("rag_usage.log", maxBytes=5_000_000, backupCount=3, encoding="utf-8")
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

app = FastAPI()

class ConversationRequest(BaseModel):
    query: str
    chat_history: List[Tuple[str, str]] = []

@app.post("/conversation")
def ask_conversation(request: ConversationRequest):
    result = qa_chain.invoke({
        "question": request.query,
        "chat_history": request.chat_history
    })
    answer = result.get("answer")

    # --- í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹… ---
    metadata = result.get("source_documents", None)  # ë³´í†µ ì—¬ê¸°ì— ë¬¸ì„œ ì¶œì²˜
    response_metadata = result.get("response_metadata", {})  # ì—¬ê¸°ì— usage ì •ë³´ê°€ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìŒ
    usage = response_metadata.get("token_usage") or response_metadata.get("usage")

    logger.info(
        f"query='{request.query}' | answer_len={len(answer)} "
        f"| usage={usage}"
    )
    return {"answer": answer}

# --- (í”¼ë“œë°± API ë° ë£¨íŠ¸ ê²½ë¡œëŠ” ê¸°ì¡´ê³¼ ë™ì¼) ---
class FeedbackRequest(BaseModel):
    query: str
    answer: str
    feedback: str # "good" or "bad"

@app.post("/feedback")
def receive_feedback(request: FeedbackRequest):
    try:
        with open("feedback_log.csv", "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            if f.tell() == 0:
                writer.writerow(["timestamp", "query", "answer", "feedback"])
            writer.writerow([
                datetime.now().isoformat(),
                request.query,
                request.answer,
                request.feedback
            ])
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "detail": str(e)}

@app.get("/")
def read_root():
    return {"message": "YBIGTA Conversational RAG Agent API is running!"}