import json
from typing import List, Dict, Any
from langchain_community.vectorstores import FAISS
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import numpy as np

class FAISSToElasticsearchConverter:
    def __init__(self, faiss_index_path: str, es_host: str = "localhost", es_port: int = 9200):
        """FAISS ì¸ë±ìŠ¤ë¥¼ Elasticsearchë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
        self.faiss_index_path = faiss_index_path
        self.es_client = Elasticsearch([{'host': es_host, 'port': es_port}])
        self.index_name = "notion_documents"
        
    def load_faiss_data(self) -> Dict[str, Any]:
        """FAISS ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            vectorstore = FAISS.load_local(self.faiss_index_path, allow_dangerous_deserialization=True)
            
            # ë¬¸ì„œì™€ ë²¡í„° ì¶”ì¶œ
            documents = list(vectorstore.docstore._dict.values())
            embeddings = vectorstore.index.reconstruct_n(0, len(documents))
            
            print(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œì™€ {len(embeddings)}ê°œì˜ ë²¡í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
            return {
                'documents': documents,
                'embeddings': embeddings
            }
            
        except Exception as e:
            print(f"âŒ FAISS ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def create_elasticsearch_index(self, mapping: Dict[str, Any] = None):
        """Elasticsearch ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if mapping is None:
            # ê¸°ë³¸ ë§¤í•‘ ì„¤ì •
            mapping = {
                "mappings": {
                    "properties": {
                        "content": {
                            "type": "text",
                            "analyzer": "standard",
                            "search_analyzer": "standard"
                        },
                        "title": {
                            "type": "text",
                            "analyzer": "standard"
                        },
                        "source": {
                            "type": "keyword"
                        },
                        "page_id": {
                            "type": "keyword"
                        },
                        "block_count": {
                            "type": "integer"
                        },
                        "embedding": {
                            "type": "dense_vector",
                            "dims": 4096,  # Upstage Solar ì„ë² ë”© ì°¨ì›
                            "index": True,
                            "similarity": "cosine"
                        },
                        "created_at": {
                            "type": "date"
                        }
                    }
                },
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "index": {
                        "knn": True,
                        "knn.algo_param.ef_search": 100
                    }
                }
            }
        
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚­ì œ
            if self.es_client.indices.exists(index=self.index_name):
                self.es_client.indices.delete(index=self.index_name)
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œë¨")
            
            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self.es_client.indices.create(index=self.index_name, body=mapping)
            print(f"âœ… Elasticsearch ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„±ë¨")
            
        except Exception as e:
            print(f"âŒ Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def convert_to_elasticsearch(self, batch_size: int = 100):
        """FAISS ë°ì´í„°ë¥¼ Elasticsearchë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # FAISS ë°ì´í„° ë¡œë“œ
            data = self.load_faiss_data()
            documents = data['documents']
            embeddings = data['embeddings']
            
            # Elasticsearch ì¸ë±ìŠ¤ ìƒì„±
            self.create_elasticsearch_index()
            
            # ë°°ì¹˜ë¡œ ë°ì´í„° ì‚½ì…
            actions = []
            for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
                action = {
                    "_index": self.index_name,
                    "_id": i,
                    "_source": {
                        "content": doc.page_content,
                        "title": doc.metadata.get("title", "ì œëª© ì—†ìŒ"),
                        "source": doc.metadata.get("source", ""),
                        "page_id": doc.metadata.get("page_id", ""),
                        "block_count": doc.metadata.get("block_count", 0),
                        "embedding": embedding.tolist(),
                        "created_at": "2024-01-01T00:00:00Z"  # ê¸°ë³¸ê°’
                    }
                }
                actions.append(action)
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì‚½ì…
                if len(actions) >= batch_size:
                    success, failed = bulk(self.es_client, actions, refresh=True)
                    print(f"ğŸ“¦ ë°°ì¹˜ ì‚½ì…: ì„±ê³µ {success}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
                    actions = []
            
            # ë‚¨ì€ ë°ì´í„° ì‚½ì…
            if actions:
                success, failed = bulk(self.es_client, actions, refresh=True)
                print(f"ğŸ“¦ ë§ˆì§€ë§‰ ë°°ì¹˜ ì‚½ì…: ì„±ê³µ {success}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
            
            print(f"ğŸ‰ ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ Elasticsearchë¡œ ë³€í™˜ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ Elasticsearch ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def search_by_text(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            response = self.es_client.search(
                index=self.index_name,
                body={
                    "query": {
                        "multi_match": {
                            "query": query,
                            "fields": ["content^2", "title"],
                            "type": "best_fields",
                            "fuzziness": "AUTO"
                        }
                    },
                    "size": top_k,
                    "_source": ["content", "title", "source", "page_id"]
                }
            )
            
            results = []
            for hit in response['hits']['hits']:
                results.append({
                    'score': hit['_score'],
                    'content': hit['_source']['content'],
                    'title': hit['_source']['title'],
                    'source': hit['_source']['source'],
                    'page_id': hit['_source']['page_id']
                })
            
            return results
            
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_vector(self, query_vector: List[float], top_k: int = 5) -> List[Dict[str, Any]]:
        """ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            response = self.es_client.search(
                index=self.index_name,
                body={
                    "query": {
                        "knn": {
                            "embedding": {
                                "vector": query_vector,
                                "k": top_k
                            }
                        }
                    },
                    "size": top_k,
                    "_source": ["content", "title", "source", "page_id"]
                }
            )
            
            results = []
            for hit in response['hits']['hits']:
                results.append({
                    'score': hit['_score'],
                    'content': hit['_source']['content'],
                    'title': hit['_source']['title'],
                    'source': hit['_source']['source'],
                    'page_id': hit['_source']['page_id']
                })
            
            return results
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def hybrid_search(self, query: str, query_vector: List[float], top_k: int = 5, 
                     text_weight: float = 0.3, vector_weight: float = 0.7) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì™€ ë²¡í„°ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            response = self.es_client.search(
                index=self.index_name,
                body={
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["content^2", "title"],
                                        "type": "best_fields",
                                        "fuzziness": "AUTO"
                                    }
                                },
                                {
                                    "knn": {
                                        "embedding": {
                                            "vector": query_vector,
                                            "k": top_k
                                        }
                                    }
                                }
                            ]
                        }
                    },
                    "size": top_k,
                    "_source": ["content", "title", "source", "page_id"]
                }
            )
            
            results = []
            for hit in response['hits']['hits']:
                results.append({
                    'score': hit['_score'],
                    'content': hit['_source']['content'],
                    'title': hit['_source']['title'],
                    'source': hit['_source']['source'],
                    'page_id': hit['_source']['page_id']
                })
            
            return results
            
        except Exception as e:
            print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # FAISSì—ì„œ Elasticsearchë¡œ ë³€í™˜
    converter = FAISSToElasticsearchConverter("notion_faiss_index")
    
    # ë³€í™˜ ì‹¤í–‰
    converter.convert_to_elasticsearch()
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” í…ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    results = converter.search_by_text("YBIGTA", top_k=3)
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['title']} (ì ìˆ˜: {result['score']:.2f})")
        print(f"   ë‚´ìš©: {result['content'][:100]}...")
        print()
    
    # ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì˜ˆì‹œ ë²¡í„°)
    print("ğŸ” ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    example_vector = [0.1] * 4096  # ì˜ˆì‹œ ë²¡í„°
    results = converter.search_by_vector(example_vector, top_k=3)
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['title']} (ì ìˆ˜: {result['score']:.2f})")
        print(f"   ë‚´ìš©: {result['content'][:100]}...")
        print() 