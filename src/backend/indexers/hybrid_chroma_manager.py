"""
HybridChromaManager - ê¸°ì¡´ unified_chroma_db + ìƒˆë¡œìš´ ì¦ë¶„ ë°ì´í„° í†µí•© ê´€ë¦¬

- unified_chroma_db: ì½ê¸° ì „ìš©ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° í™œìš©
- incremental_chroma_db: ìƒˆë¡œìš´ ë³€ê²½ì‚¬í•­ë§Œ ê´€ë¦¬
- ê²€ìƒ‰ ì‹œ ë‘ DB ê²°ê³¼ë¥¼ íˆ¬ëª…í•˜ê²Œ ë³‘í•©
"""

import os
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from langchain_core.documents import Document

from .unified_chroma_adapter import UnifiedChromaAdapter
from .chroma_index import ChromaIndexManager


class HybridChromaManager:
    """Unified + Incremental ChromaDB í•˜ì´ë¸Œë¦¬ë“œ ê´€ë¦¬ì"""
    
    def __init__(self, 
                 unified_db_path: str = "data/unified_chroma_db/unified_chroma_db",
                 incremental_db_path: str = "data/indexes/incremental_chroma_db"):
        
        # ì–´ëŒ‘í„° ì´ˆê¸°í™” (ê¸°ì¡´ ë°ì´í„°ìš©)
        self.unified_adapter = UnifiedChromaAdapter(unified_db_path)
        
        # ì¦ë¶„ ë§¤ë‹ˆì € ì´ˆê¸°í™” (ìƒˆ ë°ì´í„°ìš©) 
        self.incremental_manager = ChromaIndexManager(incremental_db_path)
        
        # ì™¸ë¶€ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
        self.external_metadata_file = os.path.join(
            os.path.dirname(incremental_db_path), "hybrid_document_metadata.json"
        )
        self.hybrid_metadata = self._load_hybrid_metadata()
        
        self.available = self.unified_adapter.available or self.incremental_manager.available
        
        print(f"ğŸ”— í•˜ì´ë¸Œë¦¬ë“œ ChromaDB ì´ˆê¸°í™”")
        print(f"  - Unified DB: {'âœ…' if self.unified_adapter.available else 'âŒ'}")
        print(f"  - Incremental DB: {'ì¤€ë¹„ë¨' if self.incremental_manager.available else 'âŒ'}")
    
    def initialize(self, embeddings):
        """ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ChromaDB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì¦ë¶„ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        if self.incremental_manager.available:
            self.incremental_manager.initialize(embeddings)
        
        # í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±
        self._merge_metadata_sources()
        
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ChromaDB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_hybrid_metadata(self) -> Dict[str, Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.external_metadata_file):
            try:
                with open(self.external_metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    def _save_hybrid_metadata(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(self.external_metadata_file), exist_ok=True)
            with open(self.external_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.hybrid_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _merge_metadata_sources(self):
        """unified + incremental ë©”íƒ€ë°ì´í„° í†µí•©"""
        print("ğŸ”„ ë©”íƒ€ë°ì´í„° ì†ŒìŠ¤ í†µí•© ì¤‘...")
        
        # Unified DB ë©”íƒ€ë°ì´í„° ë³‘í•©
        if self.unified_adapter.available:
            unified_metadata = self.unified_adapter.get_virtual_metadata()
            for doc_id, metadata in unified_metadata.items():
                if doc_id not in self.hybrid_metadata:
                    self.hybrid_metadata[doc_id] = {
                        **metadata,
                        'storage': 'unified',
                        'last_synced': datetime.now().isoformat()
                    }
        
        # Incremental DB ë©”íƒ€ë°ì´í„° ë³‘í•©
        if (hasattr(self.incremental_manager, 'document_metadata') and 
            self.incremental_manager.document_metadata):
            for doc_id, metadata in self.incremental_manager.document_metadata.items():
                self.hybrid_metadata[doc_id] = {
                    **metadata,
                    'storage': 'incremental',
                    'last_synced': datetime.now().isoformat()
                }
        
        # í†µí•© ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_hybrid_metadata()
        
        print(f"âœ… ì´ {len(self.hybrid_metadata)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í†µí•© ì™„ë£Œ")
    
    def check_document_updates(self, documents: List[Document]) -> Tuple[List[Document], List[str]]:
        """í•˜ì´ë¸Œë¦¬ë“œ í™˜ê²½ì—ì„œ ë¬¸ì„œ ì—…ë°ì´íŠ¸ í™•ì¸"""
        new_or_updated_docs = []
        deleted_doc_ids = []
        current_doc_ids = set()
        
        for doc in documents:
            # ë¬¸ì„œ ID ìƒì„± (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
            source = doc.metadata.get('source', 'unknown')
            page_id = doc.metadata.get('page_id', None)
            
            if page_id:
                doc_id = f"{source}:{page_id}"
            else:
                doc_id = f"{source}:{hashlib.md5(source.encode()).hexdigest()[:8]}"
            
            current_doc_ids.add(doc_id)
            
            # ë¬¸ì„œ í•´ì‹œ ê³„ì‚°
            hash_content = doc.page_content + str(source) + str(doc.metadata.get('last_modified', ''))
            doc_hash = hashlib.sha256(hash_content.encode()).hexdigest()
            
            # ë©”íƒ€ë°ì´í„°ì— IDì™€ í•´ì‹œ ì¶”ê°€
            doc.metadata['doc_id'] = doc_id
            doc.metadata['content_hash'] = doc_hash
            doc.metadata['indexed_at'] = datetime.now().isoformat()
            
            # ê¸°ì¡´ ë¬¸ì„œì™€ ë¹„êµ
            if doc_id in self.hybrid_metadata:
                existing_hash = self.hybrid_metadata[doc_id].get('content_hash')
                existing_storage = self.hybrid_metadata[doc_id].get('storage', 'unified')
                
                if existing_hash != doc_hash:
                    print(f"ğŸ”„ ì—…ë°ì´íŠ¸ ê°ì§€: {doc_id} (ê¸°ì¡´: {existing_storage})")
                    new_or_updated_docs.append(doc)
                    
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    self.hybrid_metadata[doc_id] = {
                        'content_hash': doc_hash,
                        'last_updated': datetime.now().isoformat(),
                        'source': doc.metadata.get('source'),
                        'title': doc.metadata.get('title', 'Unknown'),
                        'storage': 'incremental',  # ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œëŠ” incrementalë¡œ ì´ë™
                        'previous_storage': existing_storage
                    }
            else:
                print(f"â• ìƒˆ ë¬¸ì„œ ê°ì§€: {doc_id}")
                new_or_updated_docs.append(doc)
                
                # ìƒˆ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                self.hybrid_metadata[doc_id] = {
                    'content_hash': doc_hash,
                    'last_updated': datetime.now().isoformat(),
                    'source': doc.metadata.get('source'),
                    'title': doc.metadata.get('title', 'Unknown'),
                    'storage': 'incremental'
                }
        
        # ì‚­ì œëœ ë¬¸ì„œ ì°¾ê¸° (í˜„ì¬ ì†ŒìŠ¤ì—ì„œ)
        if documents:
            source = documents[0].metadata.get('source')
            if source:
                for doc_id, metadata in list(self.hybrid_metadata.items()):
                    if (metadata.get('source') == source and 
                        doc_id not in current_doc_ids):
                        print(f"â– ì‚­ì œ ê°ì§€: {doc_id}")
                        deleted_doc_ids.append(doc_id)
                        del self.hybrid_metadata[doc_id]
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_hybrid_metadata()
        
        return new_or_updated_docs, deleted_doc_ids
    
    def add_documents(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        """ë¬¸ì„œë¥¼ ì¦ë¶„ DBì— ì¶”ê°€"""
        if not documents:
            return
        
        print(f"ğŸ“„ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì— {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")
        
        # ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ë¬¸ì„œ í™•ì¸
        new_or_updated_docs, deleted_doc_ids = self.check_document_updates(documents)
        
        # ì¦ë¶„ ë§¤ë‹ˆì €ë¥¼ í†µí•´ ì¶”ê°€
        if self.incremental_manager.available and new_or_updated_docs:
            self.incremental_manager.add_documents(new_or_updated_docs, chunk_size, chunk_overlap)
        
        # ì‚­ì œ ì²˜ë¦¬ (incremental DBì—ì„œë§Œ)
        if deleted_doc_ids and self.incremental_manager.collection:
            for doc_id in deleted_doc_ids:
                try:
                    self.incremental_manager.collection.delete(where={"doc_id": doc_id})
                    print(f"ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œ: {doc_id}")
                except Exception as e:
                    print(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ {doc_id}: {e}")
    
    def vector_search(self, query: str, top_k: int = 5, filter: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ê²€ìƒ‰ (unified + incremental)"""
        all_results = []
        
        # Unified DB ê²€ìƒ‰
        if self.unified_adapter.available:
            try:
                unified_results = self.unified_adapter.search_documents(query, top_k)
                all_results.extend(unified_results)
            except Exception as e:
                print(f"âš ï¸ Unified DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # Incremental DB ê²€ìƒ‰
        if self.incremental_manager.available:
            try:
                incremental_results = self.incremental_manager.vector_search(query, top_k, filter)
                all_results.extend(incremental_results)
            except Exception as e:
                print(f"âš ï¸ Incremental DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        unique_results = {}
        for result in all_results:
            doc_id = result.get('metadata', {}).get('doc_id')
            if doc_id:
                # ë†’ì€ ì ìˆ˜ì˜ ê²°ê³¼ë§Œ ìœ ì§€
                if doc_id not in unique_results or result['score'] > unique_results[doc_id]['score']:
                    unique_results[doc_id] = result
        
        # ì ìˆ˜ ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
        sorted_results = sorted(unique_results.values(), key=lambda x: x['score'], reverse=True)
        return sorted_results[:top_k]
    
    def sync_source(self, source: str, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        """íŠ¹ì • ì†ŒìŠ¤ ë™ê¸°í™” (ì¦ë¶„ DBë¡œ)"""
        print(f"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ - {source} ì†ŒìŠ¤ ë™ê¸°í™” ì‹œì‘...")
        
        # ì†ŒìŠ¤ë³„ë¡œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        for doc in documents:
            doc.metadata['source'] = source
            doc.metadata['sync_timestamp'] = datetime.now().isoformat()
        
        # ë¬¸ì„œ ì¶”ê°€/ì—…ë°ì´íŠ¸
        self.add_documents(documents, chunk_size, chunk_overlap)
        
        print(f"âœ… {source} ì†ŒìŠ¤ ë™ê¸°í™” ì™„ë£Œ")
    
    def get_statistics(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í†µê³„"""
        stats = {
            "status": "hybrid_initialized",
            "unified_available": self.unified_adapter.available,
            "incremental_available": self.incremental_manager.available,
            "total_documents": len(self.hybrid_metadata),
            "storage_distribution": {}
        }
        
        # ì €ì¥ì†Œë³„ ë¶„í¬
        for metadata in self.hybrid_metadata.values():
            storage = metadata.get('storage', 'unknown')
            stats["storage_distribution"][storage] = stats["storage_distribution"].get(storage, 0) + 1
        
        # Unified DB í†µê³„ ì¶”ê°€
        if self.unified_adapter.available:
            unified_stats = self.unified_adapter.get_source_statistics()
            stats["unified_stats"] = unified_stats
        
        # Incremental DB í†µê³„ ì¶”ê°€
        if self.incremental_manager.available:
            incremental_stats = self.incremental_manager.get_statistics()
            stats["incremental_stats"] = incremental_stats
        
        return stats
    
    def get_update_status(self) -> Dict[str, Any]:
        """ì—…ë°ì´íŠ¸ ìƒíƒœ ë°˜í™˜"""
        status = {}
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        for doc_id, metadata in self.hybrid_metadata.items():
            source = metadata.get('source', 'unknown')
            storage = metadata.get('storage', 'unknown')
            
            if source not in status:
                status[source] = {
                    'document_count': 0,
                    'last_updated': None,
                    'storage_distribution': {},
                    'documents': []
                }
            
            status[source]['document_count'] += 1
            status[source]['storage_distribution'][storage] = (
                status[source]['storage_distribution'].get(storage, 0) + 1
            )
            
            status[source]['documents'].append({
                'id': doc_id,
                'title': metadata.get('title', 'Unknown'),
                'last_updated': metadata.get('last_updated'),
                'storage': storage
            })
            
            # ìµœì‹  ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            last_updated = metadata.get('last_updated')
            if last_updated and (not status[source]['last_updated'] or 
                                last_updated > status[source]['last_updated']):
                status[source]['last_updated'] = last_updated
        
        return status
    
    def metadata_search(self, filter_dict: Dict[str, Any], top_k: int = 10) -> List[Dict[str, Any]]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)"""
        try:
            results = []
            
            # í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„°ì—ì„œ ê²€ìƒ‰
            for doc_id, metadata in self.hybrid_metadata.items():
                if self._matches_filter(metadata, filter_dict):
                    # ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                    content = self._get_document_content(doc_id, metadata)
                    if content:
                        results.append({
                            'id': doc_id,
                            'content': content,
                            'metadata': metadata
                        })
                        
                        if len(results) >= top_k:
                            break
            
            return results
            
        except Exception as e:
            print(f"âŒ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _matches_filter(self, metadata: Dict[str, Any], filter_dict: Dict[str, Any]) -> bool:
        """í•„í„° ì¡°ê±´ ë§¤ì¹­ í™•ì¸"""
        try:
            for key, condition in filter_dict.items():
                if key == "$or":
                    # OR ì¡°ê±´ ì²˜ë¦¬
                    if not any(self._matches_filter(metadata, sub_condition) for sub_condition in condition):
                        return False
                elif key in metadata:
                    value = metadata[key]
                    if isinstance(condition, dict):
                        for op, op_value in condition.items():
                            if op == "$contains":
                                if isinstance(value, str) and isinstance(op_value, str):
                                    if op_value.lower() not in value.lower():
                                        return False
                                else:
                                    return False
                            else:
                                return False
                    else:
                        if value != condition:
                            return False
                else:
                    return False
            return True
        except Exception:
            return False
    
    def _get_document_content(self, doc_id: str, metadata: Dict[str, Any]) -> Optional[str]:
        """ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            storage = metadata.get('storage', 'unified')
            
            if storage == 'unified' and self.unified_adapter.available:
                # Unified DBì—ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                return self.unified_adapter.get_document_content(doc_id)
            elif storage == 'incremental' and self.incremental_manager.available:
                # Incremental DBì—ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                return self.incremental_manager.get_document_content(doc_id)
            else:
                return metadata.get('content', '')
                
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (doc_id={doc_id}): {e}")
            return metadata.get('content', '')