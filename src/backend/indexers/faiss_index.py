"""
FAISS ë²¡í„° ì¸ë±ìŠ¤ ê´€ë¦¬
ë²¡í„° ê²€ìƒ‰ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›
"""

import os
import pickle
import re
from typing import List, Dict, Any, Optional
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


class FAISSIndexManager:
    def __init__(self, index_path: str = "data/indexes/faiss_main"):
        self.index_path = index_path
        self.vectorstore = None
        self.embeddings = None
        
    def initialize(self, embeddings):
        """ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.embeddings = embeddings
        if os.path.exists(self.index_path):
            self.load_index()
        else:
            print(f"ğŸ“ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {self.index_path}")
            os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
    
    def load_index(self):
        """ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            self.vectorstore = FAISS.load_local(
                self.index_path, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index_path}")
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def add_documents(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        if not documents:
            return
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        if self.vectorstore is None:
            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        else:
            self.vectorstore.add_documents(chunks)
        
        # ì €ì¥
        self.save_index()
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ê°€ ì¸ë±ìŠ¤ì— ì¶”ê°€ë¨")
    
    def save_index(self):
        """FAISS ì¸ë±ìŠ¤ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if self.vectorstore:
            self.vectorstore.save_local(self.index_path)
            print(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_path}")
    
    def vector_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.vectorstore:
            return []
        
        results = self.vectorstore.similarity_search_with_score(query, k=top_k)
        
        return [{
            'document': doc,
            'score': float(score),
            'content': doc.page_content,
            'metadata': doc.metadata,
            'type': 'vector'
        } for doc, score in results]
    
    def text_search(self, query: str, top_k: int = 5, case_sensitive: bool = False) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.vectorstore:
            return []
        
        results = []
        documents = self.vectorstore.docstore._dict.values()
        
        # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬
        if not case_sensitive:
            query = query.lower()
        
        for doc in documents:
            content = doc.page_content
            if not case_sensitive:
                content = content.lower()
            
            if query in content:
                # ë§¤ì¹­ ìœ„ì¹˜ ì°¾ê¸°
                indices = [i for i in range(len(content)) if content.startswith(query, i)]
                
                # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                contexts = []
                for idx in indices[:3]:  # ìµœëŒ€ 3ê°œ ì»¨í…ìŠ¤íŠ¸
                    start = max(0, idx - 50)
                    end = min(len(content), idx + len(query) + 50)
                    context = content[start:end]
                    contexts.append(context)
                
                results.append({
                    'document': doc,
                    'score': len(indices),  # ë§¤ì¹­ íšŸìˆ˜ë¥¼ ì ìˆ˜ë¡œ ì‚¬ìš©
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'contexts': contexts,
                    'match_count': len(indices),
                    'type': 'text'
                })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def regex_search(self, pattern: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.vectorstore:
            return []
        
        results = []
        documents = self.vectorstore.docstore._dict.values()
        
        try:
            regex = re.compile(pattern, re.IGNORECASE)
        except re.error as e:
            print(f"âŒ ì •ê·œí‘œí˜„ì‹ ì˜¤ë¥˜: {e}")
            return []
        
        for doc in documents:
            matches = regex.findall(doc.page_content)
            
            if matches:
                # ìœ ë‹ˆí¬í•œ ë§¤ì¹­ ì¶”ì¶œ
                unique_matches = list(set(matches))[:10]
                
                results.append({
                    'document': doc,
                    'score': len(matches),
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'matches': unique_matches,
                    'match_count': len(matches),
                    'type': 'regex'
                })
        
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def hybrid_search(self, query: str, top_k: int = 5, vector_weight: float = 0.5) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(query, top_k * 2)
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text_results = self.text_search(query, top_k * 2)
        
        # ê²°ê³¼ ë³‘í•© (RRF - Reciprocal Rank Fusion)
        combined_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°
        for i, result in enumerate(vector_results):
            doc_id = id(result['document'])
            rank = i + 1
            score = 1 / (60 + rank) * vector_weight
            combined_scores[doc_id] = {
                'score': score,
                'result': result
            }
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
        for i, result in enumerate(text_results):
            doc_id = id(result['document'])
            rank = i + 1
            score = 1 / (60 + rank) * (1 - vector_weight)
            
            if doc_id in combined_scores:
                combined_scores[doc_id]['score'] += score
            else:
                combined_scores[doc_id] = {
                    'score': score,
                    'result': result
                }
        
        # ìµœì¢… ì •ë ¬
        final_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:top_k]
        
        return [item['result'] for item in final_results]
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.vectorstore:
            return {"status": "not_initialized"}
        
        num_docs = len(self.vectorstore.docstore._dict)
        
        # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        source_counts = {}
        for doc in self.vectorstore.docstore._dict.values():
            source = doc.metadata.get('source', 'unknown')
            source_counts[source] = source_counts.get(source, 0) + 1
        
        return {
            "status": "initialized",
            "total_documents": num_docs,
            "source_distribution": source_counts,
            "index_path": self.index_path
        }