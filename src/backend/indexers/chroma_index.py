"""
ChromaDB ë²¡í„° ì¸ë±ìŠ¤ ê´€ë¦¬
ë²¡í„° ê²€ìƒ‰, ì¦ë¶„ ì—…ë°ì´íŠ¸ ë° ë¬¸ì„œ ê°±ì‹  ì¶”ì  ì§€ì›
"""

import os
import hashlib
import json
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
try:
    import chromadb
    from chromadb.config import Settings
    from langchain_community.vectorstores import Chroma
    CHROMADB_AVAILABLE = True
except ImportError:
    print("âš ï¸ ChromaDB not available. Vector indexing disabled.")
    chromadb = None
    Settings = None
    Chroma = None
    CHROMADB_AVAILABLE = False

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


class ChromaIndexManager:
    def __init__(self, persist_directory: str = "data/indexes/chroma_db"):
        self.persist_directory = persist_directory
        self.available = CHROMADB_AVAILABLE
        
        if not self.available:
            print("âš ï¸ ChromaDB ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        self.collection_name = "ybigta_meeting_knowledge"
        self.client = None
        self.vectorstore = None
        self.embeddings = None
        self.collection = None
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì ìš©
        self.metadata_file = os.path.join(persist_directory, "document_metadata.json")
        self.document_metadata = self._load_metadata()
        
    def _load_metadata(self) -> Dict[str, Dict[str, Any]]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_metadata(self):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        os.makedirs(os.path.dirname(self.metadata_file), exist_ok=True)
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.document_metadata, f, ensure_ascii=False, indent=2)
    
    def initialize(self, embeddings):
        """ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•˜ê³  ChromaDBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if not self.available:
            return
        self.embeddings = embeddings
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=self.persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            self.collection = self.client.get_collection(
                name=self.collection_name
            )
            print(f"âœ… ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ: {self.collection_name}")
        except Exception:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "YBIGTA íšŒì˜ ì§€ì‹ë² ì´ìŠ¤"}
            )
            print(f"ğŸ“ ìƒˆë¡œìš´ ChromaDB ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
        
        # LangChain VectorStore ë˜í¼ ì´ˆê¸°í™”
        self.vectorstore = Chroma(
            client=self.client,
            collection_name=self.collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory
        )
    
    def _compute_document_hash(self, content: str, metadata: Dict[str, Any]) -> str:
        """ë¬¸ì„œì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë³€ê²½ ì¶”ì ì— ì‚¬ìš©í•  í‚¤ ì¶”ì¶œ
        hash_content = content + str(metadata.get('source', '')) + str(metadata.get('last_modified', ''))
        return hashlib.sha256(hash_content.encode()).hexdigest()
    
    def _get_document_id(self, source: str, page_id: str = None) -> str:
        """ë¬¸ì„œì˜ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if page_id:
            return f"{source}:{page_id}"
        return f"{source}:{hashlib.md5(source.encode()).hexdigest()[:8]}"
    
    def check_document_updates(self, documents: List[Document]) -> Tuple[List[Document], List[str]]:
        """
        ë¬¸ì„œ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        Returns:
            - new_or_updated_docs: ìƒˆë¡­ê±°ë‚˜ ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            - deleted_doc_ids: ì‚­ì œí•´ì•¼ í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
        """
        new_or_updated_docs = []
        current_doc_ids = set()
        
        for doc in documents:
            # ë¬¸ì„œ ID ìƒì„±
            doc_id = self._get_document_id(
                doc.metadata.get('source', 'unknown'),
                doc.metadata.get('page_id', None)
            )
            current_doc_ids.add(doc_id)
            
            # ë¬¸ì„œ í•´ì‹œ ê³„ì‚°
            doc_hash = self._compute_document_hash(doc.page_content, doc.metadata)
            
            # ë©”íƒ€ë°ì´í„°ì— IDì™€ í•´ì‹œ ì¶”ê°€
            doc.metadata['doc_id'] = doc_id
            doc.metadata['content_hash'] = doc_hash
            doc.metadata['indexed_at'] = datetime.now().isoformat()
            
            # ê¸°ì¡´ ë¬¸ì„œì™€ ë¹„êµ
            if doc_id in self.document_metadata:
                existing_hash = self.document_metadata[doc_id].get('content_hash')
                if existing_hash != doc_hash:
                    print(f"ğŸ”„ ì—…ë°ì´íŠ¸ ê°ì§€: {doc_id}")
                    new_or_updated_docs.append(doc)
                    self.document_metadata[doc_id] = {
                        'content_hash': doc_hash,
                        'last_updated': datetime.now().isoformat(),
                        'source': doc.metadata.get('source'),
                        'title': doc.metadata.get('title', 'Unknown')
                    }
            else:
                print(f"â• ìƒˆ ë¬¸ì„œ ê°ì§€: {doc_id}")
                new_or_updated_docs.append(doc)
                self.document_metadata[doc_id] = {
                    'content_hash': doc_hash,
                    'last_updated': datetime.now().isoformat(),
                    'source': doc.metadata.get('source'),
                    'title': doc.metadata.get('title', 'Unknown')
                }
        
        # ì‚­ì œëœ ë¬¸ì„œ ì°¾ê¸° (í˜„ì¬ ì†ŒìŠ¤ì—ì„œ)
        source = documents[0].metadata.get('source') if documents else None
        deleted_doc_ids = []
        if source:
            for doc_id in list(self.document_metadata.keys()):
                if (self.document_metadata[doc_id].get('source') == source and 
                    doc_id not in current_doc_ids):
                    print(f"â– ì‚­ì œ ê°ì§€: {doc_id}")
                    deleted_doc_ids.append(doc_id)
                    del self.document_metadata[doc_id]
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_metadata()
        
        return new_or_updated_docs, deleted_doc_ids
    
    def add_documents(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ChromaDBì— ì¶”ê°€í•©ë‹ˆë‹¤. (ì¦ë¶„ ì—…ë°ì´íŠ¸)"""
        if not self.available:
            print("âš ï¸ ChromaDBê°€ ë¹„í™œì„±í™”ë˜ì–´ ë¬¸ì„œ ì¶”ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        if not documents:
            return
        
        # ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ë¬¸ì„œ í™•ì¸
        new_or_updated_docs, deleted_doc_ids = self.check_document_updates(documents)
        
        # ì‚­ì œí•  ë¬¸ì„œ ì²˜ë¦¬
        if deleted_doc_ids:
            print(f"ğŸ—‘ï¸ {len(deleted_doc_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì¤‘...")
            for doc_id in deleted_doc_ids:
                try:
                    # ChromaDBì—ì„œ ë¬¸ì„œ ì‚­ì œ
                    self.collection.delete(where={"doc_id": doc_id})
                except Exception as e:
                    print(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ {doc_id}: {e}")
        
        # ìƒˆ ë¬¸ì„œë‚˜ ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œë§Œ ì²˜ë¦¬
        if not new_or_updated_docs:
            print("âœ… ì—…ë°ì´íŠ¸í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“„ {len(new_or_updated_docs)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # ê¸°ì¡´ ì²­í¬ ì‚­ì œ ë° ìƒˆ ì²­í¬ ì¶”ê°€
        for doc in new_or_updated_docs:
            doc_id = doc.metadata['doc_id']
            
            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            try:
                self.collection.delete(where={"doc_id": doc_id})
            except Exception:
                pass  # ìƒˆ ë¬¸ì„œì¸ ê²½ìš° ì‚­ì œí•  ê²ƒì´ ì—†ìŒ
            
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = text_splitter.split_documents([doc])
            
            # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, chunk in enumerate(chunks):
                chunk.metadata['doc_id'] = doc_id
                chunk.metadata['chunk_index'] = i
                chunk.metadata['total_chunks'] = len(chunks)
            
            # ChromaDBì— ì¶”ê°€
            if chunks:
                self.vectorstore.add_documents(chunks)
                print(f"âœ… {doc_id}: {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ë¨")
    
    def sync_source(self, source: str, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        """íŠ¹ì • ì†ŒìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ë™ê¸°í™”í•©ë‹ˆë‹¤."""
        print(f"ğŸ”„ {source} ì†ŒìŠ¤ ë™ê¸°í™” ì‹œì‘...")
        
        # ì†ŒìŠ¤ë³„ë¡œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        for doc in documents:
            doc.metadata['source'] = source
            doc.metadata['sync_timestamp'] = datetime.now().isoformat()
        
        # ë¬¸ì„œ ì¶”ê°€/ì—…ë°ì´íŠ¸
        self.add_documents(documents, chunk_size, chunk_overlap)
        
        print(f"âœ… {source} ì†ŒìŠ¤ ë™ê¸°í™” ì™„ë£Œ")
    
    def vector_search(self, query: str, top_k: int = 5, filter: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.available:
            print("âš ï¸ ChromaDBê°€ ë¹„í™œì„±í™”ë˜ì–´ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return []
        if not self.vectorstore:
            return []
        
        # í•„í„° ì ìš©
        where_clause = filter if filter else None
        
        results = self.vectorstore.similarity_search_with_score(
            query, 
            k=top_k,
            where=where_clause
        )
        
        return [{
            'document': doc,
            'score': float(score),
            'content': doc.page_content,
            'metadata': doc.metadata,
            'type': 'vector'
        } for doc, score in results]
    
    def metadata_search(self, filter: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.collection:
            return []
        
        # ChromaDB ì¿¼ë¦¬
        results = self.collection.get(
            where=filter,
            limit=top_k
        )
        
        documents = []
        if results['documents']:
            for i in range(len(results['documents'])):
                doc = Document(
                    page_content=results['documents'][i],
                    metadata=results['metadatas'][i] if results['metadatas'] else {}
                )
                documents.append({
                    'document': doc,
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'type': 'metadata'
                })
        
        return documents
    
    def hybrid_search(self, query: str, top_k: int = 5, filter: Dict[str, Any] = None, vector_weight: float = 0.7) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ê³¼ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(query, top_k * 2, filter)
        
        # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ (ì¿¼ë¦¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì œëª© ê²€ìƒ‰)
        metadata_filter = filter.copy() if filter else {}
        # ChromaDBëŠ” contains ì—°ì‚°ìë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³„ë„ êµ¬í˜„ í•„ìš”
        
        # ê²°ê³¼ ë³‘í•©
        combined_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°
        for i, result in enumerate(vector_results):
            doc_id = result['metadata'].get('doc_id', str(i))
            rank = i + 1
            score = 1 / (60 + rank) * vector_weight
            combined_scores[doc_id] = {
                'score': score,
                'result': result
            }
        
        # ìµœì¢… ì •ë ¬
        final_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:top_k]
        
        return [item['result'] for item in final_results]
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.collection:
            return {"status": "not_initialized"}
        
        # ì „ì²´ ë¬¸ì„œ ìˆ˜
        total_docs = self.collection.count()
        
        # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        source_counts = {}
        update_times = {}
        
        for doc_id, metadata in self.document_metadata.items():
            source = metadata.get('source', 'unknown')
            source_counts[source] = source_counts.get(source, 0) + 1
            
            # ìµœê·¼ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            last_updated = metadata.get('last_updated')
            if last_updated:
                if source not in update_times or last_updated > update_times[source]:
                    update_times[source] = last_updated
        
        return {
            "status": "initialized",
            "total_documents": len(self.document_metadata),
            "total_chunks": total_docs,
            "source_distribution": source_counts,
            "last_updates": update_times,
            "persist_directory": self.persist_directory
        }
    
    def get_update_status(self) -> Dict[str, Any]:
        """ê° ì†ŒìŠ¤ì˜ ì—…ë°ì´íŠ¸ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        status = {}
        
        for doc_id, metadata in self.document_metadata.items():
            source = metadata.get('source', 'unknown')
            if source not in status:
                status[source] = {
                    'document_count': 0,
                    'last_updated': None,
                    'documents': []
                }
            
            status[source]['document_count'] += 1
            status[source]['documents'].append({
                'id': doc_id,
                'title': metadata.get('title', 'Unknown'),
                'last_updated': metadata.get('last_updated')
            })
            
            # ìµœì‹  ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            last_updated = metadata.get('last_updated')
            if last_updated:
                if not status[source]['last_updated'] or last_updated > status[source]['last_updated']:
                    status[source]['last_updated'] = last_updated
        
        return status