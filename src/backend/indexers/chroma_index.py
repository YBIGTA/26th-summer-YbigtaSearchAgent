"""
ChromaDB ë²¡í„° ì¸ë±ìŠ¤ ê´€ë¦¬
ë²¡í„° ê²€ìƒ‰, ì¦ë¶„ ì—…ë°ì´íŠ¸ ë° ë¬¸ì„œ ê°±ì‹  ì¶”ì  ì§€ì›
"""

import os
import hashlib
import json
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
try:
    import chromadb
    from chromadb.config import Settings
    from langchain_community.vectorstores import Chroma
    CHROMADB_AVAILABLE = True
except ImportError:
    print("âš ï¸ ChromaDB not available. Vector indexing disabled.")
    chromadb = None
    Settings = None
    Chroma = None
    CHROMADB_AVAILABLE = False

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


class ChromaIndexManager:
    def __init__(self, persist_directory: str = "data/indexes/chroma_db"):
        # í™˜ê²½ë³€ìˆ˜ë¡œ ê²½ë¡œ ì˜¤ë²„ë¼ì´ë“œ í—ˆìš©
        env_dir = os.getenv("CHROMA_PERSIST_DIR")
        if env_dir and os.path.isdir(env_dir):
            persist_directory = env_dir

        self.persist_directory = persist_directory
        self.available = CHROMADB_AVAILABLE
        
        if not self.available:
            print("âš ï¸ ChromaDB ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        # í™˜ê²½ë³€ìˆ˜ë¡œ ì»¬ë ‰ì…˜ ì´ë¦„ ì˜¤ë²„ë¼ì´ë“œ í—ˆìš©
        self.collection_name = os.getenv("CHROMA_COLLECTION_NAME", "ybigta_meeting_knowledge")
        self.client = None
        self.vectorstore = None
        self.embeddings = None
        self.collection = None
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì ìš©
        self.metadata_file = os.path.join(persist_directory, "document_metadata.json")
        self.document_metadata = self._load_metadata()
        
    def _load_metadata(self) -> Dict[str, Dict[str, Any]]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_metadata(self):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        os.makedirs(os.path.dirname(self.metadata_file), exist_ok=True)
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.document_metadata, f, ensure_ascii=False, indent=2)
    
    def initialize(self, embeddings):
        """ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•˜ê³  ChromaDBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if not self.available:
            return
        self.embeddings = embeddings
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=self.persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # 1) ê¸°ì¡´ ì»¬ë ‰ì…˜ ìë™ íƒì§€
        try:
            existing = self.client.list_collections()
        except Exception as e:
            existing = []
            print(f"âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        existing_names = [c.name for c in existing] if existing else []

        # 2) í™˜ê²½ë³€ìˆ˜/ê¸°ë³¸ ì´ë¦„ì´ ì¡´ì¬í•˜ë©´ ê·¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
        if self.collection_name in existing_names:
            self.collection = self.client.get_collection(name=self.collection_name)
            print(f"âœ… ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ: {self.collection_name}")
        else:
            # 3) ì´ë¦„ì´ ë‹¤ë¥´ë”ë¼ë„ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ë©´ 'ì²« ì»¬ë ‰ì…˜'ì„ ì±„íƒ
            if existing_names:
                picked = existing_names[0]
                self.collection_name = picked
                self.collection = self.client.get_collection(name=picked)
                print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ìë™ ê°ì§€ ë° ì‚¬ìš©: {picked}")
            else:
                # 4) ì •ë§ ì•„ë¬´ ì»¬ë ‰ì…˜ë„ ì—†ì„ ë•Œë§Œ ìƒˆë¡œ ìƒì„±
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "YBIGTA íšŒì˜ ì§€ì‹ë² ì´ìŠ¤"}
                )
                print(f"ğŸ“ ìƒˆë¡œìš´ ChromaDB ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
        
        # LangChain VectorStore ë˜í¼ ì´ˆê¸°í™”
        self.vectorstore = Chroma(
            client=self.client,
            collection_name=self.collection.name,
            collection_name=self.collection.name,
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        # ê¸°ì¡´ DB ê¸°ë°˜ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¬êµ¬ì„±
        self._rebuild_metadata_from_chromadb()
    def _rebuild_metadata_from_chromadb(self):
        """ChromaDBì—ì„œ ì‹¤ì œ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
        if not self.collection:
            print("âš ï¸ ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ChromaDBì—ì„œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
            results = self.collection.get(
                include=['metadatas', 'documents', 'embeddings']
            )
            
            if not results['ids']:
                print("ğŸ“­ ChromaDBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print(f"ğŸ”„ ChromaDBì—ì„œ {len(results['ids'])}ê°œ ë¬¸ì„œ ë°œê²¬, ë©”íƒ€ë°ì´í„° ì¬êµ¬ì„± ì¤‘...")
            
            # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
            new_metadata = {}
            
            for i, doc_id in enumerate(results['ids']):
                metadata = results['metadatas'][i] if results['metadatas'] else {}
                document = results['documents'][i] if results['documents'] else ""
                
                # ë¬¸ì„œ í•´ì‹œ ê³„ì‚°
                content_hash = self._compute_document_hash(document, metadata)
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (Drive ì‹ë³„í‚¤ ë³´ì¡´)
                new_entry = {
                    'content_hash': content_hash,
                    'last_updated': metadata.get('last_updated', datetime.now().isoformat()),
                    'source': metadata.get('source', 'unknown'),
                    'title': metadata.get('title', metadata.get('source', 'Unknown')),
                    'page_id': metadata.get('page_id'),
                    'indexed_at': metadata.get('indexed_at', datetime.now().isoformat())
                }
                # Google Drive íŠ¹í™” ë©”íƒ€ ë³´ì¡´
                if metadata.get('source') == 'google_drive':
                    if 'file_id' in metadata:
                        new_entry['file_id'] = metadata.get('file_id')
                    if 'last_modified' in metadata:
                        new_entry['last_modified'] = metadata.get('last_modified')
                    if 'created_time' in metadata:
                        new_entry['created_time'] = metadata.get('created_time')
                
                new_metadata[doc_id] = new_entry
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•© (ê¸°ì¡´ ë°ì´í„° ìš°ì„ )
            merged_metadata = {**new_metadata, **self.document_metadata}
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.document_metadata = merged_metadata
            self._save_metadata()
            
            print(f"âœ… ë©”íƒ€ë°ì´í„° ì¬êµ¬ì„± ì™„ë£Œ: {len(merged_metadata)}ê°œ ë¬¸ì„œ")
            
            # ì†ŒìŠ¤ë³„ í†µê³„ ì¶œë ¥
            source_stats = {}
            for doc_id, meta in merged_metadata.items():
                source = meta.get('source', 'unknown')
                source_stats[source] = source_stats.get(source, 0) + 1
            
            print("ğŸ“Š ì†ŒìŠ¤ë³„ ë¬¸ì„œ í†µê³„:")
            for source, count in source_stats.items():
                print(f"  - {source}: {count}ê°œ")
            
        except Exception as e:
            print(f"âŒ ChromaDB ë©”íƒ€ë°ì´í„° ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _compute_document_hash(self, content: str, metadata: Dict[str, Any]) -> str:
        """ë¬¸ì„œì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë³€ê²½ ì¶”ì ì— ì‚¬ìš©í•  í‚¤ ì¶”ì¶œ
        hash_content = content + str(metadata.get('source', '')) + str(metadata.get('last_modified', ''))
        return hashlib.sha256(hash_content.encode()).hexdigest()
    
    def _get_document_id(self, source: str, page_id: str = None) -> str:
        """ë¬¸ì„œì˜ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if page_id:
            return f"{source}:{page_id}"
        return f"{source}:{hashlib.md5(source.encode()).hexdigest()[:8]}"
    
    def check_document_updates(self, documents: List[Document]) -> Tuple[List[Document], List[str]]:
        """
        ë¬¸ì„œ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        Returns:
            - new_or_updated_docs: ìƒˆë¡­ê±°ë‚˜ ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            - deleted_doc_ids: ì‚­ì œí•´ì•¼ í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
        """
        new_or_updated_docs = []
        current_doc_ids = set()
        
        for doc in documents:
            # ë¬¸ì„œ ID ìƒì„±
            doc_id = self._get_document_id(
                doc.metadata.get('source', 'unknown'),
                doc.metadata.get('page_id', None)
            )
            current_doc_ids.add(doc_id)
            
            # ë¬¸ì„œ í•´ì‹œ ê³„ì‚°
            doc_hash = self._compute_document_hash(doc.page_content, doc.metadata)
            
            # ë©”íƒ€ë°ì´í„°ì— IDì™€ í•´ì‹œ ì¶”ê°€
            doc.metadata['doc_id'] = doc_id
            doc.metadata['content_hash'] = doc_hash
            doc.metadata['indexed_at'] = datetime.now().isoformat()
            
            # ê¸°ì¡´ ë¬¸ì„œì™€ ë¹„êµ
            if doc_id in self.document_metadata:
                existing_hash = self.document_metadata[doc_id].get('content_hash')
                if existing_hash != doc_hash:
                    print(f"ğŸ”„ ì—…ë°ì´íŠ¸ ê°ì§€: {doc_id}")
                    new_or_updated_docs.append(doc)
                    self.document_metadata[doc_id] = {
                        'content_hash': doc_hash,
                        'last_updated': datetime.now().isoformat(),
                        'source': doc.metadata.get('source', 'unknown'),
                        'title': doc.metadata.get('title', doc.metadata.get('source', 'Unknown')),
                        'page_id': doc.metadata.get('page_id'),
                        'indexed_at': doc.metadata.get('indexed_at')
                    }
            else:
                print(f"ğŸ†• ìƒˆ ë¬¸ì„œ ê°ì§€: {doc_id}")
                new_or_updated_docs.append(doc)
                self.document_metadata[doc_id] = {
                    'content_hash': doc_hash,
                    'last_updated': datetime.now().isoformat(),
                    'source': doc.metadata.get('source', 'unknown'),
                    'title': doc.metadata.get('title', doc.metadata.get('source', 'Unknown')),
                    'page_id': doc.metadata.get('page_id'),
                    'indexed_at': doc.metadata.get('indexed_at')
                }
        
        # ì‚­ì œëœ ë¬¸ì„œ í™•ì¸ (í˜„ì¬ ë¬¸ì„œ ëª©ë¡ì— ì—†ëŠ” ê¸°ì¡´ ë¬¸ì„œë“¤)
        # í•˜ë‹¨ì˜ 'ì‚­ì œëœ ë¬¸ì„œ í™•ì¸' ë¶€ë¶„ì„ ì†ŒìŠ¤ ë²”ìœ„ë¡œ í•œì •
        # ê·¸ë¦¬ê³  ì—¬ê¸°ì„œëŠ” 'ê°€ëŠ¥ì„± í›„ë³´'ë§Œ ë°˜í™˜í•˜ê³  ì‹¤ì œ ì‚­ì œ ì—¬ë¶€ëŠ” sync_sourceì—ì„œ full_scan ì—¬ë¶€ë¡œ ê²°ì •
        existing_doc_ids = set(self.document_metadata.keys())
        current_doc_ids = set(current_doc_ids)  # ìœ„ì—ì„œ ìˆ˜ì§‘ë¨
        # ê°™ì€ sourceë§Œ ë¹„êµ
        existing_same_source = {doc_id for doc_id, meta in self.document_metadata.items()
                                if meta.get('source') == (documents[0].metadata.get('source') if documents else None)}
        deleted_doc_ids = list(existing_same_source - current_doc_ids)
        
        if deleted_doc_ids:
            print(f"ğŸ—‘ï¸ ì‚­ì œëœ ë¬¸ì„œ ê°ì§€: {len(deleted_doc_ids)}ê°œ")
            for doc_id in deleted_doc_ids:
                print(f"  - {doc_id}")
        
        return new_or_updated_docs, deleted_doc_ids
    
    def add_documents(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ChromaDBì— ì¶”ê°€í•©ë‹ˆë‹¤. (ì¦ë¶„ ì—…ë°ì´íŠ¸)"""
        if not self.available:
            print("âš ï¸ ChromaDBê°€ ë¹„í™œì„±í™”ë˜ì–´ ë¬¸ì„œ ì¶”ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        if not documents:
            return
        
        # ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ë¬¸ì„œ í™•ì¸
        new_or_updated_docs, deleted_doc_ids = self.check_document_updates(documents)
        
        # ì‚­ì œí•  ë¬¸ì„œ ì²˜ë¦¬
        if deleted_doc_ids:
            print(f"ğŸ—‘ï¸ {len(deleted_doc_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì¤‘...")
            for doc_id in deleted_doc_ids:
                try:
                    # ChromaDBì—ì„œ ë¬¸ì„œ ì‚­ì œ
                    self.collection.delete(where={"doc_id": doc_id})
                except Exception as e:
                    print(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ {doc_id}: {e}")
        
        # ìƒˆ ë¬¸ì„œë‚˜ ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œë§Œ ì²˜ë¦¬
        if not new_or_updated_docs:
            print("âœ… ì—…ë°ì´íŠ¸í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“„ {len(new_or_updated_docs)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # ê¸°ì¡´ ì²­í¬ ì‚­ì œ ë° ìƒˆ ì²­í¬ ì¶”ê°€
        for doc in new_or_updated_docs:
            doc_id = doc.metadata['doc_id']
            
            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            try:
                self.collection.delete(where={"doc_id": doc_id})
            except Exception:
                pass  # ìƒˆ ë¬¸ì„œì¸ ê²½ìš° ì‚­ì œí•  ê²ƒì´ ì—†ìŒ
            
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = text_splitter.split_documents([doc])
            
            # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, chunk in enumerate(chunks):
                chunk.metadata['doc_id'] = doc_id
                chunk.metadata['chunk_index'] = i
                chunk.metadata['total_chunks'] = len(chunks)
            
            # ChromaDBì— ì¶”ê°€
            if chunks:
                self.vectorstore.add_documents(chunks)
                print(f"âœ… {doc_id}: {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ë¨")
    
    def sync_source(self, source: str, documents: List[Document], full_scan: bool = False):
        """
        sourceì—ì„œ ìˆ˜ì§‘í•œ documentsë¥¼ ë™ê¸°í™”.
        - full_scan=True: ì´ë²ˆ ë°°ì¹˜ê°€ ì†ŒìŠ¤ì˜ 'ì „ì²´ ìŠ¤ëƒ…ìƒ·'ì¼ ë•Œë§Œ ê¸°ì¡´-í˜„ì¬ ì°¨ì§‘í•©ì„ ì‚­ì œë¡œ ê°„ì£¼
        - full_scan=False: ì¦ë¶„ ìˆ˜ì§‘. ì‚­ì œëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ.
        """
        if not documents:
            return

        # ê¸°ì¡´ ì½”ë“œ: ì—…ë°ì´íŠ¸/ì‹ ê·œ íŒë‹¨
        new_or_updated_docs, deleted_doc_ids = self.check_document_updates(documents)

        # ìˆ˜ì •: ì¦ë¶„ ëª¨ë“œì—ì„œëŠ” ì‚­ì œ ê¸ˆì§€
        if not full_scan:
            deleted_doc_ids = []

        # ì‚­ì œ ìˆ˜í–‰
        if deleted_doc_ids:
            try:
                self.collection.delete(ids=deleted_doc_ids)
                # ë©”íƒ€ë°ì´í„° ì‚­ì œ ë°˜ì˜
                for did in deleted_doc_ids:
                    self.document_metadata.pop(did, None)
                print(f"ğŸ—‘ï¸ {len(deleted_doc_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ì‹ ê·œ/ì—…ë°ì´íŠ¸ ë¬¸ì„œ upsert
        # (ê¸°ì¡´ add_documents ë“± ì‚¬ìš©)
        self.add_documents(new_or_updated_docs)
        self._save_metadata()
        
        print(f"âœ… {source} ì†ŒìŠ¤ ë™ê¸°í™” ì™„ë£Œ")
    
    def vector_search(self, query: str, top_k: int = 5, filter: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.available:
            print("âš ï¸ ChromaDBê°€ ë¹„í™œì„±í™”ë˜ì–´ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return []
        if not self.vectorstore:
            return []
        
        # í•„í„° ì ìš©
        where_clause = filter if filter else None
        
        results = self.vectorstore.similarity_search_with_score(
            query, 
            k=top_k,
            where=where_clause
        )
        
        return [{
            'document': doc,
            'score': float(score),
            'content': doc.page_content,
            'metadata': doc.metadata,
            'type': 'vector'
        } for doc, score in results]
    
    def metadata_search(self, filter: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.collection:
            return []
        
        # ChromaDB ì¿¼ë¦¬
        results = self.collection.get(
            where=filter,
            limit=top_k
        )
        
        documents = []
        if results['documents']:
            for i in range(len(results['documents'])):
                doc = Document(
                    page_content=results['documents'][i],
                    metadata=results['metadatas'][i] if results['metadatas'] else {}
                )
                documents.append({
                    'document': doc,
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'type': 'metadata'
                })
        
        return documents
    
    def hybrid_search(self, query: str, top_k: int = 5, filter: Dict[str, Any] = None, vector_weight: float = 0.7) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ê³¼ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(query, top_k * 2, filter)
        
        # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ (ì¿¼ë¦¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì œëª© ê²€ìƒ‰)
        metadata_filter = filter.copy() if filter else {}
        # ChromaDBëŠ” contains ì—°ì‚°ìë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³„ë„ êµ¬í˜„ í•„ìš”
        
        # ê²°ê³¼ ë³‘í•©
        combined_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°
        for i, result in enumerate(vector_results):
            doc_id = result['metadata'].get('doc_id', str(i))
            rank = i + 1
            score = 1 / (60 + rank) * vector_weight
            combined_scores[doc_id] = {
                'score': score,
                'result': result
            }
        
        # ìµœì¢… ì •ë ¬
        final_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:top_k]
        
        return [item['result'] for item in final_results]
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.collection:
            return {"status": "not_initialized"}
        
        # ì „ì²´ ë¬¸ì„œ ìˆ˜
        total_docs = self.collection.count()
        
        # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        source_counts = {}
        update_times = {}
        
        for doc_id, metadata in self.document_metadata.items():
            source = metadata.get('source', 'unknown')
            source_counts[source] = source_counts.get(source, 0) + 1
            
            # ìµœê·¼ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            last_updated = metadata.get('last_updated')
            if last_updated:
                if source not in update_times or last_updated > update_times[source]:
                    update_times[source] = last_updated
        
        return {
            "status": "initialized",
            "total_documents": len(self.document_metadata),
            "total_chunks": total_docs,
            "source_distribution": source_counts,
            "last_updates": update_times,
            "persist_directory": self.persist_directory
        }
    
    def get_update_status(self) -> Dict[str, Any]:
        """ê° ì†ŒìŠ¤ì˜ ì—…ë°ì´íŠ¸ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        status = {}
        
        for doc_id, metadata in self.document_metadata.items():
            source = metadata.get('source', 'unknown')
            if source not in status:
                status[source] = {
                    'document_count': 0,
                    'last_updated': None,
                    'documents': []
                }
            
            status[source]['document_count'] += 1
            status[source]['documents'].append({
                'id': doc_id,
                'title': metadata.get('title', 'Unknown'),
                'last_updated': metadata.get('last_updated')
            })
            
            # ìµœì‹  ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            last_updated = metadata.get('last_updated')
            if last_updated:
                if not status[source]['last_updated'] or last_updated > status[source]['last_updated']:
                    status[source]['last_updated'] = last_updated
        
        return status