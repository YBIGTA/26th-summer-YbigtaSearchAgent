"""
í™”ì ì¤‘ì‹¬ ë¶„ì„ ëª¨ë“ˆ (SpeakerAnalyzer)

íšŒì˜ì—ì„œ í™”ìë³„ íŠ¹ì„±, ê¸°ì—¬ë„, ìƒí˜¸ì‘ìš©ì„ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤.
- í™”ìë³„ ë°œí™” íŒ¨í„´ ë¶„ì„
- ì•„ì  ë‹¤ë³„ í™”ì ê¸°ì—¬ë„ ì¸¡ì •
- í™”ìê°„ ìƒí˜¸ì‘ìš© íŒ¨í„´ ë¶„ì„
- ì˜ê²¬ ë¶„í¬ ë° í•©ì˜ íŒ¨í„´ ë¶„ì„
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from collections import defaultdict, Counter
import re

from .base_agent import BaseAgent

logger = logging.getLogger(__name__)


class SpeakerAnalyzer(BaseAgent):
    """í™”ì ì¤‘ì‹¬ íšŒì˜ ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_client=None):
        super().__init__(
            name="SpeakerAnalyzer",
            description="í™”ìë³„ ë°œí™” íŒ¨í„´, ê¸°ì—¬ë„, ìƒí˜¸ì‘ìš©ì„ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì—ì´ì „íŠ¸",
            llm_client=llm_client
        )
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """í™”ì ì¤‘ì‹¬ ë¶„ì„ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ¤ {self.name}: í™”ì ì¤‘ì‹¬ ë¶„ì„ ì‹œì‘")
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if not self._validate_input(input_data):
                return self._create_empty_result("ì…ë ¥ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # í™”ìë³„ ë°œí™” ë°ì´í„° ì¶”ì¶œ
            speaker_data = self._extract_speaker_data(input_data)
            
            if not speaker_data:
                return self._create_empty_result("í™”ì ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë¶„ì„ ì‹¤í–‰
            analysis_result = {
                "speaker_statistics": self._analyze_speaker_statistics(speaker_data),
                "speaking_patterns": self._analyze_speaking_patterns(speaker_data),
                "interaction_analysis": self._analyze_speaker_interactions(speaker_data),
                "agenda_contributions": await self._analyze_agenda_contributions(speaker_data, input_data),
                "sentiment_analysis": self._analyze_speaker_sentiments(speaker_data),
                "summary": self._generate_speaker_summary(speaker_data)
            }
            
            logger.info(f"âœ… {self.name}: í™”ì ì¤‘ì‹¬ ë¶„ì„ ì™„ë£Œ - {len(speaker_data)} í™”ì")
            
            return {
                "analysis_type": "speaker_centered",
                "total_speakers": len(speaker_data),
                "analysis_timestamp": datetime.now().isoformat(),
                "confidence": 0.85,  # ê¸°ë³¸ ì‹ ë¢°ë„
                **analysis_result
            }
            
        except Exception as e:
            logger.error(f"âŒ {self.name} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return self._create_error_result(str(e))
    
    def _validate_input(self, input_data: Dict[str, Any]) -> bool:
        """ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        timeline = input_data.get("timeline", [])
        segments = input_data.get("segments", [])
        
        if not timeline and not segments:
            logger.error(f"{self.name}: íƒ€ì„ë¼ì¸ ë˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        return True
    
    def _extract_speaker_data(self, input_data: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """í™”ìë³„ ë°œí™” ë°ì´í„° ì¶”ì¶œ"""
        speaker_utterances = defaultdict(list)
        
        # íƒ€ì„ë¼ì¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        timeline = input_data.get("timeline", [])
        for item in timeline:
            speaker = item.get("speaker", "Unknown")
            if speaker and speaker != "Unknown":
                utterance = {
                    "text": item.get("text", ""),
                    "start_time": item.get("start", 0),
                    "end_time": item.get("end", 0),
                    "duration": item.get("end", 0) - item.get("start", 0),
                    "timestamp": item.get("timestamp", "")
                }
                speaker_utterances[speaker].append(utterance)
        
        # ì„¸ê·¸ë¨¼íŠ¸ì—ì„œë„ ë°ì´í„° ì¶”ì¶œ (íƒ€ì„ë¼ì¸ì´ ì—†ëŠ” ê²½ìš°)
        if not speaker_utterances:
            segments = input_data.get("segments", [])
            for seg in segments:
                speaker = self._extract_speaker_from_segment(seg)
                if speaker and speaker != "Unknown":
                    utterance = {
                        "text": self._extract_text_from_segment(seg),
                        "start_time": seg.get("start", 0),
                        "end_time": seg.get("end", 0),
                        "duration": seg.get("duration", 0) or (seg.get("end", 0) - seg.get("start", 0)),
                        "timestamp": seg.get("timestamp", "")
                    }
                    speaker_utterances[speaker].append(utterance)
        
        logger.debug(f"í™”ìë³„ ë°œí™” ë°ì´í„°: {[(spk, len(utts)) for spk, utts in speaker_utterances.items()]}")
        return dict(speaker_utterances)
    
    def _analyze_speaker_statistics(self, speaker_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """í™”ìë³„ ê¸°ë³¸ í†µê³„ ë¶„ì„"""
        stats = {}
        total_speaking_time = 0
        total_utterances = 0
        total_words = 0
        
        # ê° í™”ìë³„ í†µê³„ ê³„ì‚°
        for speaker, utterances in speaker_data.items():
            speaking_time = sum(utt["duration"] for utt in utterances)
            utterance_count = len(utterances)
            word_count = sum(len(utt["text"].split()) for utt in utterances)
            avg_utterance_length = word_count / utterance_count if utterance_count > 0 else 0
            
            stats[speaker] = {
                "utterance_count": utterance_count,
                "total_speaking_time": speaking_time,
                "total_words": word_count,
                "average_utterance_length": round(avg_utterance_length, 1),
                "speaking_rate": round(word_count / (speaking_time / 60), 1) if speaking_time > 0 else 0  # ë¶„ë‹¹ ë‹¨ì–´ ìˆ˜
            }
            
            total_speaking_time += speaking_time
            total_utterances += utterance_count
            total_words += word_count
        
        # ë¹„ìœ¨ ê³„ì‚°
        for speaker in stats:
            stats[speaker]["speaking_percentage"] = round(
                (stats[speaker]["total_speaking_time"] / total_speaking_time * 100) if total_speaking_time > 0 else 0, 1
            )
            stats[speaker]["utterance_percentage"] = round(
                (stats[speaker]["utterance_count"] / total_utterances * 100) if total_utterances > 0 else 0, 1
            )
        
        return {
            "individual_stats": stats,
            "summary": {
                "total_speakers": len(speaker_data),
                "total_speaking_time": total_speaking_time,
                "total_utterances": total_utterances,
                "total_words": total_words,
                "most_active_speaker": max(stats.keys(), key=lambda s: stats[s]["total_words"]) if stats else None,
                "most_talkative_speaker": max(stats.keys(), key=lambda s: stats[s]["total_speaking_time"]) if stats else None
            }
        }
    
    def _analyze_speaking_patterns(self, speaker_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """í™”ìë³„ ë°œí™” íŒ¨í„´ ë¶„ì„"""
        patterns = {}
        
        for speaker, utterances in speaker_data.items():
            if not utterances:
                continue
            
            # ë°œí™” ê¸¸ì´ ë¶„í¬
            utterance_lengths = [len(utt["text"].split()) for utt in utterances]
            
            # ë°œí™” ê°„ê²© ë¶„ì„
            intervals = []
            if len(utterances) > 1:
                for i in range(1, len(utterances)):
                    interval = utterances[i]["start_time"] - utterances[i-1]["end_time"]
                    if interval >= 0:  # ìœ íš¨í•œ ê°„ê²©ë§Œ
                        intervals.append(interval)
            
            # ë°œí™” ì‹œì‘ ì‹œê°„ ë¶„í¬ (íšŒì˜ ì§„í–‰ íŒ¨í„´)
            speaking_times = [utt["start_time"] for utt in utterances]
            
            patterns[speaker] = {
                "utterance_length_stats": {
                    "min": min(utterance_lengths) if utterance_lengths else 0,
                    "max": max(utterance_lengths) if utterance_lengths else 0,
                    "average": round(sum(utterance_lengths) / len(utterance_lengths), 1) if utterance_lengths else 0,
                    "distribution": self._categorize_utterance_lengths(utterance_lengths)
                },
                "speaking_intervals": {
                    "average_interval": round(sum(intervals) / len(intervals), 1) if intervals else 0,
                    "total_pauses": len(intervals),
                    "longest_pause": max(intervals) if intervals else 0
                },
                "temporal_pattern": {
                    "first_speech_time": min(speaking_times) if speaking_times else 0,
                    "last_speech_time": max(speaking_times) if speaking_times else 0,
                    "speaking_distribution": self._analyze_temporal_distribution(speaking_times)
                }
            }
        
        return patterns
    
    def _analyze_speaker_interactions(self, speaker_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """í™”ìê°„ ìƒí˜¸ì‘ìš© íŒ¨í„´ ë¶„ì„"""
        interactions = {}
        all_utterances = []
        
        # ëª¨ë“  ë°œí™”ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        for speaker, utterances in speaker_data.items():
            for utt in utterances:
                utt_with_speaker = utt.copy()
                utt_with_speaker["speaker"] = speaker
                all_utterances.append(utt_with_speaker)
        
        all_utterances.sort(key=lambda x: x["start_time"])
        
        if len(all_utterances) < 2:
            return {"error": "ìƒí˜¸ì‘ìš© ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # í™”ì ì „í™˜ íŒ¨í„´ ë¶„ì„
        transitions = []
        response_times = []
        
        for i in range(1, len(all_utterances)):
            prev_speaker = all_utterances[i-1]["speaker"]
            curr_speaker = all_utterances[i]["speaker"]
            
            # í™”ì ì „í™˜
            if prev_speaker != curr_speaker:
                transitions.append((prev_speaker, curr_speaker))
                
                # ì‘ë‹µ ì‹œê°„ (ì´ì „ ë°œí™” ì¢…ë£Œ ~ í˜„ì¬ ë°œí™” ì‹œì‘)
                response_time = all_utterances[i]["start_time"] - all_utterances[i-1]["end_time"]
                if response_time >= 0:
                    response_times.append(response_time)
        
        # ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤
        interaction_matrix = defaultdict(lambda: defaultdict(int))
        for from_speaker, to_speaker in transitions:
            interaction_matrix[from_speaker][to_speaker] += 1
        
        interactions = {
            "speaker_transitions": {
                "total_transitions": len(transitions),
                "transition_matrix": dict(interaction_matrix),
                "most_common_transitions": Counter(transitions).most_common(5)
            },
            "response_patterns": {
                "average_response_time": round(sum(response_times) / len(response_times), 1) if response_times else 0,
                "fastest_response": min(response_times) if response_times else 0,
                "slowest_response": max(response_times) if response_times else 0
            },
            "interaction_metrics": self._calculate_interaction_metrics(speaker_data, interaction_matrix)
        }
        
        return interactions
    
    async def _analyze_agenda_contributions(self, speaker_data: Dict[str, List[Dict]], input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì•„ì  ë‹¤ë³„ í™”ì ê¸°ì—¬ë„ ë¶„ì„"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í† í”½ ë¶„ì„ (LLMì´ ìˆìœ¼ë©´ ë” ì •êµí•œ ë¶„ì„ ê°€ëŠ¥)
        contributions = {}
        
        # ê¸°ë³¸ í† í”½ í‚¤ì›Œë“œ (ì‹¤ì œë¡œëŠ” ì•„ì  ë‹¤ ë§ˆì´ë‹ ê²°ê³¼ë¥¼ í™œìš©)
        topic_keywords = {
            "í”„ë¡œì íŠ¸": ["í”„ë¡œì íŠ¸", "ê°œë°œ", "ì¼ì •", "ê³„íš"],
            "ì˜ˆì‚°": ["ì˜ˆì‚°", "ë¹„ìš©", "ê¸ˆì•¡", "íˆ¬ì"],
            "ì¸ì‚¬": ["ì±„ìš©", "ì¸ë ¥", "íŒ€", "ì¡°ì§"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "ì‹œìŠ¤í…œ", "ê°œë°œ", "êµ¬í˜„"],
            "ë§ˆì¼€íŒ…": ["ë§ˆì¼€íŒ…", "í™ë³´", "ê³ ê°", "íŒë§¤"]
        }
        
        for speaker, utterances in speaker_data.items():
            speaker_contributions = {}
            
            for topic, keywords in topic_keywords.items():
                topic_mentions = 0
                topic_words = 0
                
                for utt in utterances:
                    text_lower = utt["text"].lower()
                    for keyword in keywords:
                        if keyword in text_lower:
                            topic_mentions += 1
                            topic_words += len(utt["text"].split())
                
                speaker_contributions[topic] = {
                    "mentions": topic_mentions,
                    "words": topic_words,
                    "relevance_score": topic_mentions * 0.3 + (topic_words / 100) * 0.7
                }
            
            contributions[speaker] = speaker_contributions
        
        # í† í”½ë³„ ì£¼ìš” ê¸°ì—¬ì ì‹ë³„
        topic_leaders = {}
        for topic in topic_keywords.keys():
            topic_scores = [(spk, contrib[topic]["relevance_score"]) for spk, contrib in contributions.items()]
            topic_scores.sort(key=lambda x: x[1], reverse=True)
            topic_leaders[topic] = topic_scores[0][0] if topic_scores and topic_scores[0][1] > 0 else None
        
        return {
            "individual_contributions": contributions,
            "topic_leadership": topic_leaders,
            "analysis_method": "keyword_based"  # LLM ì‚¬ìš© ì‹œ "llm_based"ë¡œ ë³€ê²½ ê°€ëŠ¥
        }
    
    def _analyze_speaker_sentiments(self, speaker_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """í™”ìë³„ ê°ì •/ì–´ì¡° ë¶„ì„ (ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        sentiments = {}
        
        # ê°ì • í‚¤ì›Œë“œ ì‚¬ì „ (ê°„ë‹¨í•œ ë²„ì „)
        positive_words = ["ì¢‹ë‹¤", "í›Œë¥­", "ì™„ë²½", "ì„±ê³µ", "ë§Œì¡±", "ê¸ì •", "ì°¬ì„±", "ë™ì˜"]
        negative_words = ["ë‚˜ì˜ë‹¤", "ë¬¸ì œ", "ê±±ì •", "ìš°ë ¤", "ë°˜ëŒ€", "ë¶€ì •", "ì‹¤íŒ¨", "ì–´ë ¤ì›€"]
        question_words = ["ì–´ë–»ê²Œ", "ë¬´ì—‡", "ì™œ", "ì–¸ì œ", "ì–´ë””", "ëˆ„êµ¬", "?"]
        
        for speaker, utterances in speaker_data.items():
            positive_count = 0
            negative_count = 0
            question_count = 0
            total_words = 0
            
            for utt in utterances:
                text = utt["text"]
                words = text.split()
                total_words += len(words)
                
                text_lower = text.lower()
                
                for word in positive_words:
                    if word in text_lower:
                        positive_count += 1
                
                for word in negative_words:
                    if word in text_lower:
                        negative_count += 1
                
                for word in question_words:
                    if word in text_lower or "?" in text:
                        question_count += 1
            
            # ê°ì • ì ìˆ˜ ê³„ì‚°
            sentiment_score = (positive_count - negative_count) / max(total_words, 1) * 100
            
            sentiments[speaker] = {
                "positive_indicators": positive_count,
                "negative_indicators": negative_count,
                "questions_asked": question_count,
                "sentiment_score": round(sentiment_score, 2),
                "sentiment_category": self._categorize_sentiment(sentiment_score),
                "engagement_level": self._calculate_engagement_level(question_count, len(utterances))
            }
        
        return sentiments
    
    def _generate_speaker_summary(self, speaker_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """í™”ìë³„ ì¢…í•© ìš”ì•½"""
        if not speaker_data:
            return {"summary": "ë¶„ì„í•  í™”ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ì „ì²´ í†µê³„
        total_speakers = len(speaker_data)
        total_utterances = sum(len(utts) for utts in speaker_data.values())
        total_words = sum(sum(len(utt["text"].split()) for utt in utts) for utts in speaker_data.values())
        
        # í™”ìë³„ ê¸°ì—¬ë„ ìˆœìœ„
        speaker_rankings = []
        for speaker, utterances in speaker_data.items():
            word_count = sum(len(utt["text"].split()) for utt in utterances)
            speaker_rankings.append({
                "speaker": speaker,
                "utterances": len(utterances),
                "words": word_count,
                "contribution_score": word_count * 0.7 + len(utterances) * 0.3
            })
        
        speaker_rankings.sort(key=lambda x: x["contribution_score"], reverse=True)
        
        return {
            "meeting_overview": {
                "total_speakers": total_speakers,
                "total_utterances": total_utterances,
                "total_words": total_words,
                "average_words_per_speaker": round(total_words / total_speakers, 1)
            },
            "speaker_rankings": speaker_rankings,
            "top_contributors": [spk["speaker"] for spk in speaker_rankings[:3]],
            "participation_balance": self._assess_participation_balance(speaker_rankings),
            "key_insights": self._generate_key_insights(speaker_data, speaker_rankings)
        }
    
    def _categorize_utterance_lengths(self, lengths: List[int]) -> Dict[str, int]:
        """ë°œí™” ê¸¸ì´ë³„ ë¶„í¬ ë¶„ë¥˜"""
        short = sum(1 for l in lengths if l <= 5)
        medium = sum(1 for l in lengths if 6 <= l <= 15)
        long_ = sum(1 for l in lengths if l > 15)
        
        return {"short": short, "medium": medium, "long": long_}
    
    def _analyze_temporal_distribution(self, speaking_times: List[float]) -> str:
        """ì‹œê°„ëŒ€ë³„ ë°œí™” ë¶„í¬ ë¶„ì„"""
        if not speaking_times:
            return "no_data"
        
        max_time = max(speaking_times)
        early_count = sum(1 for t in speaking_times if t <= max_time * 0.33)
        middle_count = sum(1 for t in speaking_times if max_time * 0.33 < t <= max_time * 0.66)
        late_count = sum(1 for t in speaking_times if t > max_time * 0.66)
        
        if early_count >= middle_count and early_count >= late_count:
            return "early_dominant"
        elif middle_count >= late_count:
            return "middle_dominant" 
        else:
            return "late_dominant"
    
    def _calculate_interaction_metrics(self, speaker_data: Dict, interaction_matrix: Dict) -> Dict[str, Any]:
        """ìƒí˜¸ì‘ìš© ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        speakers = list(speaker_data.keys())
        
        for speaker in speakers:
            # ì´ í™”ìê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ í™”ìë“¤ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ”ê°€
            interactions_initiated = sum(interaction_matrix.get(speaker, {}).values())
            interactions_received = sum(interaction_matrix.get(other, {}).get(speaker, 0) for other in speakers)
            
            metrics[speaker] = {
                "interactions_initiated": interactions_initiated,
                "interactions_received": interactions_received,
                "interaction_ratio": round(interactions_initiated / max(interactions_received, 1), 2),
                "interaction_diversity": len(interaction_matrix.get(speaker, {}))  # ëª‡ ëª…ê³¼ ìƒí˜¸ì‘ìš©í–ˆëŠ”ê°€
            }
        
        return metrics
    
    def _categorize_sentiment(self, score: float) -> str:
        """ê°ì • ì ìˆ˜ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        if score > 1.0:
            return "very_positive"
        elif score > 0.2:
            return "positive"
        elif score > -0.2:
            return "neutral"
        elif score > -1.0:
            return "negative"
        else:
            return "very_negative"
    
    def _calculate_engagement_level(self, questions: int, utterances: int) -> str:
        """ì°¸ì—¬ ìˆ˜ì¤€ ê³„ì‚°"""
        if utterances == 0:
            return "no_participation"
        
        question_ratio = questions / utterances
        
        if question_ratio > 0.3:
            return "highly_engaged"
        elif question_ratio > 0.1:
            return "moderately_engaged"
        else:
            return "low_engagement"
    
    def _assess_participation_balance(self, rankings: List[Dict]) -> str:
        """ì°¸ì—¬ ê· í˜•ë„ í‰ê°€"""
        if len(rankings) < 2:
            return "insufficient_data"
        
        scores = [r["contribution_score"] for r in rankings]
        max_score = max(scores)
        min_score = min(scores)
        
        if max_score == 0:
            return "no_participation"
        
        balance_ratio = min_score / max_score
        
        if balance_ratio > 0.7:
            return "well_balanced"
        elif balance_ratio > 0.3:
            return "moderately_balanced"
        else:
            return "unbalanced"
    
    def _generate_key_insights(self, speaker_data: Dict, rankings: List[Dict]) -> List[str]:
        """ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        if not rankings:
            return ["ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."]
        
        # ìµœë‹¤ ë°œì–¸ì
        top_speaker = rankings[0]
        insights.append(f"{top_speaker['speaker']}ê°€ ê°€ì¥ í™œë°œí•˜ê²Œ ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤ ({top_speaker['words']}ë‹¨ì–´, {top_speaker['utterances']}íšŒ ë°œì–¸).")
        
        # ì°¸ì—¬ë„ ê²©ì°¨
        if len(rankings) > 1:
            participation_gap = rankings[0]["contribution_score"] / rankings[-1]["contribution_score"] if rankings[-1]["contribution_score"] > 0 else float('inf')
            if participation_gap > 5:
                insights.append("í™”ìê°„ ì°¸ì—¬ë„ ê²©ì°¨ê°€ í½ë‹ˆë‹¤. ì¼ë¶€ ì°¸ì„ìì˜ ë” ë§ì€ ì°¸ì—¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í™”ì ìˆ˜ ëŒ€ë¹„ ë°œì–¸ ë¶„í¬
        total_speakers = len(speaker_data)
        active_speakers = len([r for r in rankings if r["utterances"] > 2])
        if active_speakers / total_speakers < 0.5:
            insights.append("ì°¸ì„ì ì¤‘ ì ˆë°˜ ì´ìƒì´ ì ê·¹ì ìœ¼ë¡œ ë°œì–¸í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return insights
    
    def _extract_text_from_segment(self, seg: Dict[str, Any]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_fields = ["text", "msg", "content", "utterance"]
        for field in text_fields:
            if field in seg and seg[field]:
                return seg[field].strip()
        return ""
    
    def _extract_speaker_from_segment(self, seg: Dict[str, Any]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ í™”ì ì¶”ì¶œ"""
        speaker_fields = ["speaker", "spk"]
        for field in speaker_fields:
            if field in seg and seg[field] is not None:
                speaker_value = seg[field]
                if isinstance(speaker_value, int):
                    return f"Speaker {speaker_value}"
                else:
                    return str(speaker_value)
        return "Unknown"
    
    def _create_empty_result(self, reason: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            "analysis_type": "speaker_centered",
            "error": reason,
            "total_speakers": 0,
            "analysis_timestamp": datetime.now().isoformat(),
            "confidence": 0.0
        }
    
    def _create_error_result(self, error_msg: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return {
            "analysis_type": "speaker_centered",
            "error": error_msg,
            "total_speakers": 0,
            "analysis_timestamp": datetime.now().isoformat(),
            "confidence": 0.0
        }