"""
í‚¤ì›Œë“œ ê²€ìƒ‰ ì—”ì§„ (KeywordSearchEngine)

ì „ë¬¸ ê²€ìƒ‰(Full Text Search) ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import re
from typing import Dict, List, Any, Optional
import asyncio
import os
from collections import defaultdict
from sqlalchemy.orm import Session
from sqlalchemy import text

logger = logging.getLogger(__name__)


class KeywordSearchEngine:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, db_session_factory=None, index_manager=None, chroma_manager=None, llm_client=None):
        self.db_session_factory = db_session_factory
        self.index_manager = index_manager
        self.chroma_manager = chroma_manager
        self.llm_client = llm_client  # Upstage API í´ë¼ì´ì–¸íŠ¸
        
        # ê¸°ë³¸ ë¶ˆìš©ì–´ ëª©ë¡
        self.stopwords = set([
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with"
        ])
    
    async def search(self, 
                    query: str, 
                    filters: Optional[Dict[str, Any]] = None, 
                    top_k: int = 10, 
                    sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            filters: ê²€ìƒ‰ í•„í„°
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            sources: ê²€ìƒ‰í•  ì†ŒìŠ¤ ëª©ë¡
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘: '{query}' (top_k={top_k})")
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            processed_query = self._preprocess_query(query)
            if not processed_query:
                logger.warning("ì¿¼ë¦¬ ì „ì²˜ë¦¬ í›„ ë¹ˆ ë¬¸ìì—´")
                return {"documents": [], "scores": [], "metadata": []}
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = await self._extract_keywords_with_llm(processed_query)
            logger.info(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            
            # ê²€ìƒ‰ ì‹¤í–‰
            if self.db_session_factory:
                results = await self._search_in_index(keywords, filters, top_k, sources)
                logger.info(f"âœ… FTS ê²€ìƒ‰ ì™„ë£Œ: {len(results.get('documents', []))}ê°œ ê²°ê³¼")
            else:
                logger.warning("DB ì„¸ì…˜ íŒ©í† ë¦¬ê°€ ì—†ì–´ í´ë°± ê²€ìƒ‰ ì‚¬ìš©")
                results = await self._fallback_search(keywords, query, top_k)
            
            return results
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return {"documents": [], "scores": [], "metadata": []}
    
    def _preprocess_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        
        # ê¸°ë³¸ ì •ì œ
        query = re.sub(r'[^\w\sê°€-í£]', ' ', query)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        query = re.sub(r'\s+', ' ', query).strip()   # ê³µë°± ì •ë¦¬
        
        return query.lower()
    
    async def _extract_keywords_with_llm(self, query: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not self.llm_client:
                logger.warning("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©")
                return await self._simple_keyword_extraction(query)
            
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ë‚˜ ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ë¶ˆìš©ì–´, ì¡°ì‚¬, ì§ˆë¬¸ì–´ëŠ” ì œì™¸í•˜ê³  ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë˜ëŠ” ëª…ì‚¬, ê³ ìœ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸/ì¿¼ë¦¬: "{query}"

í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‹µë³€í•˜ì„¸ìš”. ì˜ˆì‹œ:
- "ìœ¤í¬ì°¬ì´ ëˆ„êµ¬ì•¼?" â†’ "ìœ¤í¬ì°¬"
- "ê¹€ì •ì¸ì€ ì–´ë–¤ ì‚¬ëŒì¸ê°€ìš”?" â†’ "ê¹€ì •ì¸"
- "YBIGTA íšŒì¥ì€ ëˆ„êµ¬ì¸ê°€ìš”?" â†’ "YBIGTA, íšŒì¥"
- "ë„¤íŠ¸ì›Œí¬ ê°•ì˜ ìë£Œ ì–´ë””ì— ìˆì–´?" â†’ "ë„¤íŠ¸ì›Œí¬, ê°•ì˜, ìë£Œ"

ë‹µë³€:"""

            # LLM í˜¸ì¶œ
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client.invoke_async(messages)
            
            # ì‘ë‹µ ì²˜ë¦¬ (ë¬¸ìì—´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì²˜ë¦¬)
            if response:
                if isinstance(response, str):
                    keywords_text = response.strip()
                elif isinstance(response, dict) and response.get("content"):
                    keywords_text = response["content"].strip()
                else:
                    logger.warning("LLM ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„")
                    return await self._simple_keyword_extraction(query)
                
                # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
                
                logger.info(f"ğŸ” LLM í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼: {keywords}")
                return keywords
            else:
                logger.warning("LLM ì‘ë‹µì´ ë¹„ì–´ìˆì–´ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©")
                return await self._simple_keyword_extraction(query)
            
        except Exception as e:
            logger.error(f"LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©
            return await self._simple_keyword_extraction(query)
    
    async def _simple_keyword_extraction(self, query: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ëŒ€ì‹  ì‚¬ìš©)"""
        # ì§ˆë¬¸ í˜•íƒœ ì²˜ë¦¬
        question_words = ["ëˆ„êµ¬", "ë¬´ì—‡", "ì–´ë–¤", "ì–´ë””", "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "ì–¼ë§ˆë‚˜"]
        question_endings = ["ì•¼?", "ì¸ê°€ìš”?", "ì…ë‹ˆê¹Œ?", "ì„¸ìš”?", "ì„¸ìš”", "ê¹Œ?"]
        
        # ì§ˆë¬¸ í˜•íƒœì¸ì§€ í™•ì¸
        is_question = any(word in query for word in question_words) or any(query.endswith(ending) for ending in question_endings)
        
        if is_question:
            # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
            # "ìœ¤í¬ì°¬ì´ ëˆ„êµ¬ì•¼?" -> "ìœ¤í¬ì°¬"
            # "ê¹€ì •ì¸ì€ ì–´ë–¤ ì‚¬ëŒì¸ê°€ìš”?" -> "ê¹€ì •ì¸"
            words = query.split()
            keywords = []
            
            for word in words:
                # ì§ˆë¬¸ ë‹¨ì–´ë‚˜ ì¡°ì‚¬ ì œê±°
                if word in question_words or word in ["ì´", "ì€", "ëŠ”", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "í•˜ê³ ", "ë©°", "ë©´ì„œ"]:
                    continue
                # ì§ˆë¬¸ ì–´ë¯¸ ì œê±°
                if any(word.endswith(ending.replace("?", "")) for ending in question_endings):
                    continue
                # ë¶ˆìš©ì–´ ì œê±°
                if word.lower() in self.stopwords:
                    continue
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±°
                if len(word.strip()) < 2:
                    continue
                    
                keywords.append(word.strip())
            
            logger.info(f"ğŸ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼: {keywords}")
            return keywords
        else:
            # ì¼ë°˜ ì¿¼ë¦¬ì˜ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            words = query.split()
            keywords = []
            
            for word in words:
                if word.lower() in self.stopwords:
                    continue
                if len(word.strip()) < 2:
                    continue
                keywords.append(word.strip())
            
            logger.info(f"ğŸ” ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼: {keywords}")
            return keywords
    
    def _extract_keywords_fallback(self, query: str) -> List[str]:
        """í´ë°± í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§"""
        return self._extract_keywords(query)
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        
        # ê¸°ë³¸ ì •ì œ
        query = re.sub(r'[^\w\sê°€-í£]', ' ', query)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        query = re.sub(r'\s+', ' ', query).strip()   # ê³µë°± ì •ë¦¬
        
        return query.lower().split()
    
    async def _search_in_index(self, 
                              keywords: List[str], 
                              filters: Optional[Dict], 
                              top_k: int, 
                              sources: Optional[List[str]]) -> Dict[str, Any]:
        """ChromaDB ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        
        try:
            # ChromaDB ë§¤ë‹ˆì €ê°€ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(self, 'chroma_manager') and self.chroma_manager:
                return await self._search_in_chroma(keywords, filters, top_k, sources)
            else:
                logger.warning("ChromaDB ë§¤ë‹ˆì €ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return await self._fallback_search(keywords, " ".join(keywords), top_k)
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return {"documents": [], "scores": [], "metadata": []}
    
    async def _search_in_chroma(self, keywords: List[str], filters: Optional[Dict], top_k: int, sources: Optional[List[str]]) -> Dict[str, Any]:
        """ChromaDBì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        
        try:
            # ChromaDB ë§¤ë‹ˆì €ê°€ ìˆëŠ”ì§€ í™•ì¸
            if not self.chroma_manager:
                logger.warning("ChromaDB ë§¤ë‹ˆì €ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {"documents": [], "scores": [], "metadata": []}
            
            # ChromaDB ì»¬ë ‰ì…˜ì—ì„œ ì§ì ‘ ê²€ìƒ‰
            all_documents = []
            
            # Unified DBì—ì„œ ê²€ìƒ‰
            if hasattr(self.chroma_manager, 'unified_adapter') and self.chroma_manager.unified_adapter.available:
                try:
                    # ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
                    collection = self.chroma_manager.unified_adapter.collection
                    if collection:
                        # ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                        results = collection.get(include=['documents', 'metadatas'])
                        
                        if results and results['documents']:
                            for i, doc_content in enumerate(results['documents']):
                                doc_metadata = results['metadatas'][i] if results['metadatas'] else {}
                                
                                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                                score = 0
                                for keyword in keywords:
                                    if keyword.lower() in doc_content.lower():
                                        score += doc_content.lower().count(keyword.lower())
                                    if keyword.lower() in doc_metadata.get('title', '').lower():
                                        score += doc_metadata.get('title', '').lower().count(keyword.lower()) * 2
                                
                                if score > 0:
                                    all_documents.append({
                                        'content': doc_content,
                                        'metadata': doc_metadata,
                                        'score': score
                                    })
                        
                        logger.info(f"ğŸ” Unified DBì—ì„œ {len(all_documents)}ê°œ ë¬¸ì„œ ë§¤ì¹­")
                except Exception as e:
                    logger.error(f"Unified DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
            # Incremental DBì—ì„œ ê²€ìƒ‰
            if hasattr(self.chroma_manager, 'incremental_manager') and self.chroma_manager.incremental_manager.available:
                try:
                    collection = self.chroma_manager.incremental_manager.collection
                    if collection:
                        results = collection.get(include=['documents', 'metadatas'])
                        
                        if results and results['documents']:
                            for i, doc_content in enumerate(results['documents']):
                                doc_metadata = results['metadatas'][i] if results['metadatas'] else {}
                                
                                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                                score = 0
                                for keyword in keywords:
                                    if keyword.lower() in doc_content.lower():
                                        score += doc_content.lower().count(keyword.lower())
                                    if keyword.lower() in doc_metadata.get('title', '').lower():
                                        score += doc_metadata.get('title', '').lower().count(keyword.lower()) * 2
                                
                                if score > 0:
                                    all_documents.append({
                                        'content': doc_content,
                                        'metadata': doc_metadata,
                                        'score': score
                                    })
                        
                        logger.info(f"ğŸ” Incremental DBì—ì„œ {len([d for d in all_documents if d['metadata'].get('storage') == 'incremental'])}ê°œ ë¬¸ì„œ ë§¤ì¹­")
                except Exception as e:
                    logger.error(f"Incremental DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            all_documents.sort(key=lambda x: x['score'], reverse=True)
            
            documents = [doc['content'] for doc in all_documents[:top_k]]
            scores = [doc['score'] for doc in all_documents[:top_k]]
            metadata = [doc['metadata'] for doc in all_documents[:top_k]]
            
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ (í‚¤ì›Œë“œ: {keywords})")
            
            return {
                "documents": documents,
                "scores": scores,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"ChromaDB í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return {"documents": [], "scores": [], "metadata": []}
    
    def _build_fts_query(self, keywords: List[str]) -> str:
        """FTS ì¿¼ë¦¬ êµ¬ì„±"""
        if not keywords:
            return ""
        
        # ê° í‚¤ì›Œë“œë¥¼ í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ê³  ORë¡œ ì—°ê²°
        quoted_keywords = [f'"{keyword}"' for keyword in keywords]
        return " OR ".join(quoted_keywords)
    
    def _build_where_conditions(self, filters: Optional[Dict], sources: Optional[List[str]]) -> str:
        """WHERE ì¡°ê±´ êµ¬ì„±"""
        conditions = []
        
        if filters:
            for key, value in filters.items():
                if isinstance(value, str):
                    conditions.append(f"d.{key} = '{value}'")
                elif isinstance(value, list):
                    placeholders = "', '".join(value)
                    conditions.append(f"d.{key} IN ('{placeholders}')")
                else:
                    conditions.append(f"d.{key} = {value}")
        
        if sources:
            placeholders = "', '".join(sources)
            conditions.append(f"d.source IN ('{placeholders}')")
        
        return " AND ".join(conditions) if conditions else "1=1"
    
    async def _execute_fts_query(self, session: Session, fts_query: str, where_conditions: str, top_k: int) -> Dict[str, Any]:
        """FTS ì¿¼ë¦¬ ì‹¤í–‰"""
        
        logger.info(f"ğŸ” FTS ì¿¼ë¦¬ ì‹¤í–‰: '{fts_query}' (top_k={top_k})")
        
        # FTS ê²€ìƒ‰ê³¼ documents í…Œì´ë¸” JOIN
        sql_query = f"""
        SELECT 
            d.id,
            d.title,
            d.content,
            d.source,
            d.url,
            d.doc_metadata,
            fts.rank,
            fts.highlight(document_fts, 0, '<mark>', '</mark>') as highlighted_content
        FROM document_fts fts
        JOIN documents d ON fts.rowid = d.id
        WHERE document_fts MATCH :query AND {where_conditions}
        ORDER BY fts.rank
        LIMIT :limit
        """
        
        try:
            logger.info(f"ğŸ“ SQL ì¿¼ë¦¬: {sql_query}")
            logger.info(f"ğŸ“ íŒŒë¼ë¯¸í„°: query='{fts_query}', limit={top_k}")
            
            # SQLAlchemyì˜ ëª…ëª…ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            result = session.execute(text(sql_query), {"query": fts_query, "limit": top_k})
            rows = result.fetchall()
            
            logger.info(f"ğŸ“Š FTS ì¿¼ë¦¬ ê²°ê³¼: {len(rows)}ê°œ í–‰")
            
            documents = []
            scores = []
            metadata = []
            
            for i, row in enumerate(rows):
                # FTS rankë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ (rankê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
                # rankëŠ” 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ê´€ë ¨ì„±ì´ ë†’ìŒ
                score = self._convert_rank_to_score(row.rank)
                
                documents.append(row.content or "")
                scores.append(score)
                metadata.append({
                    "id": row.id,
                    "title": row.title,
                    "source": row.source,
                    "url": row.url,
                    "highlighted_content": row.highlighted_content,
                    "doc_metadata": row.doc_metadata,
                    "rank": row.rank,
                    "type": "fts_search"
                })
                
                if i < 3:  # ì²˜ìŒ 3ê°œ ê²°ê³¼ë§Œ ë¡œê·¸
                    logger.info(f"  ğŸ“„ ê²°ê³¼ {i+1}: {row.title} (rank={row.rank}, score={score:.3f})")
            
            logger.info(f"âœ… FTS ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            
            return {
                "documents": documents,
                "scores": scores,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"FTS ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return {"documents": [], "scores": [], "metadata": []}
    
    def _convert_rank_to_score(self, rank: float) -> float:
        """FTS rankë¥¼ ì ìˆ˜ë¡œ ë³€í™˜"""
        # FTS rankëŠ” 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ê´€ë ¨ì„±ì´ ë†’ìŒ
        # 0~1 ë²”ìœ„ì˜ ì ìˆ˜ë¡œ ë³€í™˜
        if rank == 0:
            return 1.0
        elif rank < 0:
            return 0.5  # ìŒìˆ˜ rankëŠ” ì¤‘ê°„ ì ìˆ˜
        else:
            # rankê°€ í´ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ (ìµœì†Œ 0.1)
            return max(0.1, 1.0 / (1.0 + rank))
    
    async def _fallback_search(self, keywords: List[str], original_query: str, top_k: int) -> Dict[str, Any]:
        """í´ë°± ê²€ìƒ‰ (ì¸ë±ìŠ¤ê°€ ì—†ì„ ë•Œ)"""
        
        # ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë®¬ë ˆì´ì…˜
        documents = []
        scores = []
        metadata = []
        
        for i, keyword in enumerate(keywords[:top_k]):
            documents.append(f"í´ë°± ê²€ìƒ‰ ê²°ê³¼: '{keyword}' í¬í•¨ ë¬¸ì„œ ('{original_query}' ê²€ìƒ‰)")
            scores.append(max(0.5 - (i * 0.05), 0.1))  # ì˜ë¯¸ì  ê²€ìƒ‰ë³´ë‹¤ ë‚®ì€ ì ìˆ˜
            metadata.append({
                "source": "fallback",
                "type": "keyword_match",
                "matched_keywords": [keyword],
                "match_type": "contains"
            })
        
        return {
            "documents": documents,
            "scores": scores,
            "metadata": metadata
        }
    
    def calculate_tf_idf_score(self, term: str, document: str, corpus: List[str]) -> float:
        """TF-IDF ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨ êµ¬í˜„)"""
        
        # ìš©ì–´ ë¹ˆë„ (TF)
        doc_words = document.lower().split()
        tf = doc_words.count(term.lower()) / len(doc_words) if doc_words else 0
        
        # ë¬¸ì„œ ë¹ˆë„ (DF)
        df = sum(1 for doc in corpus if term.lower() in doc.lower().split())
        
        # ì—­ë¬¸ì„œ ë¹ˆë„ (IDF)
        import math
        idf = math.log(len(corpus) / (df + 1)) if df > 0 else 0
        
        return tf * idf
    
    def fuzzy_match_score(self, query_word: str, document_word: str) -> float:
        """í¼ì§€ ë§¤ì¹­ ì ìˆ˜ (ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜)"""
        
        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        distance = levenshtein_distance(query_word.lower(), document_word.lower())
        max_len = max(len(query_word), len(document_word))
        
        if max_len == 0:
            return 1.0
        
        return 1.0 - (distance / max_len)
    
    def get_search_stats(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ë°˜í™˜"""
        
        return {
            "engine_type": "keyword_fts",
            "stopwords_count": len(self.stopwords),
            "database_available": self.db_session_factory is not None,
            "index_available": self.index_manager is not None,
            "supported_features": [
                "fts5_search",
                "exact_match",
                "keyword_highlighting",
                "rank_scoring",
                "metadata_filtering"
            ]
        }