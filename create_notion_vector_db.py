
import os
import asyncio
import aiohttp
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from typing import List, Dict, Any, Optional
from openai import OpenAI
import itertools
import time

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()
all_docs = []

# Upstage API í‚¤ í’€ ì„¤ì • (ë¡œë“œ ë°¸ëŸ°ì‹±ìš©)
UPSTAGE_API_KEYS = []
for i in range(1, 9):  # UPSTAGE_API_KEY1 ~ UPSTAGE_API_KEY8
    key = os.getenv(f"UPSTAGE_API_KEY{i}")
    if key:
        UPSTAGE_API_KEYS.append(key)

print(f"ğŸ”‘ {len(UPSTAGE_API_KEYS)}ê°œì˜ Upstage API í‚¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

# API í‚¤ ìˆœí™˜ì„ ìœ„í•œ ì´í„°ë ˆì´í„°
api_key_cycle = itertools.cycle(UPSTAGE_API_KEYS) if UPSTAGE_API_KEYS else itertools.cycle([os.getenv("UPSTAGE_API_KEY")])

# ì»¤ìŠ¤í…€ Upstage ì„ë² ë”© í´ë˜ìŠ¤ (ë¹„ë™ê¸°)
class AsyncUpstageEmbeddings:
    def __init__(self, api_keys: List[str], model="embedding-query"):
        self.api_keys = api_keys
        self.model = model
        self.base_url = "https://api.upstage.ai/v1"
        self.key_cycle = itertools.cycle(api_keys) if api_keys else itertools.cycle([os.getenv("UPSTAGE_API_KEY")])
    
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œë“¤ì„ ë³‘ë ¬ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        async with aiohttp.ClientSession() as session:
            tasks = []
            for text in texts:
                api_key = next(self.key_cycle)
                tasks.append(self._embed_single(session, text, api_key))
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            embeddings = []
            for result in results:
                if isinstance(result, Exception):
                    print(f"âš ï¸ ì„ë² ë”© ì‹¤íŒ¨: {result}")
                    # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
                    embeddings.append([0.0] * 4096)
                else:
                    embeddings.append(result)
            
            return embeddings
    
    async def _embed_single(self, session: aiohttp.ClientSession, text: str, api_key: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "input": text,
            "model": self.model
        }
        
        url = f"{self.base_url}/embeddings"
        async with session.post(url, headers=headers, json=data, timeout=30) as response:
            if response.status == 200:
                result = await response.json()
                return result["data"][0]["embedding"]
            else:
                raise Exception(f"ì„ë² ë”© API í˜¸ì¶œ ì‹¤íŒ¨: {response.status}")
    
    def embed_query(self, text: str) -> List[float]:
        """ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤ (ë™ê¸°)."""
        return asyncio.run(self._embed_single_sync(text))
    
    async def _embed_single_sync(self, text: str) -> List[float]:
        async with aiohttp.ClientSession() as session:
            api_key = next(self.key_cycle)
            return await self._embed_single(session, text, api_key)

# Upstage ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
embeddings = AsyncUpstageEmbeddings(UPSTAGE_API_KEYS)

# --- 1. Notion í˜ì´ì§€ ë°ì´í„° ë¡œë“œ ---
print("="*50)
print("Phase 1: Notion í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

def get_notion_page_ids():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë“  Notion í˜ì´ì§€ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    page_ids = []
    i = 1
    while True:
        page_id = os.getenv(f"NOTION_PAGE_ID_{i}")
        if page_id:
            # í˜ì´ì§€ IDì—ì„œ ì ‘ë‘ì‚¬ ì œê±° ë° í˜•ì‹ ì •ê·œí™”
            if '-' in page_id:
                # ì ‘ë‘ì‚¬-UUID í˜•íƒœì¸ ê²½ìš° UUID ë¶€ë¶„ë§Œ ì¶”ì¶œ
                parts = page_id.split('-')
                if len(parts) > 1 and len(parts[-1]) >= 32:
                    # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ 32ì ì´ìƒì´ë©´ UUIDë¡œ ê°„ì£¼
                    uuid_part = ''.join(parts[1:])  # ì²« ë²ˆì§¸ ë¶€ë¶„(ì ‘ë‘ì‚¬) ì œê±°
                    page_id = uuid_part.replace('-', '')  # í•˜ì´í”ˆ ì œê±°
            
            # 32ìë¦¬ UUIDì¸ì§€ í™•ì¸
            if len(page_id) == 32 and page_id.replace('-', '').isalnum():
                page_ids.append(page_id)
                print(f"âœ… í˜ì´ì§€ ID {i} ì •ê·œí™”: {page_id}")
            else:
                print(f"âš ï¸ ì˜ëª»ëœ í˜ì´ì§€ ID {i} í˜•ì‹: {page_id}")
            i += 1
        else:
            break
    return page_ids

def extract_rich_text(rich_text_list: List[Dict[str, Any]]) -> str:
    """Rich text ë°°ì—´ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not rich_text_list:
        return ""
    
    text_parts = []
    for rt in rich_text_list:
        text = rt.get("plain_text", "")
        annotations = rt.get("annotations", {})
        
        # í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ë§ ì ìš©
        if annotations.get("bold"):
            text = f"**{text}**"
        if annotations.get("italic"):
            text = f"*{text}*"
        if annotations.get("strikethrough"):
            text = f"~~{text}~~"
        if annotations.get("underline"):
            text = f"__{text}__"
        if annotations.get("code"):
            text = f"`{text}`"
        
        # ë§í¬ ì²˜ë¦¬
        if rt.get("href"):
            text = f"[{text}]({rt['href']})"
        
        text_parts.append(text)
    
    return "".join(text_parts)

def process_block_content(block: Dict[str, Any], headers: Dict[str, str]) -> List[str]:
    """ë¸”ë¡ì˜ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    content_parts = []
    block_type = block.get("type")
    block_data = block.get(block_type, {})
    
    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤
    text_blocks = [
        "paragraph", "heading_1", "heading_2", "heading_3", 
        "bulleted_list_item", "numbered_list_item", "quote", "callout",
        "toggle", "to_do"
    ]
    
    if block_type in text_blocks and block_data.get("rich_text"):
        text = extract_rich_text(block_data["rich_text"])
        if text.strip():
            # ë¸”ë¡ íƒ€ì…ë³„ ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…
            if block_type == "heading_1":
                content_parts.append(f"# {text}")
            elif block_type == "heading_2":
                content_parts.append(f"## {text}")
            elif block_type == "heading_3":
                content_parts.append(f"### {text}")
            elif block_type == "bulleted_list_item":
                content_parts.append(f"â€¢ {text}")
            elif block_type == "numbered_list_item":
                content_parts.append(f"1. {text}")
            elif block_type == "quote":
                content_parts.append(f"> {text}")
            elif block_type == "callout":
                icon = block_data.get("icon", {}).get("emoji", "ğŸ’¡")
                content_parts.append(f"{icon} **{text}**")
            elif block_type == "toggle":
                content_parts.append(f"<details><summary>{text}</summary>")
            elif block_type == "to_do":
                checked = block_data.get("checked", False)
                checkbox = "â˜‘ï¸" if checked else "â˜"
                content_parts.append(f"{checkbox} {text}")
            else:
                content_parts.append(text)
    
    # ì½”ë“œ ë¸”ë¡
    elif block_type == "code":
        text = extract_rich_text(block_data.get("rich_text", []))
        language = block_data.get("language", "")
        if text.strip():
            content_parts.append(f"```{language}\n{text}\n```")
    
    # í…Œì´ë¸”
    elif block_type == "table":
        table_rows = block_data.get("table_rows", [])
        if table_rows:
            content_parts.append("| " + " | ".join([extract_rich_text(cell.get("rich_text", [])) for cell in table_rows[0].get("cells", [])]) + " |")
            content_parts.append("| " + " | ".join(["---"] * len(table_rows[0].get("cells", []))) + " |")
            for row in table_rows[1:]:
                content_parts.append("| " + " | ".join([extract_rich_text(cell.get("rich_text", [])) for cell in row.get("cells", [])]) + " |")
    
    # ì´ë¯¸ì§€ - ì„ë² ë”©ì—ì„œ ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif block_type == "image":
        image_data = block_data.get("image", {})
        caption = extract_rich_text(image_data.get("caption", []))
        if caption.strip():
            content_parts.append(f"ğŸ–¼ï¸ **ì´ë¯¸ì§€**: {caption}")
        else:
            content_parts.append("ğŸ–¼ï¸ **ì´ë¯¸ì§€** (ìº¡ì…˜ ì—†ìŒ)")
    
    # íŒŒì¼ - ì„ë² ë”©ì—ì„œ ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif block_type == "file":
        file_data = block_data.get("file", {})
        caption = extract_rich_text(file_data.get("caption", []))
        if caption.strip():
            content_parts.append(f"ğŸ“ **íŒŒì¼**: {caption}")
        else:
            content_parts.append("ğŸ“ **ì²¨ë¶€ íŒŒì¼**")
    
    # ë¶ë§ˆí¬ - ì„ë² ë”©ì—ì„œ ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif block_type == "bookmark":
        url = block_data.get("url", "")
        caption = extract_rich_text(block_data.get("caption", []))
        if caption.strip():
            content_parts.append(f"ğŸ”– **ë¶ë§ˆí¬**: {caption}")
        else:
            content_parts.append(f"ğŸ”– **ë¶ë§ˆí¬**: {url}")
    
    # êµ¬ë¶„ì„ 
    elif block_type == "divider":
        content_parts.append("---")
    
    # ë™ê¸°í™”ëœ ë¸”ë¡ (ì¬ê·€ì  ì²˜ë¦¬)
    elif block_type == "synced_block":
        synced_from = block_data.get("synced_from")
        if synced_from:
            # ë™ê¸°í™”ëœ ì›ë³¸ ë¸”ë¡ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜´
            try:
                synced_blocks = get_block_children(synced_from.get("block_id"), headers)
                for synced_block in synced_blocks:
                    content_parts.extend(process_block_content(synced_block, headers))
            except Exception as e:
                content_parts.append(f"âš ï¸ ë™ê¸°í™”ëœ ë¸”ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # í•˜ìœ„ í˜ì´ì§€
    elif block_type == "child_page":
        title = block_data.get("title", "")
        content_parts.append(f"ğŸ“„ **{title}** (í•˜ìœ„ í˜ì´ì§€)")
    
    # í•˜ìœ„ ë°ì´í„°ë² ì´ìŠ¤
    elif block_type == "child_database":
        title = block_data.get("title", "")
        content_parts.append(f"ğŸ—ƒï¸ **{title}** (í•˜ìœ„ ë°ì´í„°ë² ì´ìŠ¤)")
    
    # ëª©ì°¨
    elif block_type == "table_of_contents":
        content_parts.append("ğŸ“‘ **ëª©ì°¨**")
    
    # ì»¬ëŸ¼
    elif block_type == "column_list":
        # ì»¬ëŸ¼ì€ í•˜ìœ„ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬ë¨
        pass
    
    # ì»¬ëŸ¼
    elif block_type == "column":
        # ì»¬ëŸ¼ì€ í•˜ìœ„ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬ë¨
        pass
    
    return content_parts

async def get_block_children(session: aiohttp.ClientSession, block_id: str, headers: Dict[str, str]) -> List[Dict[str, Any]]:
    """ë¸”ë¡ì˜ í•˜ìœ„ ë¸”ë¡ë“¤ì„ ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    all_blocks = []
    start_cursor = None
    
    while True:
        url = f"https://api.notion.com/v1/blocks/{block_id}/children"
        params = {"page_size": 100}
        if start_cursor:
            params["start_cursor"] = start_cursor
        
        try:
            async with session.get(url, headers=headers, params=params, timeout=30) as response:
                if response.status != 200:
                    print(f"  âŒ ë¸”ë¡ í•˜ìœ„ ìš”ì†Œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {response.status}")
                    break
                
                data = await response.json()
                blocks = data.get("results", [])
                all_blocks.extend(blocks)
                print(f"  ğŸ“¦ ë¸”ë¡ {len(blocks)}ê°œ ë¡œë“œë¨ (ì´ {len(all_blocks)}ê°œ)")
                
                # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                if not data.get("has_more"):
                    break
                
                start_cursor = data.get("next_cursor")
        
        except asyncio.TimeoutError:
            print(f"  â° ë¸”ë¡ {block_id} ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
            break
        
        # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
        await asyncio.sleep(0.05)
    
    return all_blocks

async def process_blocks_recursively(session: aiohttp.ClientSession, blocks: List[Dict[str, Any]], headers: Dict[str, str], depth: int = 0, max_depth: int = 10) -> List[str]:
    """ë¸”ë¡ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ëª¨ë“  ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if depth > max_depth:
        print(f"  âš ï¸ ìµœëŒ€ ì¤‘ì²© ê¹Šì´({max_depth}) ë„ë‹¬, ì¬ê·€ ì¤‘ë‹¨")
        return []
    
    content_parts = []
    
    # ë¸”ë¡ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œì”©)
    chunk_size = 5
    block_chunks = [blocks[i:i+chunk_size] for i in range(0, len(blocks), chunk_size)]
    
    for chunk_idx, chunk in enumerate(block_chunks):
        print(f"  {'  ' * depth}ğŸ”„ ì²­í¬ {chunk_idx+1}/{len(block_chunks)} ë³‘ë ¬ ì²˜ë¦¬ ì¤‘... ({len(chunk)}ê°œ ë¸”ë¡)")
        
        # ì²­í¬ ë‚´ ë¸”ë¡ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for block in chunk:
            tasks.append(process_single_block(session, block, headers, depth, max_depth))
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"  {'  ' * depth}âŒ ë¸”ë¡ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
            else:
                content_parts.extend(result)
    
    return content_parts

async def process_single_block(session: aiohttp.ClientSession, block: Dict[str, Any], headers: Dict[str, str], depth: int, max_depth: int) -> List[str]:
    """ë‹¨ì¼ ë¸”ë¡ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    content_parts = []
    
    # í˜„ì¬ ë¸”ë¡ì˜ ë‚´ìš© ì²˜ë¦¬
    try:
        block_content = process_block_content(block, headers)
        content_parts.extend(block_content)
    except Exception as e:
        print(f"  {'  ' * depth}âŒ ë¸”ë¡ ë‚´ìš© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return content_parts
    
    # í•˜ìœ„ ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸ (ì¬ê·€ì  ì²˜ë¦¬)
    if block.get("has_children", False) and depth < max_depth:
        try:
            child_blocks = await get_block_children(session, block["id"], headers)
            if child_blocks:
                child_content = await process_blocks_recursively(session, child_blocks, headers, depth + 1, max_depth)
                content_parts.extend(child_content)
        except Exception as e:
            print(f"  {'  ' * depth}âš ï¸ í•˜ìœ„ ë¸”ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return content_parts

async def load_notion_page_content(session: aiohttp.ClientSession, page_id: str, api_key: str) -> Document:
    """Notion APIë¥¼ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•˜ì—¬ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    # í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    page_url = f"https://api.notion.com/v1/pages/{page_id}"
    async with session.get(page_url, headers=headers, timeout=30) as page_response:
        if page_response.status != 200:
            raise Exception(f"í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {page_response.status}")
        
        page_data = await page_response.json()
    
    # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
    title = "ì œëª© ì—†ìŒ"
    if "properties" in page_data:
        for prop_name, prop_data in page_data["properties"].items():
            if prop_data.get("type") == "title" and prop_data.get("title"):
                title = extract_rich_text(prop_data["title"])
                break
    
    # í˜ì´ì§€ì˜ ëª¨ë“  ë¸”ë¡ì„ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
    print(f"  ğŸ” í˜ì´ì§€ ë¸”ë¡ íƒìƒ‰ ì¤‘...")
    blocks = await get_block_children(session, page_id, headers)
    
    # ëª¨ë“  ë¸”ë¡ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    print(f"  ğŸ“ {len(blocks)}ê°œ ë¸”ë¡ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")
    content_parts = await process_blocks_recursively(session, blocks, headers)
    
    content = "\n\n".join(content_parts)
    
    # Document ê°ì²´ ìƒì„±
    doc = Document(
        page_content=content,
        metadata={
            "title": title,
            "source": f"notion_page_{page_id}",
            "page_id": page_id,
            "block_count": len(blocks)
        }
    )
    
    return doc

# Notion í˜ì´ì§€ ID ëª©ë¡ (ë™ì ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
NOTION_PAGE_IDS = get_notion_page_ids()

if not NOTION_PAGE_IDS:
    print("âš ï¸ ì„¤ì •ëœ Notion í˜ì´ì§€ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("   .env íŒŒì¼ì— NOTION_PAGE_ID_1, NOTION_PAGE_ID_2 ë“±ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
else:
    print(f"ğŸ” ì´ {len(NOTION_PAGE_IDS)}ê°œì˜ Notion í˜ì´ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

async def load_all_notion_pages(page_ids: List[str], api_key: str) -> List[Document]:
    """ëª¨ë“  Notion í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    async with aiohttp.ClientSession() as session:
        tasks = []
        
        for i, page_id in enumerate(page_ids, 1):
            print(f"ğŸ“„ í˜ì´ì§€ {i}/{len(page_ids)} ë³‘ë ¬ ë¡œë“œ ì¤€ë¹„... (ID: {page_id})")
            tasks.append(load_single_page_with_logging(session, page_id, api_key, i, len(page_ids)))
        
        print(f"ğŸš€ {len(tasks)}ê°œ í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œ ì‹œì‘...")
        start_time = time.time()
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        print(f"â±ï¸ ë³‘ë ¬ ë¡œë“œ ì™„ë£Œ: {end_time - start_time:.2f}ì´ˆ")
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        notion_docs = []
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                print(f"âŒ í˜ì´ì§€ {i} ë¡œë“œ ì‹¤íŒ¨: {result}")
            else:
                notion_docs.append(result)
        
        return notion_docs

async def load_single_page_with_logging(session: aiohttp.ClientSession, page_id: str, api_key: str, page_num: int, total_pages: int) -> Document:
    """ë‹¨ì¼ í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    try:
        print(f"\nğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ë¡œë“œ ì‹œì‘... (ID: {page_id})")
        
        doc = await load_notion_page_content(session, page_id, api_key)
        
        print(f"âœ… í˜ì´ì§€ {page_num} ë¡œë“œ ì™„ë£Œ!")
        print(f"  - ğŸ“„ {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"  - ğŸ“Š ë¸”ë¡ ìˆ˜: {doc.metadata.get('block_count', 0)}ê°œ")
        print(f"  - ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(doc.page_content)}ì")
        
        return doc
        
    except Exception as e:
        print(f"âŒ í˜ì´ì§€ {page_num} (ID: {page_id}) ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e

# Notion í˜ì´ì§€ë“¤ì„ ë³‘ë ¬ë¡œ ë¡œë“œ
notion_docs = asyncio.run(load_all_notion_pages(NOTION_PAGE_IDS, os.getenv("NOTION_API_KEY")))

all_docs.extend(notion_docs)
print(f"\nâœ… ì´ {len(notion_docs)}ê°œì˜ Notion ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

# --- 2. ìµœì¢… DB ìƒì„± ---
print("="*50)
if all_docs:
    print(f"Phase 2: ì´ {len(all_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(all_docs)

    # Upstage ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DB ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
    print(f"ğŸ”— {len(UPSTAGE_API_KEYS)}ê°œì˜ Upstage API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì„ë² ë”© ìƒì„±...")
    
    # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text_contents = [doc.page_content for doc in texts]
    
    # ë³‘ë ¬ ì„ë² ë”© ìƒì„±
    start_time = time.time()
    embeddings_list = asyncio.run(embeddings.embed_documents(text_contents))
    end_time = time.time()
    print(f"âš¡ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {end_time - start_time:.2f}ì´ˆ ({len(embeddings_list)}ê°œ ë²¡í„°)")
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_embeddings(
        list(zip(text_contents, embeddings_list)),
        embedding=embeddings,
        metadatas=[doc.metadata for doc in texts]
    )
    vectorstore.save_local("notion_faiss_index")
    print("\nğŸ‰ ëª¨ë“  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Vector DBê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("\nâš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ì–´ DBë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")