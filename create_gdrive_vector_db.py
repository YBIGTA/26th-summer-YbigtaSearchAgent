import os
import requests
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.document_loaders import UnstructuredFileLoader
from typing import List
import json

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import io
import tempfile

# --- ì„¤ì •(Configuration) ---
load_dotenv()
FAISS_INDEX_PATH = "gdrive_faiss_index"
BATCH_SIZE = 5
# íŒŒì‹±ëœ ê²°ê³¼ë¥¼ ì €ì¥í•  ìºì‹œ íŒŒì¼ ê²½ë¡œ
PARSED_CACHE_FILE = "parsed_cache.json"

# --- ë§ì¶¤ ì„ë² ë”© í´ë˜ìŠ¤ ---
class CustomUpstageEmbeddings(Embeddings):
    def __init__(self, model: str = "embedding-passage", chunk_size: int = 100):
        self.model = model
        self.api_key = os.getenv("UPSTAGE_API_KEY")
        self.base_url = "https://api.upstage.ai/v1/embeddings"
        self.chunk_size = chunk_size

    def _embed(self, texts: List[str]) -> List[List[float]]:
        if not self.api_key:
            raise ValueError(".env íŒŒì¼ì— UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        all_embeddings = []
        # APIì˜ í† í° ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(texts), self.chunk_size):
            batch = texts[i:i + self.chunk_size]
            
            headers = {"Authorization": f"Bearer {self.api_key}"}
            data = {"input": batch, "model": self.model}
            
            response = requests.post(self.base_url, headers=headers, json=data)
            
            if response.status_code == 200:
                response_data = response.json().get("data", [])
                batch_embeddings = [None] * len(batch)
                for item in response_data:
                    batch_embeddings[item['index']] = item['embedding']
                all_embeddings.extend(batch_embeddings)
            else:
                raise Exception(f"Upstage Embeddings API ì˜¤ë¥˜: {response.status_code} - {response.text}")
        
        return all_embeddings

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        query_model = self.model.replace("passage", "query")
        temp_model = self.model
        self.model = query_model
        embedding = self._embed([text])[0]
        self.model = temp_model
        return embedding

# --- Upstage Document Parser í˜¸ì¶œ í•¨ìˆ˜ ---
def parse_with_upstage(file_path, original_file_name):
    print(f"  - ğŸ¤– Upstage Parserë¡œ '{original_file_name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    url = "https://api.upstage.ai/v1/document-digitization"
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        print("  - ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    headers = {"Authorization": f"Bearer {api_key}"}
    try:
        with open(file_path, "rb") as f:
            files = {"document": f}
            data = {"model": "document-parse"}
            response = requests.post(url, headers=headers, files=files, data=data)
        if response.status_code == 200:
            content = response.json().get("content", {})
            full_text = content.get("markdown") or content.get("html") or ""
            if not full_text:
                elements = response.json().get("elements", [])
                full_text = "\n".join([el.get("content", {}).get("text", "") for el in elements])
            print(f"  - âœ… '{original_file_name}' ì²˜ë¦¬ ì™„ë£Œ.")
            return full_text
        else:
            print(f"  - ğŸš¨ Upstage Parser ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"  - ğŸš¨ Upstage API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

# --- Vector DB ì—…ë°ì´íŠ¸ ë° ì €ì¥ í•¨ìˆ˜ ---
def update_vectorstore(vectorstore, documents, embeddings):
    if not documents:
        return vectorstore

    # ğŸ‘‡ [ìˆ˜ì •] DBì— ì¶”ê°€í•˜ê¸° ì „ì— í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì¡°ê°(chunk)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)
    
    print(f"\nğŸ’¾ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Vector DBì— ì¶”ê°€/ì €ì¥í•©ë‹ˆë‹¤...")
    
    if vectorstore is None:
        # DBê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        vectorstore = FAISS.from_documents(split_docs, embeddings)
    else:
        # ê¸°ì¡´ DBì— ë¬¸ì„œ ì¶”ê°€
        vectorstore.add_documents(split_docs)
    
    vectorstore.save_local(FAISS_INDEX_PATH)
    print("ğŸ’¾ ì €ì¥ ì™„ë£Œ.")
    return vectorstore

# --- ë©”ì¸ ë¡œì§ ---
print("="*50)
print("Vector DB ìƒì„±/ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

# 1. ì„ë² ë”© ëª¨ë¸ ë° ê¸°ì¡´ DB ë¡œë“œ
print("ğŸ›°ï¸ Upstage Solar ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤...")
embeddings = CustomUpstageEmbeddings(model="embedding-passage")
vectorstore = None
if os.path.exists(FAISS_INDEX_PATH):
    try:
        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ê¸°ì¡´ '{FAISS_INDEX_PATH}' ì¸ë±ìŠ¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
else:
    print(f"â„¹ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

# ìºì‹œ íŒŒì¼ ë¡œë“œ
if os.path.exists(PARSED_CACHE_FILE):
    with open(PARSED_CACHE_FILE, 'r', encoding='utf-8') as f:
        parsed_cache = json.load(f)
    print(f"âœ… ê¸°ì¡´ ìºì‹œ íŒŒì¼ '{PARSED_CACHE_FILE}'ì—ì„œ {len(parsed_cache)}ê°œì˜ í•­ëª©ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
else:
    parsed_cache = {}

# 2. Google Drive ë°ì´í„° ì²˜ë¦¬
print("="*50)
print("Google Driveì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
batch_docs = []

try:
    folder_id = os.getenv("GDRIVE_FOLDER_ID")
    creds = service_account.Credentials.from_service_account_file(
        "gdrive-credentials.json", scopes=["https://www.googleapis.com/auth/drive.readonly"]
    )
    service = build("drive", "v3", credentials=creds)

    with tempfile.TemporaryDirectory() as temp_dir:
        def process_folder(folder_id):
            global vectorstore, batch_docs, parsed_cache
            query = f"'{folder_id}' in parents"
            results = service.files().list(q=query, fields="files(id, name, mimeType)").execute()
            items = results.get("files", [])

            for item in items:
                original_file_name = item['name']
                file_id = item['id']
                mime_type = item['mimeType']
                
                if mime_type == "application/vnd.google-apps.folder":
                    print(f"ğŸ“‚ í•˜ìœ„ í´ë” '{original_file_name}' íƒìƒ‰ ì‹œì‘...")
                    process_folder(file_id)
                    continue

                print(f"  - ğŸ“„ íŒŒì¼ '{original_file_name}' ë°œê²¬.")
                
                if file_id in parsed_cache:
                    print(f"  - âš¡ï¸ ìºì‹œ íˆíŠ¸! '{original_file_name}'ëŠ” ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì…ë‹ˆë‹¤.")
                    page_content = parsed_cache[file_id]
                    doc = Document(page_content=page_content, metadata={"source": original_file_name})
                    batch_docs.append(doc)
                else:
                    sanitized_file_name = original_file_name.replace("/", "_")
                    _, file_ext = os.path.splitext(sanitized_file_name)
                    ext = file_ext.lower()
                    
                    doc_content = None
                    if ext in ['.pdf', '.docx', '.pptx', '.xlsx', '.py', '.txt']:
                        request = service.files().get_media(fileId=file_id)
                        fh = io.BytesIO()
                        downloader = MediaIoBaseDownload(fh, request)
                        done = False
                        while not done:
                            status, done = downloader.next_chunk()
                        temp_file_path = os.path.join(temp_dir, sanitized_file_name)
                        with open(temp_file_path, "wb") as f:
                            f.write(fh.getvalue())
                        
                        if ext in ['.pdf', '.docx', '.pptx', '.xlsx']:
                            doc_content = parse_with_upstage(temp_file_path, original_file_name)
                        elif ext in ['.py', '.txt']:
                            print(f"  - ğŸ“ ì¼ë°˜ í…ìŠ¤íŠ¸ ë¡œë”ë¡œ '{original_file_name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
                            loader = UnstructuredFileLoader(temp_file_path)
                            loaded_docs = loader.load()
                            if loaded_docs:
                                doc_content = loaded_docs[0].page_content
                            print(f"  - âœ… '{original_file_name}' ì²˜ë¦¬ ì™„ë£Œ.")
                    else:
                        print(f"  - â­ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹({ext})ì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")

                    if doc_content is not None:
                        print(f"  - ğŸ’¾ ìºì‹œì— '{original_file_name}'ì˜ íŒŒì‹± ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
                        parsed_cache[file_id] = doc_content
                        doc = Document(page_content=doc_content, metadata={"source": original_file_name})
                        batch_docs.append(doc)

                if len(batch_docs) >= BATCH_SIZE:
                    vectorstore = update_vectorstore(vectorstore, batch_docs, embeddings)
                    batch_docs = []
        
        process_folder(folder_id)

finally:
    print("="*50)
    print("ìµœì¢… ì •ë¦¬ ë° ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    vectorstore = update_vectorstore(vectorstore, batch_docs, embeddings)
    
    with open(PARSED_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(parsed_cache, f, ensure_ascii=False, indent=4)
    print(f"âœ… ì—…ë°ì´íŠ¸ëœ ìºì‹œë¥¼ '{PARSED_CACHE_FILE}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    print("\nğŸ‰ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
