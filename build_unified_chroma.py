import os
import io
import json
import time
import uuid
import argparse
import tempfile
import asyncio
import itertools
from typing import List, Dict, Any, Optional, Tuple

import requests
import aiohttp
from dotenv import load_dotenv

# LangChain types
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader

# Google Drive
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# Chroma
import chromadb

# === ê³µí†µ ì„¤ì • ===
load_dotenv()
CHROMA_DB_PATH = "unified_chroma_db"
DEFAULT_COLLECTION_NAME = "unified_knowledge_db"
BATCH_SIZE = 5  # ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°

# === ê³µí†µ: Upstage ì„ë² ë”© ===
class CustomUpstageEmbeddings:
    def __init__(self, model: str = "embedding-passage", chunk_size: int = 100):
        self.model = model
        self.chunk_size = chunk_size
        self.api_key = os.getenv("UPSTAGE_API_KEY")
        if not self.api_key:
            raise ValueError(".env íŒŒì¼ì— UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.base_url = "https://api.upstage.ai/v1/embeddings"

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts, self.model)

    def embed_query(self, text: str) -> List[float]:
        query_model = self.model.replace("passage", "query") if "passage" in self.model else self.model
        return self._embed([text], query_model)[0]

    def _embed(self, texts: List[str], model: str) -> List[List[float]]:
        headers = {"Authorization": f"Bearer {self.api_key}"}
        all_embeddings: List[List[float]] = []
        for i in range(0, len(texts), self.chunk_size):
            batch = texts[i:i + self.chunk_size]
            resp = requests.post(self.base_url, headers=headers, json={"input": batch, "model": model}, timeout=60)
            if resp.status_code != 200:
                raise RuntimeError(f"Upstage Embeddings API ì˜¤ë¥˜: {resp.status_code} - {resp.text}")
            data = resp.json().get("data", [])
            # ì‘ë‹µì˜ index ìˆœì„œë¥¼ ë³´ì¡´í•˜ì—¬ ë°°ì¹˜ ìˆœì„œëŒ€ë¡œ ì¬ë°°ì—´
            batch_embeddings: List[Optional[List[float]]] = [None] * len(batch)
            for item in data:
                batch_embeddings[item["index"]] = item["embedding"]
            all_embeddings.extend([e for e in batch_embeddings if e is not None])
        return all_embeddings


# === ê³µí†µ: Chroma ì ì¬ ===
def add_to_chroma(documents: List[Document], collection_name: str) -> Tuple[int, int]:
    if not documents:
        return (0, 0)

    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    collection = client.get_or_create_collection(name=collection_name)

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)

    texts = [d.page_content for d in split_docs]
    metadatas = [d.metadata for d in split_docs]

    embeddings = CustomUpstageEmbeddings(model="embedding-passage")
    vectors = embeddings.embed_documents(texts)

    batch_size = 100
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_meta = metadatas[i:i + batch_size]
        batch_vecs = vectors[i:i + batch_size]
        batch_ids = [str(uuid.uuid4()) for _ in range(len(batch_texts))]
        collection.add(documents=batch_texts, metadatas=batch_meta, ids=batch_ids, embeddings=batch_vecs)

    return (len(split_docs), collection.count())


# === GDRIVE ìˆ˜ì§‘ ===
def run_gdrive(folder_id: str, collection_name: str) -> int:
    print("=" * 50)
    print("Google Driveì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    creds = service_account.Credentials.from_service_account_file(
        "gdrive-credentials.json", scopes=["https://www.googleapis.com/auth/drive.readonly"]
    )
    service = build("drive", "v3", credentials=creds)

    parsed_cache_path = "parsed_cache.json"
    if os.path.exists(parsed_cache_path):
        with open(parsed_cache_path, "r", encoding="utf-8") as f:
            parsed_cache: Dict[str, str] = json.load(f)
    else:
        parsed_cache = {}

    batch_docs: List[Document] = []
    total_added = 0
    
    with tempfile.TemporaryDirectory() as temp_dir:
        def process_folder(fid: str) -> None:
            nonlocal batch_docs, total_added
            query = f"'{fid}' in parents"
            try:
                results = service.files().list(q=query, fields="files(id, name, mimeType)").execute()
                items = results.get("files", [])
            except Exception as e:
                print(f"  - ğŸš¨ í´ë” ID '{fid}' ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                print(f"  - â­ï¸ í•´ë‹¹ í´ë” ê±´ë„ˆë›°ê¸°")
                return

            for item in items:
                name = item["name"]
                file_id = item["id"]
                mime_type = item["mimeType"]
                print(f"  - ğŸ“„ íŒŒì¼ '{name}' (MIME: {mime_type})")
                
                # í´ë”ëŠ” ì¬ê·€ ì²˜ë¦¬
                if mime_type == "application/vnd.google-apps.folder":
                    process_folder(file_id)
                    continue

                # ìºì‹œ íˆíŠ¸ í™•ì¸
                if file_id in parsed_cache:
                    print(f"  - âš¡ï¸ ìºì‹œ íˆíŠ¸! '{name}'ëŠ” ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì…ë‹ˆë‹¤.")
                    page_content = parsed_cache[file_id]
                    doc = Document(page_content=page_content, metadata={"source": name, "gdrive_file_id": file_id})
                    batch_docs.append(doc)
                else:
                    # content ë³€ìˆ˜ ì´ˆê¸°í™”
                    content: Optional[str] = None
                    
                    # íŒŒì¼ í™•ì¥ì í™•ì¸
                    sanitized_file_name = name.replace("/", "_")
                    _, file_ext = os.path.splitext(sanitized_file_name)
                    ext = file_ext.lower()
                    
                    # ì§€ì›í•˜ëŠ” í™•ì¥ìë§Œ ì²˜ë¦¬
                    if ext in ['.pdf', '.docx', '.pptx', '.xlsx', '.py', '.txt']:
                        # Google Workspace íŒŒì¼ì¸ì§€ í™•ì¸
                        if "google-apps" in mime_type:
                            # Google Workspace íŒŒì¼ì€ Export API ì‚¬ìš©
                            export_mime_types = {
                                "application/vnd.google-apps.document": "text/plain",
                                "application/vnd.google-apps.spreadsheet": "text/csv",
                                "application/vnd.google-apps.presentation": "text/plain",
                                "application/vnd.google-apps.drawing": "image/png",
                                "application/vnd.google-apps.form": "text/plain"
                            }
                            
                            export_mime = export_mime_types.get(mime_type)
                            if export_mime and mime_type != "application/vnd.google-apps.folder":
                                try:
                                    request = service.files().export_media(fileId=file_id, mimeType=export_mime)
                                    fh = io.BytesIO()
                                    downloader = MediaIoBaseDownload(fh, request)
                                    done = False
                                    while not done:
                                        status, done = downloader.next_chunk()
                                    
                                    if export_mime == "text/plain" or export_mime == "text/csv":
                                        content = fh.getvalue().decode('utf-8', errors='ignore')
                                        print(f"  - âœ… Google Workspace íŒŒì¼ '{name}' Export ì„±ê³µ")
                                    else:
                                        # ì´ë¯¸ì§€ ë“±ì€ ê±´ë„ˆë›°ê¸°
                                        content = None
                                        print(f"  - â­ï¸ ì´ë¯¸ì§€ íŒŒì¼ '{name}' ê±´ë„ˆë›°ê¸°")
                                except Exception as e:
                                    print(f"  - âš ï¸ Google Workspace íŒŒì¼ '{name}' Export ì‹¤íŒ¨: {e}")
                                    content = None
                            elif mime_type == "application/vnd.google-apps.shortcut":
                                print(f"  - â­ï¸ Google Workspace shortcut íŒŒì¼ '{name}' ê±´ë„ˆë›°ê¸°")
                                content = None
                            else:
                                print(f"  - â­ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” Google Workspace íŒŒì¼ íƒ€ì… '{name}' (MIME: {mime_type}) ê±´ë„ˆë›°ê¸°")
                                content = None
                        else:
                            # ì¼ë°˜ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
                            try:
                                request = service.files().get_media(fileId=file_id)
                                fh = io.BytesIO()
                                downloader = MediaIoBaseDownload(fh, request)
                                done = False
                                while not done:
                                    status, done = downloader.next_chunk()
                                
                                temp_file_path = os.path.join(temp_dir, sanitized_file_name)
                                with open(temp_file_path, "wb") as f:
                                    f.write(fh.getvalue())

                                # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬
                                if ext in ['.pdf', '.docx', '.pptx', '.xlsx']:
                                    content = parse_with_upstage(temp_file_path, name)
                                    if content:
                                        print(f"  - âœ… ì—…ìŠ¤í…Œì´ì§€ íŒŒì„œë¡œ '{name}' ì²˜ë¦¬ ì„±ê³µ")
                                    else:
                                        print(f"  - âš ï¸ ì—…ìŠ¤í…Œì´ì§€ íŒŒì„œë¡œ '{name}' ì²˜ë¦¬ ì‹¤íŒ¨")
                                elif ext in ['.py', '.txt']:
                                    print(f"  - ğŸ“ ì¼ë°˜ í…ìŠ¤íŠ¸ ë¡œë”ë¡œ '{name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
                                    try:
                                        loader = UnstructuredFileLoader(temp_file_path)
                                        loaded_docs = loader.load()
                                        if loaded_docs:
                                            content = loaded_docs[0].page_content
                                        print(f"  - âœ… '{name}' ì²˜ë¦¬ ì™„ë£Œ.")
                                    except Exception as e:
                                        print(f"  - âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ '{name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
                                        content = None
                            except Exception as e:
                                print(f"  - âš ï¸ íŒŒì¼ '{name}' ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                                content = None
                    else:
                        print(f"  - â­ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹({ext})ì´ë¯€ë¡œ ê±´ë„ˆë›°ê¸°")

                    if content:
                        print(f"  - ğŸ’¾ ìºì‹œì— '{name}'ì˜ íŒŒì‹± ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
                        parsed_cache[file_id] = content
                        doc = Document(page_content=content, metadata={"source": name, "gdrive_file_id": file_id})
                        batch_docs.append(doc)

                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì¦‰ì‹œ ì²˜ë¦¬
                if len(batch_docs) >= BATCH_SIZE:
                    print(f"\nğŸ’¾ {len(batch_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
                    added, total = add_to_chroma(batch_docs, collection_name)
                    total_added += added
                    print(f"  - âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ. ì¶”ê°€ëœ ì²­í¬: {added}, ì´ ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {total}")
                    batch_docs = []

        process_folder(folder_id)

    # ë‚¨ì€ ë¬¸ì„œë“¤ ì²˜ë¦¬
    if batch_docs:
        print(f"\nğŸ’¾ ë‚¨ì€ {len(batch_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ìµœì¢… ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        added, total = add_to_chroma(batch_docs, collection_name)
        total_added += added
        print(f"  - âœ… ìµœì¢… ì²˜ë¦¬ ì™„ë£Œ. ì¶”ê°€ëœ ì²­í¬: {added}, ì´ ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {total}")

    # ìºì‹œ ì €ì¥
    with open(parsed_cache_path, "w", encoding="utf-8") as f:
        json.dump(parsed_cache, f, ensure_ascii=False, indent=2)

    print(f"GDrive ì´ ì¶”ê°€ëœ ì²­í¬ ìˆ˜: {total_added}")
    return total_added


def parse_with_upstage(file_path: str, original_file_name: str) -> Optional[str]:
    print(f"  - ğŸ¤– Upstage Parserë¡œ '{original_file_name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    url = "https://api.upstage.ai/v1/document-digitization"
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        print("  - ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    headers = {"Authorization": f"Bearer {api_key}"}
    try:
        with open(file_path, "rb") as f:
            files = {"document": f}
            data = {"model": "document-parse"}
            response = requests.post(url, headers=headers, files=files, data=data, timeout=120)
        if response.status_code == 200:
            content = response.json().get("content", {})
            full_text = content.get("markdown") or content.get("html") or ""
            if not full_text:
                elements = response.json().get("elements", [])
                full_text = "\n".join([el.get("content", {}).get("text", "") for el in elements])
            print(f"  - âœ… '{original_file_name}' ì²˜ë¦¬ ì™„ë£Œ.")
            return full_text
        else:
            print(f"  - ğŸš¨ Upstage Parser ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"  - ğŸš¨ Upstage API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None


# === GitHub ìˆ˜ì§‘ ===
def get_all_repos_from_org(org_name: str) -> List[Dict[str, Any]]:
    """ì¡°ì§ì˜ ëª¨ë“  ê³µê°œ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    repos = []
    page = 1
    headers = {"Accept": "application/vnd.github.v3+json"}
    token = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
    if token:
        headers["Authorization"] = f"token {token}"

    while True:
        url = f"https://api.github.com/orgs/{org_name}/repos?type=public&page={page}&per_page=100"
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            if not data:
                break
            repos.extend(data)
            page += 1
        except requests.RequestException as e:
            print(f"ğŸš¨ GitHub APIì—ì„œ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
    return repos

def get_readme_content(repo_full_name: str) -> Optional[str]:
    """GitHub APIë¥¼ í†µí•´ ë¦¬í¬ì§€í† ë¦¬ì˜ README íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    headers = {"Accept": "application/vnd.github.v3.raw"}
    token = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
    if token:
        headers["Authorization"] = f"token {token}"
    
    readme_names = ["README.md", "README.rst", "README.txt", "readme.md"]
    for name in readme_names:
        url = f"https://api.github.com/repos/{repo_full_name}/contents/{name}"
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
        except requests.RequestException:
            continue
    return None

def run_github(org_name: str, collection_name: str) -> int:
    print("=" * 50)
    print(f"GitHub ì¡°ì§ '{org_name}'ì—ì„œ READMEë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    
    # ìºì‹œ íŒŒì¼ ë¡œë“œ
    processed_cache_path = "processed_repos_cache.json"
    if os.path.exists(processed_cache_path):
        with open(processed_cache_path, "r", encoding="utf-8") as f:
            processed_repos = set(json.load(f))
        print(f"âœ… ê¸°ì¡´ ìºì‹œì—ì„œ {len(processed_repos)}ê°œì˜ ì²˜ë¦¬ëœ ë¦¬í¬ì§€í† ë¦¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    else:
        processed_repos = set()

    total_added = 0

    try:
        print("=" * 50)
        print(f"ğŸ¢ {org_name} ì¡°ì§ì˜ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
        all_repos = get_all_repos_from_org(org_name)
        print(f"ğŸ” ì´ {len(all_repos)}ê°œì˜ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        for repo in all_repos:
            repo_name = repo['name']
            repo_full_name = repo['full_name']
            print("-" * 50)
            
            if repo_name in processed_repos:
                print(f"âš¡ï¸ ìºì‹œ íˆíŠ¸! '{repo_name}' ë¦¬í¬ì§€í† ë¦¬ëŠ” ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            print(f"ğŸš€ '{repo_name}' ë¦¬í¬ì§€í† ë¦¬ ì²˜ë¦¬ ì‹œì‘...")

            try:
                print(f"  - ğŸ“„ README.md íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤...")
                readme_content = get_readme_content(repo_full_name)

                if readme_content is None:
                    print(f"  - âš ï¸ README íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ READMEë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                    readme_content = f"# {repo_name}\n\nì´ ë¦¬í¬ì§€í† ë¦¬ì— ëŒ€í•œ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤."

                doc = Document(
                    page_content=readme_content,
                    metadata={'source': f"https://github.com/{repo_full_name}"}
                )
                
                # ê° repoë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ (ì˜› ë²„ì „ê³¼ ë™ì¼)
                added, total = add_to_chroma([doc], collection_name)
                total_added += added
                processed_repos.add(repo_name)
                print(f"  - âœ… '{repo_name}' ì²˜ë¦¬ ì™„ë£Œ. ì¶”ê°€ëœ ì²­í¬: {added}")

            except Exception as e:
                print(f"  - ğŸš¨ ì‹¤íŒ¨! '{repo_name}' ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    except Exception as e:
        print(f"\nğŸš¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        print("=" * 50)
        print("ìµœì¢… ì •ë¦¬ ë° ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
        with open(processed_cache_path, "w", encoding="utf-8") as f:
            json.dump(list(processed_repos), f, ensure_ascii=False, indent=4)
        print(f"âœ… ì—…ë°ì´íŠ¸ëœ ìºì‹œë¥¼ '{processed_cache_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    print(f"GitHub ì´ ì¶”ê°€ëœ ì²­í¬ ìˆ˜: {total_added}")
    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return total_added


# === Notion ìˆ˜ì§‘ (ì™„ì „í•œ ë¸”ë¡ ì²˜ë¦¬ ë¡œì§) ===
def get_notion_page_ids():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë“  Notion í˜ì´ì§€ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    page_ids = []
    i = 1
    while True:
        page_id = os.getenv(f"NOTION_PAGE_ID_{i}")
        if page_id:
            # í˜ì´ì§€ IDì—ì„œ ì ‘ë‘ì‚¬ ì œê±° ë° í˜•ì‹ ì •ê·œí™”
            if '-' in page_id:
                # ì ‘ë‘ì‚¬-UUID í˜•íƒœì¸ ê²½ìš° UUID ë¶€ë¶„ë§Œ ì¶”ì¶œ
                parts = page_id.split('-')
                if len(parts) > 1 and len(parts[-1]) >= 32:
                    # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ 32ì ì´ìƒì´ë©´ UUIDë¡œ ê°„ì£¼
                    uuid_part = ''.join(parts[1:])  # ì²« ë²ˆì§¸ ë¶€ë¶„(ì ‘ë‘ì‚¬) ì œê±°
                    page_id = uuid_part.replace('-', '')  # í•˜ì´í”ˆ ì œê±°
            
            # 32ìë¦¬ UUIDì¸ì§€ í™•ì¸
            if len(page_id) == 32 and page_id.replace('-', '').isalnum():
                page_ids.append(page_id)
                print(f"âœ… í˜ì´ì§€ ID {i} ì •ê·œí™”: {page_id}")
            else:
                print(f"âš ï¸ ì˜ëª»ëœ í˜ì´ì§€ ID {i} í˜•ì‹: {page_id}")
            i += 1
        else:
            break
    return page_ids

async def extract_rich_text(rich_text_list: List[Dict[str, Any]]) -> str:
    """Rich text ë°°ì—´ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not rich_text_list:
        return ""
    
    text_parts = []
    for rt in rich_text_list:
        text = rt.get("plain_text", "")
        annotations = rt.get("annotations", {})
        
        # í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ë§ ì ìš©
        if annotations.get("bold"):
            text = f"**{text}**"
        if annotations.get("italic"):
            text = f"*{text}*"
        if annotations.get("strikethrough"):
            text = f"~~{text}~~"
        if annotations.get("underline"):
            text = f"__{text}__"
        if annotations.get("code"):
            text = f"`{text}`"
        
        # ë§í¬ ì²˜ë¦¬
        if rt.get("href"):
            text = f"[{text}]({rt['href']})"
        
        text_parts.append(text)
    
    return "".join(text_parts)

async def process_block_content(block: Dict[str, Any], headers: Dict[str, str]) -> List[str]:
    """ë¸”ë¡ì˜ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    content_parts = []
    block_type = block.get("type")
    block_data = block.get(block_type, {})
    
    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤
    text_blocks = [
        "paragraph", "heading_1", "heading_2", "heading_3", 
        "bulleted_list_item", "numbered_list_item", "quote", "callout",
        "toggle", "to_do"
    ]
    
    if block_type in text_blocks and block_data.get("rich_text"):
        text = await extract_rich_text(block_data["rich_text"])
        if text.strip():
            # ë¸”ë¡ íƒ€ì…ë³„ ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…
            if block_type == "heading_1":
                content_parts.append(f"# {text}")
            elif block_type == "heading_2":
                content_parts.append(f"## {text}")
            elif block_type == "heading_3":
                content_parts.append(f"### {text}")
            elif block_type == "bulleted_list_item":
                content_parts.append(f"â€¢ {text}")
            elif block_type == "numbered_list_item":
                content_parts.append(f"1. {text}")
            elif block_type == "quote":
                content_parts.append(f"> {text}")
            elif block_type == "callout":
                icon = block_data.get("icon", {}).get("emoji", "ğŸ’¡")
                content_parts.append(f"{icon} **{text}**")
            elif block_type == "toggle":
                content_parts.append(f"<details><summary>{text}</summary>")
            elif block_type == "to_do":
                checked = block_data.get("checked", False)
                checkbox = "â˜‘ï¸" if checked else "â˜"
                content_parts.append(f"{checkbox} {text}")
            else:
                content_parts.append(text)
    
    # ì½”ë“œ ë¸”ë¡
    elif block_type == "code":
        text = await extract_rich_text(block_data.get("rich_text", []))
        language = block_data.get("language", "")
        if text.strip():
            content_parts.append(f"```{language}\n{text}\n```")
    
    # í…Œì´ë¸”
    elif block_type == "table":
        table_rows = block_data.get("table_rows", [])
        if table_rows:
            content_parts.append("| " + " | ".join([await extract_rich_text(cell.get("rich_text", [])) for cell in table_rows[0].get("cells", [])]) + " |")
            content_parts.append("| " + " | ".join(["---"] * len(table_rows[0].get("cells", []))) + " |")
            for row in table_rows[1:]:
                content_parts.append("| " + " | ".join([await extract_rich_text(cell.get("rich_text", [])) for cell in row.get("cells", [])]) + " |")
    
    # ì´ë¯¸ì§€ - ì„ë² ë”©ì—ì„œ ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif block_type == "image":
        image_data = block_data.get("image", {})
        caption = await extract_rich_text(image_data.get("caption", []))
        if caption.strip():
            content_parts.append(f"ğŸ–¼ï¸ **ì´ë¯¸ì§€**: {caption}")
        else:
            content_parts.append("ğŸ–¼ï¸ **ì´ë¯¸ì§€** (ìº¡ì…˜ ì—†ìŒ)")
    
    # íŒŒì¼ - ì„ë² ë”©ì—ì„œ ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif block_type == "file":
        file_data = block_data.get("file", {})
        caption = await extract_rich_text(file_data.get("caption", []))
        if caption.strip():
            content_parts.append(f"ğŸ“ **íŒŒì¼**: {caption}")
        else:
            content_parts.append("ğŸ“ **ì²¨ë¶€ íŒŒì¼**")
    
    # ë¶ë§ˆí¬ - ì„ë² ë”©ì—ì„œ ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif block_type == "bookmark":
        url = block_data.get("url", "")
        caption = await extract_rich_text(block_data.get("caption", []))
        if caption.strip():
            content_parts.append(f"ğŸ”– **ë¶ë§ˆí¬**: {caption}")
        else:
            content_parts.append(f"ğŸ”– **ë¶ë§ˆí¬**: {url}")
    
    # êµ¬ë¶„ì„ 
    elif block_type == "divider":
        content_parts.append("---")
    
    # ë™ê¸°í™”ëœ ë¸”ë¡ (ì¬ê·€ì  ì²˜ë¦¬)
    elif block_type == "synced_block":
        synced_from = block_data.get("synced_from")
        if synced_from:
            # ë™ê¸°í™”ëœ ì›ë³¸ ë¸”ë¡ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜´
            try:
                synced_blocks = await get_block_children(synced_from.get("block_id"), headers)
                for synced_block in synced_blocks:
                    content_parts.extend(await process_block_content(synced_block, headers))
            except Exception as e:
                content_parts.append(f"âš ï¸ ë™ê¸°í™”ëœ ë¸”ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # í•˜ìœ„ í˜ì´ì§€
    elif block_type == "child_page":
        title = block_data.get("title", "")
        content_parts.append(f"ğŸ“„ **{title}** (í•˜ìœ„ í˜ì´ì§€)")
    
    # í•˜ìœ„ ë°ì´í„°ë² ì´ìŠ¤
    elif block_type == "child_database":
        title = block_data.get("title", "")
        content_parts.append(f"ğŸ—ƒï¸ **{title}** (í•˜ìœ„ ë°ì´í„°ë² ì´ìŠ¤)")
    
    # ëª©ì°¨
    elif block_type == "table_of_contents":
        content_parts.append("ğŸ“‘ **ëª©ì°¨**")
    
    # ì»¬ëŸ¼
    elif block_type == "column_list":
        # ì»¬ëŸ¼ì€ í•˜ìœ„ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬ë¨
        pass
    
    # ì»¬ëŸ¼
    elif block_type == "column":
        # ì»¬ëŸ¼ì€ í•˜ìœ„ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬ë¨
        pass
    
    return content_parts

async def get_block_children(session: aiohttp.ClientSession, block_id: str, headers: Dict[str, str]) -> List[Dict[str, Any]]:
    """ë¸”ë¡ì˜ í•˜ìœ„ ë¸”ë¡ë“¤ì„ ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    all_blocks = []
    start_cursor = None
    
    while True:
        url = f"https://api.notion.com/v1/blocks/{block_id}/children"
        params = {"page_size": 100}
        if start_cursor:
            params["start_cursor"] = start_cursor
        
        try:
            async with session.get(url, headers=headers, params=params, timeout=30) as response:
                if response.status != 200:
                    print(f"  âŒ ë¸”ë¡ í•˜ìœ„ ìš”ì†Œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {response.status}")
                    break
                
                data = await response.json()
                blocks = data.get("results", [])
                all_blocks.extend(blocks)
                print(f"  ğŸ“¦ ë¸”ë¡ {len(blocks)}ê°œ ë¡œë“œë¨ (ì´ {len(all_blocks)}ê°œ)")
                
                # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                if not data.get("has_more"):
                    break
                
                start_cursor = data.get("next_cursor")
        
        except asyncio.TimeoutError:
            print(f"  â° ë¸”ë¡ {block_id} ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
            break
        
        # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
        await asyncio.sleep(0.05)
    
    return all_blocks

async def process_blocks_recursively(session: aiohttp.ClientSession, blocks: List[Dict[str, Any]], headers: Dict[str, str], depth: int = 0, max_depth: int = 10) -> List[str]:
    """ë¸”ë¡ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ëª¨ë“  ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if depth > max_depth:
        print(f"  âš ï¸ ìµœëŒ€ ì¤‘ì²© ê¹Šì´({max_depth}) ë„ë‹¬, ì¬ê·€ ì¤‘ë‹¨")
        return []
    
    content_parts = []
    
    # ë¸”ë¡ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œì”©)
    chunk_size = 5
    block_chunks = [blocks[i:i+chunk_size] for i in range(0, len(blocks), chunk_size)]
    
    for chunk_idx, chunk in enumerate(block_chunks):
        print(f"  {'  ' * depth}ğŸ”„ ì²­í¬ {chunk_idx+1}/{len(block_chunks)} ë³‘ë ¬ ì²˜ë¦¬ ì¤‘... ({len(chunk)}ê°œ ë¸”ë¡)")
        
        # ì²­í¬ ë‚´ ë¸”ë¡ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for block in chunk:
            tasks.append(process_single_block(session, block, headers, depth, max_depth))
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"  {'  ' * depth}âŒ ë¸”ë¡ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
            else:
                content_parts.extend(result)
    
    return content_parts

async def process_single_block(session: aiohttp.ClientSession, block: Dict[str, Any], headers: Dict[str, str], depth: int, max_depth: int) -> List[str]:
    """ë‹¨ì¼ ë¸”ë¡ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    content_parts = []
    
    # í˜„ì¬ ë¸”ë¡ì˜ ë‚´ìš© ì²˜ë¦¬
    try:
        block_content = await process_block_content(block, headers)
        content_parts.extend(block_content)
    except Exception as e:
        print(f"  {'  ' * depth}âŒ ë¸”ë¡ ë‚´ìš© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return content_parts
    
    # í•˜ìœ„ ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸ (ì¬ê·€ì  ì²˜ë¦¬)
    if block.get("has_children", False) and depth < max_depth:
        try:
            child_blocks = await get_block_children(session, block["id"], headers)
            if child_blocks:
                child_content = await process_blocks_recursively(session, child_blocks, headers, depth + 1, max_depth)
                content_parts.extend(child_content)
        except Exception as e:
            print(f"  {'  ' * depth}âš ï¸ í•˜ìœ„ ë¸”ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return content_parts

async def load_notion_page_content(session: aiohttp.ClientSession, page_id: str, api_key: str) -> Document:
    """Notion APIë¥¼ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•˜ì—¬ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    # í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    page_url = f"https://api.notion.com/v1/pages/{page_id}"
    async with session.get(page_url, headers=headers, timeout=30) as page_response:
        if page_response.status != 200:
            raise Exception(f"í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {page_response.status}")
        
        page_data = await page_response.json()
    
    # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
    title = "ì œëª© ì—†ìŒ"
    if "properties" in page_data:
        for prop_name, prop_data in page_data["properties"].items():
            if prop_data.get("type") == "title" and prop_data.get("title"):
                title = await extract_rich_text(prop_data["title"])
                break
    
    # í˜ì´ì§€ì˜ ëª¨ë“  ë¸”ë¡ì„ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
    print(f"  ğŸ” í˜ì´ì§€ ë¸”ë¡ íƒìƒ‰ ì¤‘...")
    blocks = await get_block_children(session, page_id, headers)
    
    # ëª¨ë“  ë¸”ë¡ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    print(f"  ğŸ“ {len(blocks)}ê°œ ë¸”ë¡ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")
    content_parts = await process_blocks_recursively(session, blocks, headers)
    
    content = "\n\n".join(content_parts)
    
    # Document ê°ì²´ ìƒì„±
    doc = Document(
        page_content=content,
        metadata={
            "title": title,
            "source": f"notion_page_{page_id}",
            "page_id": page_id,
            "block_count": len(blocks)
        }
    )
    
    return doc

async def load_all_notion_pages(page_ids: List[str], api_key: str) -> List[Document]:
    """ëª¨ë“  Notion í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    import aiohttp
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        
        for i, page_id in enumerate(page_ids, 1):
            print(f"ğŸ“„ í˜ì´ì§€ {i}/{len(page_ids)} ë³‘ë ¬ ë¡œë“œ ì¤€ë¹„... (ID: {page_id})")
            tasks.append(load_single_page_with_logging(session, page_id, api_key, i, len(page_ids)))
        
        print(f"ğŸš€ {len(tasks)}ê°œ í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œ ì‹œì‘...")
        start_time = time.time()
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        print(f"â±ï¸ ë³‘ë ¬ ë¡œë“œ ì™„ë£Œ: {end_time - start_time:.2f}ì´ˆ")
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        notion_docs = []
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                print(f"âŒ í˜ì´ì§€ {i} ë¡œë“œ ì‹¤íŒ¨: {result}")
            else:
                notion_docs.append(result)
        
        return notion_docs

async def load_single_page_with_logging(session: aiohttp.ClientSession, page_id: str, api_key: str, page_num: int, total_pages: int) -> Document:
    """ë‹¨ì¼ í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    try:
        print(f"\nğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ë¡œë“œ ì‹œì‘... (ID: {page_id})")
        
        doc = await load_notion_page_content(session, page_id, api_key)
        
        print(f"âœ… í˜ì´ì§€ {page_num} ë¡œë“œ ì™„ë£Œ!")
        print(f"  - ğŸ“„ {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"  - ğŸ“Š ë¸”ë¡ ìˆ˜: {doc.metadata.get('block_count', 0)}ê°œ")
        print(f"  - ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(doc.page_content)}ì")
        
        return doc
        
    except Exception as e:
        print(f"âŒ í˜ì´ì§€ {page_num} (ID: {page_id}) ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e

def run_notion(collection_name: str) -> int:
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ Notion í˜ì´ì§€ IDë“¤ ë¡œë“œ (NOTION_PAGE_ID_1 ...)
    page_ids = get_notion_page_ids()

    if not page_ids:
        print("âš ï¸ ì„¤ì •ëœ Notion í˜ì´ì§€ IDê°€ ì—†ìŠµë‹ˆë‹¤. NOTION_PAGE_ID_1,2,... í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return 0

    api_key = os.getenv("NOTION_API_KEY")
    if not api_key:
        print("âš ï¸ NOTION_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return 0

    docs = asyncio.run(load_all_notion_pages(page_ids, api_key))
    added, total = add_to_chroma(docs, collection_name)
    print(f"Notion ë¬¸ì„œ {len(docs)}ê°œ â†’ ì²­í¬ {added}ê°œ ì¶”ê°€. ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {total}")
    return added


# === ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ===
def main():
    parser = argparse.ArgumentParser(description="GDrive/GitHub/Notion ë°ì´í„°ë¥¼ unified_chroma_dbë¡œ ì ì¬í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--source", choices=["gdrive", "github", "notion", "all"], required=True, help="ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ")
    parser.add_argument("--collection", default=DEFAULT_COLLECTION_NAME, help="ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„")
    parser.add_argument("--gdrive_folder_id", default=os.getenv("GDRIVE_FOLDER_ID", ""), help="Google Drive í´ë” ID(ì†ŒìŠ¤=gdrive ë˜ëŠ” all)")
    parser.add_argument("--github_org", default=os.getenv("ORG_NAME", "YBIGTA"), help="GitHub Org ì´ë¦„(ì†ŒìŠ¤=github ë˜ëŠ” all)")
    args = parser.parse_args()

    collection_name = args.collection

    total_added = 0

    if args.source in ("gdrive", "all"):
        if not args.gdrive_folder_id:
            print("âš ï¸ GDrive í´ë” IDê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤. --gdrive_folder_id ë˜ëŠ” GDRIVE_FOLDER_ID ì„¤ì • í•„ìš”")
        else:
            total_added += run_gdrive(args.gdrive_folder_id, collection_name)

    if args.source in ("github", "all"):
        total_added += run_github(args.github_org, collection_name)

    if args.source in ("notion", "all"):
        total_added += run_notion(collection_name)

    print("=" * 50)
    print(f"ğŸ‰ í†µí•© ì‘ì—… ì™„ë£Œ. ì´ ì¶”ê°€ëœ ì²­í¬ ìˆ˜(ëŒ€ëµ): {total_added}")


if __name__ == "__main__":
    main() 