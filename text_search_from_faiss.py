import pickle
import re
from typing import List, Dict, Any
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

class FAISSTextSearcher:
    def __init__(self, faiss_index_path: str):
        """FAISS ì¸ë±ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
        self.faiss_index_path = faiss_index_path
        self.documents = []
        self.load_faiss_index()
    
    def load_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  ì›ë³¸ ë¬¸ì„œë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            vectorstore = FAISS.load_local(self.faiss_index_path, allow_dangerous_deserialization=True)
            
            # ì›ë³¸ ë¬¸ì„œë“¤ ì¶”ì¶œ
            self.documents = vectorstore.docstore._dict.values()
            print(f"âœ… {len(self.documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def search_by_text(self, query: str, top_k: int = 5, case_sensitive: bool = False) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        results = []
        
        # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬
        if not case_sensitive:
            query = query.lower()
        
        for doc in self.documents:
            content = doc.page_content
            if not case_sensitive:
                content = content.lower()
            
            # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­
            if query in content:
                # ë§¤ì¹­ëœ ë¶€ë¶„ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                context = self._extract_context(content, query, window_size=100)
                
                results.append({
                    'document': doc,
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'context': context,
                    'match_type': 'exact',
                    'score': 1.0
                })
        
        # ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def search_by_regex(self, pattern: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        results = []
        
        try:
            regex = re.compile(pattern, re.IGNORECASE)
        except re.error as e:
            print(f"âŒ ì •ê·œí‘œí˜„ì‹ ì˜¤ë¥˜: {e}")
            return results
        
        for doc in self.documents:
            content = doc.page_content
            matches = regex.findall(content)
            
            if matches:
                # ë§¤ì¹­ëœ ë¶€ë¶„ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                context = self._extract_regex_context(content, regex, window_size=100)
                
                results.append({
                    'document': doc,
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'context': context,
                    'matches': matches,
                    'match_count': len(matches),
                    'match_type': 'regex',
                    'score': len(matches)
                })
        
        # ë§¤ì¹­ ê°œìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def search_by_keywords(self, keywords: List[str], top_k: int = 5, operator: str = 'AND') -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        results = []
        
        for doc in self.documents:
            content = doc.page_content.lower()
            score = 0
            
            if operator == 'AND':
                # ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
                if all(keyword.lower() in content for keyword in keywords):
                    score = sum(content.count(keyword.lower()) for keyword in keywords)
            elif operator == 'OR':
                # í•˜ë‚˜ë¼ë„ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ë¨
                score = sum(content.count(keyword.lower()) for keyword in keywords)
            
            if score > 0:
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                context = self._extract_keywords_context(content, keywords, window_size=100)
                
                results.append({
                    'document': doc,
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'context': context,
                    'keywords_found': [kw for kw in keywords if kw.lower() in content],
                    'match_type': 'keywords',
                    'score': score
                })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def _extract_context(self, content: str, query: str, window_size: int = 100) -> str:
        """ë§¤ì¹­ëœ ë¶€ë¶„ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            start_pos = content.find(query)
            if start_pos == -1:
                return content[:window_size] + "..." if len(content) > window_size else content
            
            end_pos = start_pos + len(query)
            
            # ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ê³„ì‚°
            context_start = max(0, start_pos - window_size)
            context_end = min(len(content), end_pos + window_size)
            
            context = content[context_start:context_end]
            
            # ì»¨í…ìŠ¤íŠ¸ ì‹œì‘/ë í‘œì‹œ
            if context_start > 0:
                context = "..." + context
            if context_end < len(content):
                context = context + "..."
            
            return context
        except Exception as e:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return content[:window_size] + "..." if len(content) > window_size else content
    
    def _extract_regex_context(self, content: str, regex, window_size: int = 100) -> str:
        """ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­ ë¶€ë¶„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            match = regex.search(content)
            if not match:
                return content[:window_size] + "..." if len(content) > window_size else content
            
            start_pos = match.start()
            end_pos = match.end()
            
            # ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ê³„ì‚°
            context_start = max(0, start_pos - window_size)
            context_end = min(len(content), end_pos + window_size)
            
            context = content[context_start:context_end]
            
            # ì»¨í…ìŠ¤íŠ¸ ì‹œì‘/ë í‘œì‹œ
            if context_start > 0:
                context = "..." + context
            if context_end < len(content):
                context = context + "..."
            
            return context
        except Exception as e:
            print(f"âš ï¸ ì •ê·œí‘œí˜„ì‹ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return content[:window_size] + "..." if len(content) > window_size else content
    
    def _extract_keywords_context(self, content: str, keywords: List[str], window_size: int = 100) -> str:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ë¶„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
            first_match_pos = -1
            for keyword in keywords:
                pos = content.find(keyword.lower())
                if pos != -1:
                    first_match_pos = pos
                    break
            
            if first_match_pos == -1:
                return content[:window_size] + "..." if len(content) > window_size else content
            
            # ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ê³„ì‚°
            context_start = max(0, first_match_pos - window_size)
            context_end = min(len(content), first_match_pos + window_size)
            
            context = content[context_start:context_end]
            
            # ì»¨í…ìŠ¤íŠ¸ ì‹œì‘/ë í‘œì‹œ
            if context_start > 0:
                context = "..." + context
            if context_end < len(content):
                context = context + "..."
            
            return context
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return content[:window_size] + "..." if len(content) > window_size else content

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # FAISS ì¸ë±ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    searcher = FAISSTextSearcher("notion_faiss_index")
    
    # 1. ì •í™•í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    print("ğŸ” ì •í™•í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰:")
    results = searcher.search_by_text("YBIGTA", top_k=3)
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['metadata'].get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"   ì»¨í…ìŠ¤íŠ¸: {result['context']}")
        print()
    
    # 2. ì •ê·œí‘œí˜„ì‹ ê²€ìƒ‰
    print("ğŸ” ì •ê·œí‘œí˜„ì‹ ê²€ìƒ‰:")
    results = searcher.search_by_regex(r"\b[A-Z]{2,}\b", top_k=3)  # ëŒ€ë¬¸ì ë‹¨ì–´ ê²€ìƒ‰
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['metadata'].get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"   ë§¤ì¹­: {result['matches'][:5]}")  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print()
    
    # 3. í‚¤ì›Œë“œ ê²€ìƒ‰
    print("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰:")
    results = searcher.search_by_keywords(["í”„ë¡œì íŠ¸", "ê°œë°œ", "AI"], top_k=3, operator="OR")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['metadata'].get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"   ë°œê²¬ëœ í‚¤ì›Œë“œ: {result['keywords_found']}")
        print() 